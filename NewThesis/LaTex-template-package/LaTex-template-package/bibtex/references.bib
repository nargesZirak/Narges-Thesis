% This file was created with JabRef 2.10.
% Encoding: ASCII


@InProceedings{2005:esec_fse:sliwerski,
  Title                    = {{HATARI}: Raising risk awareness},
  Author                   = {{\'S}liwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
  Booktitle                = esec_fse,
  Year                     = {2005},
  Pages                    = {107--110},

  Abstract                 = {As a software system evolves, programmers make changes which sometimes lead to problems. The risk of later problems significantly depends on the location of the change. Which are the locations where changes impose the greatest risk? Our HATARI prototype relates a version history (such as CVS) to a bug database (such as BUGZILLA) to detect those locations where changes have been risky in the past. HATARI makes this risk visible for developers by annotating source code with color bars. Furthermore, HATARI provides views to browse through the most risky locations and to analyze the risk history of a particular location.},
  Doi                      = {10.1145/1081706.1081725}
}

@InProceedings{2007:gpce:savga,
  Title                    = {Refactoring-based support for binary compatibility in evolving frameworks},
  Author                   = {{\c{S}}avga, Ilie and Rudolf, Michael},
  Booktitle                = gpce,
  Year                     = {2007},
  Pages                    = {175--184},

  Abstract                 = {The evolution of a software framework may invalidate existing plugins---modules that used one of its previous versions. To preserve binary compatibility (i.e., plugins will link and run with a new framework release without recompilation), we automatically create an adaptation layer that translates between plugins and the framework. The creation of these adapters is guided by information about syntactic framework changes (refactorings). For each supported refactoring we formally define a comeback---a refactoring used to construct adapters. For an ordered set of refactorings that occured between two framework versions, the backward execution of the corresponding comebacks yields the adaptation layer.},
  Doi                      = {10.1145/1289971.1290000},
  Key                      = {Savga and Rudolf}
}

@InProceedings{2008:gpce:savga,
  Title                    = {Practical refactoring-based framework upgrade},
  Author                   = {Ilie {\c{S}}avga and Michael Rudolf and Sebastian G{\"o}tz and Uwe A{\ss}mann},
  Booktitle                = gpce,
  Year                     = {2008},
  Pages                    = {171--180},

  Abstract                 = {Although the API of a software framework should stay stable, in practice it often changes during maintenance. When deploying a new framework version such changes may invalidate plugins/modules that used one of its previous versions. While manual plugin adaptation is expensive and error-prone, automatic adaptation demands cumbersome specifications, which the developers are reluctant to write and maintain. Basing on the history of structural framework changes (refactorings), in our previous work we formally defined how to automatically derive an adaptation layer that shields plugins from framework changes. In this paper we make our approach practical. Two case studies of unconstrained API evolution show that our approach scales in a large number of adaptation scenarios and comparing to other adaptation techniques. The evaluation of our logic-based tool ComeBack! demonstrates that it can adapt efficiently most of the problem-causing API refactorings.},
  Doi                      = {10.1145/1449913.1449939},
  Key                      = {Savga et al.}
}

@Article{2005:tse:cubranic,
  Title                    = {{H}ipikat: A Project Memory for Software Development},
  Author                   = {Davor {\v{C}}ubrani{\'c} and Gail C. Murphy and Janice Singer and Kellogg S. Booth},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {446--465},
  Volume                   = {31},

  Abstract                 = {Sociological and technical difficulties, such as a lack of informal encounters, can make it difficult for new members of noncollocated software development teams to learn from their more experienced colleagues. To address this situation, we have developed a tool, named Hipikat that provides developers with efficient and effective access to the group memory for a software development project that is implicitly formed by all of the artifacts produced during the development. This project memory is built automatically with little or no change to existing work practices. After describing the Hipikat tool, we present two studies investigating Hipikat's usefulness in software modification tasks. One study evaluated the usefulness of Hipikat's recommendations on a sample of 20 modification tasks performed on the Eclipse Java IDE during the development of release 2.1 of the Eclipse software. We describe the study, present quantitative measures of Hipikat's performance, and describe in detail three cases that illustrate a range of issues that we have identified in the results. In the other study, we evaluated whether software developers who are new to a project can benefit from the artifacts that Hipikat recommends from the project memory. We describe the study, present qualitative observations, and suggest implications of using project memory as a learning aid for project newcomers.},
  Doi                      = {10.1109/TSE.2005.71},
  Key                      = {Cubranic et al.}
}

@Article{1994:aicom:aamodt,
  Title                    = {Case-based reasoning: foundational issues, methodological variations, and system approaches},
  Author                   = {Aamodt, Agnar and Plaza, Enric},
  Journal                  = aicom,
  Year                     = {1994},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {39--59},
  Volume                   = {7},

  ISSN                     = {0921-7126},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.9093&rep=rep1&type=pdf}
}

@Book{1996:book:abrial,
  Title                    = {The B-book: Assigning Programs to Meanings},
  Author                   = {Jean-Raymond Abrial},
  Publisher                = {Cambridge University Press},
  Year                     = {1996},

  Address                  = {Cambridge, UK},
  Month                    = oct
}

@InCollection{1980:book:mckeag:abrial,
  Title                    = {Specification Language},
  Author                   = {J.-R. Abrial and S. A. Schuman and B. Meyer},
  Booktitle                = {On the Construction of Programs},
  Publisher                = {Cambridge University Press},
  Year                     = {1980},

  Address                  = {Cambridge, UK},
  Editor                   = {R. M. McKeag and A. M. Macnaughten},
  Month                    = nov,
  Pages                    = {343--410},

  Abstract                 = {The concept of specification language is now widely spread; formalising a problem is well recognised as a necessary step preceding any programming. The formalisation technique, however, is still the purpose for intensive research: the number of proposals in this field is a sufficient account of this fact. But a few basic principles seem to emerge and be generally agreed upon: using a strict formalism inherited from mathematical practice; recognising the set theory as a sound basis for the formalisation; necessity of strong structuring of the formal text. The proposed language takes its inspiration from these principles; it is especially indebted to the effort made within the last fifty years to present mathematical works in a satisfactory way.},
  Url                      = {se.ethz.ch/~meyer/publications/languages/Z_original.pdf}
}

@InProceedings{2007:esec_fse:acharya,
  Title                    = {Mining {API} patterns as partial orders from source code: From usage scenarios to specifications},
  Author                   = {Acharya, Mithun and Xie, Tao and Pei, Jian and Xu, Jun},
  Booktitle                = esec_fse,
  Year                     = {2007},
  Pages                    = {25--34}
}

@InProceedings{2009:csmr:ackermann,
  Title                    = {Redesign for Flexibility and Maintainability: A Case Study},
  Author                   = {Ackermann, Christopher and Lindvall, Mikael and Dennis, Greg},
  Booktitle                = csmr,
  Year                     = {2009},
  Pages                    = {259--262},

  Abstract                 = {In this paper, we analyze software that we inherited from another party. We analyze its architecture and use common design principles to identify critical changes in order to improve its flexibility with respect to a set of planned extensions. We describe flexibility issues that we encountered and how they were addressed by a redesign and re-implementation. The study shows that basic and well-established design concepts can be used to guide the design and redesign of software.},
  Doi                      = {10.1109/CSMR.2009.60}
}

@InCollection{2010:book:aggarwal:aggarwal,
  Title                    = {A Survey of Clustering Algorithms for Graph Data},
  Author                   = {Charu C. Aggarwal and Haixun Wang},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {9},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {275--301},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {In this chapter, we will provide a survey of clustering algorithms for graph data. We will discuss the different categories of clustering algorithms and recent ef- forts to design clustering methods for various kinds of graphical data. Clustering algorithms are typically of two types. The first type consists of node clustering algorithms in which we attempt to determine dense regions of the graph based on edge behavior. The second type consists of structural clustering algorithms, in which we attempt to cluster the different graphs based on overall structural behavior. We will also discuss the applicability of the approach to other kinds of data such as semi-structured data, and the utility of graph mining algorithms to such representations.},
  Doi                      = {10.1007/978-1-4419-6045-0_9}
}

@InProceedings{1993:sigmod:agrawal,
  Title                    = {Mining association rules between sets of items in large databases},
  Author                   = {Rakesh Agrawal and Tomasz Imielinski and Arun Swami},
  Booktitle                = sigmod,
  Year                     = {1993},
  Pages                    = {207--216},

  Abstract                 = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
  Doi                      = {10.1145/170036.170072}
}

@Article{2011:software:agresti,
  Title                    = {Software Reuse: Developers' Experiences and Perceptions},
  Author                   = {Agresti, William W.},
  Journal                  = jsea,
  Year                     = {2011},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {48--58},
  Volume                   = {4},

  Doi                      = {10.4236/jsea.2011.41006}
}

@InProceedings{2000:oopsla:aguiar,
  Title                    = {A Minimalist Approach to Framework Documentation},
  Author                   = {Ademar Aguiar},
  Booktitle                = oopslaadd,
  Year                     = {2000},
  Pages                    = {143--144},

  Abstract                 = {Good documentation is crucial for the success of frameworks. In this research, a new documenting approach is proposed combining existing document styles in a kind of ``minimalist" framework manual with a special emphasis on framework understandability and usability, rather than on describing framework design. Benefits and drawbacks are evaluated from frameworks of different domains and complexity.},
  Doi                      = {10.1145/367845.368050}
}

@Article{2006:bib:aittokallio,
  Title                    = {Graph-based methods for analysing networks in cell biology},
  Author                   = {Tero Aittokallio and Benno Schwikowski},
  Journal                  = bib,
  Year                     = {2006},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {243--255},
  Volume                   = {7},

  Abstract                 = {Availability of large-scale experimental data for cell biology is enabling computational methods to systematically model the behaviour of cellular networks. This review surveys the recent advances in the field of graph-driven methods for analysing complex cellular networks.The methods are outlined on three levels of increasing complexity, ranging from methods that can characterize global or local structural properties of networks to methods that can detect groups of interconnected nodes, called motifs or clusters, potentially involved in common elementary biological functions. We also briefly summarize recent approaches to data integration and network inference through graph-based formalisms. Finally, we highlight some challenges in the field and offer our personal view of the key future trends and developments in graph-based analysis of large-scale datasets.},
  Doi                      = {10.1093/bib/bbl022}
}

@Article{2007:jss:ajila,
  Title                    = {Empirical study of the effects of open source adoption on software development economics},
  Author                   = {Samuel A. Ajila and Di Wu},
  Journal                  = jss,
  Year                     = {2007},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {1517--1529},
  Volume                   = {80},

  Abstract                 = {In this paper, we present the results of empirical study of the effects of open source software (OSS) components reuse on software development economics. Specifically, we examined three economic factors---cost, productivity, and quality. This study started with an extensive literature review followed by an exploratory study conducted through interviews with 18 senior project/quality managers, and senior software developers. Then, the result of the literature review and the exploratory study was used to formulate research model, hypotheses, and survey questionnaire. Software intensive companies from Canada and the US were targeted for this study. The period of study was between September 2004 and March 2006. Our findings show that there are strong significant statistical correlations between the factors of OSS components reuse and software development economics. The conclusion from this study shows that software organizations can achieve some economic gains in terms of software development productivity and product quality if they implement OSS components reuse adoption in a systematic way. A big lesson learned in this study is that OSS components are of highest quality and that open source community is not setting a bad example (contrary to some opinion) so far as ``good practices'' are concerned.},
  Doi                      = {10.1016/j.jss.2007.01.011}
}

@Article{2005:jsmerp:aldallal,
  Title                    = {Reusing class-based test cases for testing object-oriented framework interface classes},
  Author                   = {Al Dallal, Jehad and Paul Sorenson},
  Journal                  = jsmerp,
  Year                     = {2005},

  Month                    = may # {/} # jun,
  Number                   = {3},
  Pages                    = {169--196},
  Volume                   = {17},

  Abstract                 = {An application framework provides a reusable design and implementation for a family of software systems. Frameworks are introduced to reduce the cost of a product line (i.e., family of products that share the common features) and to increase the maintainability of software products through the deployment of reliable large-scale reusable components. A key challenge with frameworks is the development, evolution and maintenance of test cases to ensure the framework operates appropriately in a given application or product. Reusable test cases increase the maintainability of the software products because an entirely new set of test cases does not have to be generated each time the framework is deployed. At the framework deployment stage, the application developers (i.e., framework users) may need the flexibility to ignore or modify part of the specification used to generate the reusable class-based test cases. This paper addresses how to deal effectively with the different modification forms such that the use of the test cases becomes easy and straightforward in testing the framework interface classes (FICs) developed at the application development stage. Finally, the paper discusses the fault coverage and experimentally examines the specification coverage of the reusable test cases.},
  Doi                      = {10.1002/smr.v17:3}
}

@InProceedings{1999:fdo:albert,
  Title                    = {A Transparent Method for Correlating Profiles with Source Programs},
  Author                   = {Eugene Albert},
  Booktitle                = fdo,
  Year                     = {1999},
  Pages                    = {4:1--4:7},

  Abstract                 = {Feedback directed optimizations can improve the performance of a wide range of applications. However, using feedback to build large applications is cumbersome. Applications must be built twice, once to generate the executable from which the profile is to be generated and once to make use of the profile. Furthermore, the executable to be profiled must be built in a manner that allows the compiler to relate the profile to the basic block structure of the source program. For example, the executable to be profiled may have to be built without optimization. This alone can be enough to discourage the use of feedback altogether in a production environment. This paper describes a transparent technique used in Compaq's GEM compiler back-end [1], which greatly simplifies profile correlation. GEM is the common optimizer and code generator used by Compaq's C, C++, and Fortran Alpha [2] compilers. With this method, GEM can make use of profiles generated from fully optimized, production quality applications. This eliminates the need for a special build, making feedback optimization easier to use. The technique is transparent in the sense that it does not perturb the code generated by the compiler.},
  Url                      = {http://cseweb.ucsd.edu/~calder/fdo/fdo2/papers/fdo2-albert.ps}
}

@Article{2005:jcs:albert,
  Title                    = {Scale-free networks in cell biology},
  Author                   = {R{\'e}ka Albert},
  Journal                  = jcs,
  Year                     = {2005},

  Month                    = {1 } # nov,
  Number                   = {21},
  Pages                    = {4947--4957},
  Volume                   = {118},

  Abstract                 = {A cell's behavior is a consequence of the complex interactions between its numerous constituents, such as DNA, RNA, proteins and small molecules. Cells use signaling pathways and regulatory mechanisms to coordinate multiple processes, allowing them to respond to and adapt to an ever-changing environment. The large number of components, the degree of interconnectivity and the complex control of cellular networks are becoming evident in the integrated genomic and proteomic analyses that are emerging. It is increasingly recognized that the understanding of properties that arise from whole-cell function require integrated, theoretical descriptions of the relationships between different cellular components. Recent theoretical advances allow us to describe cellular network structure with graph concepts and have revealed organizational features shared with numerous non-biological networks. We now have the opportunity to describe quantitatively a network of hundreds or thousands of interacting components. Moreover, the observed topologies of cellular networks give us clues about their evolution and how their organization influences their function and dynamic responses.},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/16254242}
}

@Article{2002:rmp:albert,
  Title                    = {Statistical mechanics of complex networks},
  Author                   = {R{\'e}ka Albert and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = rmp,
  Year                     = {2002},

  Month                    = jan # {--} # mar,
  Number                   = {1},
  Pages                    = {47--97},
  Volume                   = {74},

  Abstract                 = {Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the network's robustness against failures and attacks.},
  Doi                      = {10.1103/RevModPhys.74.47}
}

@Article{2000:prl:albert:a,
  Title                    = {Dynamics of Complex Systems: Scaling Laws for the Period of Boolean Networks},
  Author                   = {R{\'e}ka Albert and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = prl,
  Year                     = {2000},

  Month                    = {12 } # jun,
  Number                   = {24},
  Pages                    = {5660--5663},
  Volume                   = {84},

  Abstract                 = {Boolean networks serve as models for complex systems, such as social or genetic networks, where each vertex, based on inputs received from selected vertices, makes its own decision about its state. Despite their simplicity, little is known about the dynamical properties of these systems. Here we propose a method to calculate the period of a finite Boolean system, by identifying the mechanisms determining its value. The proposed method can be applied to systems of arbitrary topology, and can serve as a roadmap for understanding the dynamics of large interacting systems in general.},
  Doi                      = {10.1103/PhysRevLett.84.5660}
}

@Article{2000:prl:albert:b,
  Title                    = {Topology of evolving networks: Local events and universality},
  Author                   = {R{\'e}ka Albert and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = prl,
  Year                     = {2000},

  Month                    = {11 } # dec,
  Number                   = {24},
  Pages                    = {5234--5237},
  Volume                   = {85},

  Abstract                 = {Networks grow and evolve by local events, such as the addition of new nodes and links, or rewiring of links from one node to another. We show that depending on the frequency of these processes two topologically different networks can emerge, the connectivity distribution following either a generalized power law or an exponential. We propose a continuum theory that predicts these two regimes as well as the scaling function and the exponents, in good agreement with numerical results. Finally, we use the obtained predictions to fit the connectivity distribution of the network describing the professional links between movie actors.},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/11102229}
}

@Article{2000:nature:albert,
  Title                    = {Error and attack tolerance of complex networks},
  Author                   = {R{\'e}ka Albert and Hawoong Jeong and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = nature,
  Year                     = {2000},

  Month                    = {27 } # jul,
  Number                   = {6794},
  Pages                    = {378--382},
  Volume                   = {406},

  Abstract                 = {Many complex systems display a surprising degree of tolerance against errors. For example, relatively simple organisms grow, persist and reproduce despite drastic pharmaceutical or environmental interventions, an error tolerance attributed to the robustness of the underlying metabolic network. Complex communication networks display a surprising degree of robustness: although key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these and other complex systems is often attributed to the redundant wiring of the functional web defined by the systems' components. Here we demonstrate that error tolerance is not shared by all redundant systems: it is displayed only by a class of inhomogeneously wired networks, called scale-free networks, which include the World-Wide Web, the Internet, social networks and cells. We find that such networks display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected even by unrealistically high failure rates. However, error tolerance comes at a high price in that these networks are extremely vulnerable to attacks (that is, to the selection and removal of a few nodes that play a vital role in maintaining the network's connectivity). Such error tolerance and attack vulnerability are generic properties of communication networks.},
  Doi                      = {10.1038/35019019}
}

@Article{1999:nature:albert,
  Title                    = {Diameter of the {W}orld {W}ide {W}eb},
  Author                   = {R{\'e}ka Albert and Hawoong Jeong and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = nature,
  Year                     = {1999},

  Month                    = {9 } # sep,
  Number                   = {6749},
  Pages                    = {130--131},
  Volume                   = {401},

  Abstract                 = {Despite its increasing role in communication, the World-Wide Web remains uncontrolled: any individual or institution can create a website with any number of documents and links. This unregulated growth leads to a huge and complex web, which becomes a large directed graph whose vertices are documents and whose edges are links (URLs) that point from one document to another. The topology of this graph determines the web's connectivity and consequently how effectively we can locate information on it. But its enormous size (estimated to be at least $8 \times 10^8$ documents) and the continual changing of documents and links make it impossible to catalogue all the vertices and edges.},
  Doi                      = {10.1038/43601}
}

@Article{2005:netmath:alderson,
  Title                    = {Towards a theory of scale-free graphs: Definition, properties, and implications},
  Author                   = {David Alderson and John C. Doyle and Lun Li and Walter Willinger},
  Journal                  = netmath,
  Year                     = {2005},
  Number                   = {4},
  Pages                    = {431--523},
  Volume                   = {2},

  Abstract                 = {There is a large, popular, and growing literature on ``scale-free" networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks.},
  Url                      = {http://projecteuclid.org/euclid.im/1150477667}
}

@InProceedings{2002:icse:aldrich,
  Title                    = {{ArchJava}: Connecting software architecture to implementation},
  Author                   = {Aldrich, Jonathan and Chambers, Craig and Notkin, David},
  Booktitle                = icse,
  Year                     = {2002},
  Pages                    = {187--197},

  Abstract                 = {Software architecture describes the structure of a system, enabling more effective design, program understanding, and formal analysis. However, existing approaches decouple implementation code from architecture, allowing inconsistencies, causing confusion, violating architectural properties, and inhibiting software evolution. ArchJava is an extension to Java that seamlessly unifies software architecture with implementation, ensuring that the implementation conforms to architectural constraints. A case study applying ArchJava to a circuit-design application suggests that ArchJava can express architectural structure effectively within an implementation, and that it can aid in program understanding and software evolution.},
  Doi                      = {10.1145/581339.581365}
}

@Book{1964:book:alexander,
  Title                    = {Notes on the Synthesis of Form},
  Author                   = {Christopher Alexander},
  Publisher                = {Harvard University Press},
  Year                     = {1964}
}

@InProceedings{1999:metrics:allen,
  Title                    = {Measuring coupling and cohesion: An information-theory approach},
  Author                   = {Edward B. Allen and Taghi M. Khoshgoftaar},
  Booktitle                = metrics,
  Year                     = {1999},
  Pages                    = {119--129},

  Abstract                 = {The design of software is often depicted by graphs that show components and their relationships. For example, a structure chart shows the calling relationships among components. Object-oriented design is based on various graphs, as well. Such graphs are abstractions of the software, devised to depict certain design decisions. Coupling and cohesion are attributes that summarizes the degree of interdependence or connectivity among subsystems and within subsystems, respectively. When used in conjunction with measures of other attributes, coupling and cohesion can contribute to an assessment or prediction of software quality. Let a graph be an abstraction of a software system and let a sub-graph represent a module (subsystem).This paper proposes information theory-based measures of coupling and cohesion of a modular system. These measures have the properties of system-level coupling and cohesion defined by Briand, Morasca, and Basili. Coupling is based on relationships between modules.We also propose a similar measure for intra-module coupling based on an intra-module abstraction of the software, rather than inter-module, but intra-module coupling is calculated in the same way as inter-module coupling. We define cohesion in terms of intra-module coupling, normalized to between zero and one. We illustrate the measures with example graphs. Preliminary analysis showed that the information-theory approach has finer discrimination than counting.},
  Doi                      = {10.1109/METRIC.1999.809733}
}

@Book{2009:book:alpaydin,
  Title                    = {Introduction to Machine Learning},
  Author                   = {Alpaydin, Ethem},
  Publisher                = {MIT Press},
  Year                     = {2009},
  Edition                  = {2nd},

  Abstract                 = {The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.}
}

@Article{2003:tse:alshayeb,
  Title                    = {An empirical validation of object-oriented metrics in two different iterative software processes},
  Author                   = {Mohammad Alshayeb and Wei Li},
  Journal                  = tse,
  Year                     = {2003},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {1043--1049},
  Volume                   = {29},

  Abstract                 = {Object-oriented (OO) metrics are used mainly to predict software engineering activities/efforts such as maintenance effort, error proneness, and error rate. There have been discussions about the effectiveness of metrics in different contexts. In this paper, we present an empirical study of OO metrics in two iterative processes: the short-cycled agile process and the long-cycled framework evolution process. We find that OO metrics are effective in predicting design efforts and source lines of code added, changed, and deleted in the short-cycled agile process and ineffective in predicting the same aspects in the long-cycled framework process. This leads us to believe that OO metrics' predictive capability is limited to the design and implementation changes during the development iterations, not the long-term evolution of an established system in different releases.},
  Doi                      = {10.1109/TSE.2003.1245305}
}

@InCollection{1998:book:lenz:althoff,
  Title                    = {{CBR} for Experimental Software Engineering},
  Author                   = {Althoff, Klaus-Dieter and Andreas Birk and Gresse von Wangenheim, Christiane and Carsten Tautz},
  Booktitle                = {Case-Based Reasoning Technology: From Foundations to Applications},
  Publisher                = {Springer},
  Year                     = {1998},
  Editor                   = {Mario Lenz and Hans-Dieter Burkhard and Brigitte Bartsch-Sp{\"o}rl and Stefan Wess},
  Pages                    = {235--254},
  Series                   = lncs,
  Volume                   = {1400},

  Abstract                 = {The objective of our work is to exploit the mutual interrelations between case-based reasoning and experimental software engineering (ESE) for the sake of both fields. In particular, we address the following topics: - Presentation of a logical infrastructure of organizational learning in the software domain that makes use of principles, methods, and established practices from CBR. - Utilization of CBR technology for implementing experience bases. - Outlining of additional uses of CBR for ESE that go beyond infrastructures for organizational learning (i.e., data analysis, problem solving, etc.). - Evolution of a methodology for developing CBR-based software systems based on software engineering methods and practices that are established in other application domains. These topics are gradually developed throughout this chapter. We start with brief introductions to software engineering and ESE (Sections 9.2 and 9.3). Following this, we outline possible CBR support for ESE and give an overview of research related to both CBR and ESE (Sections 9.4 and 9.5). In the main part of this chapter, we describe the approach we take at the Fraunhofer Institute for Experimental Software Engineering in Kaiserslautern (Section 9.6). We also report on two case studies and give an outlook on work still to be done (Sections 9.7 and 9.8).},
  Doi                      = {10.1007/3-540-69351-3_9}
}

@InProceedings{2006:vlhcc:de_alwis,
  Title                    = {Using visual momentum to explain disorientation in the {E}clipse {IDE}},
  Author                   = {Brian de Alwis and Gail C. Murphy},
  Booktitle                = vlhcc,
  Year                     = {2006},
  Pages                    = {51--54},

  Abstract                 = {We report on a field study about how software developers experience disorientation when using the Eclipse Java integrated development environment. We analyzed the data using the theory of visual momentum, identifying three factors that may lead to disorientation: the absence of connecting navigation context during program exploration, thrashing between displays to view necessary pieces of code, and the pursuit of sometimes unrelated subtasks.},
  Doi                      = {10.1109/VLHCC.2006.49}
}

@Article{2000:pnas:amaral,
  Title                    = {Classes of small-world networks},
  Author                   = {L. A. N. Amaral and A. Scala and M. Barth{\'e}l{\'e}my and H. E. Stanley},
  Journal                  = pnas,
  Year                     = {2000},

  Month                    = {10 } # oct,
  Number                   = {21},
  Pages                    = {11149--11152},
  Volume                   = {97},

  Abstract                 = {We study the statistical properties of a variety of diverse real-world networks. We present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. Moreover, we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. Our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks.},
  Doi                      = {10.1073/pnas.200327197}
}

@Article{2004:informatica:ambriola,
  Title                    = {Transformations for Architectural Restructuring},
  Author                   = {Ambriola, Vicenzo and Alina Kmiecik},
  Journal                  = informatica,
  Year                     = {2004},

  Month                    = jul,
  Number                   = {2},
  Pages                    = {117--128},
  Volume                   = {28},

  Abstract                 = {Model-driven engineering reaches more and more followers and gradually grows up as an incoming so-lution to the software-intensive systems production. Architectural modeling (or design) seems to play a fundamental role in this kind of development not only because of its very nature but also because of its impact to the final product structure and behavior as well as the user requirements satisfaction. Incremental and iterative character of architectural design sets a special attention to the aspects of architec-ture model restructuring, where architectural transformations are the key architect's instrument for introducing quality dedicated changes. Since architectural design constantly grows in complication be-cause of systems complexity, there is a substantial need to support the architect during model architectural transformations. This paper presents our efforts in defining model-level architectural transformations. It provides a definition and a classification of architectural transformations and describes the semantics of three selected transformations: for component moving, for component splitting and for class splitting. To provide some view of transformations definition complexity it lists informal description for T\_SPLIT\_CLASS pre-conditions and post-conditions and presents their formal OCL documentation in Appendix. An example of employing the transformations to improve the architecture of an industrial Geographic Information Web System (WebGIS) is also given.},
  Url                      = {http://www.informatica.si/PDF/Informatica_2004_2.pdf}
}

@InProceedings{2002:seke:ambriola,
  Title                    = {Architectural transformations},
  Author                   = {Ambriola, Vincenzo and Kmiecik, Alina},
  Booktitle                = seke,
  Year                     = {2002},
  Pages                    = {275--278},

  Abstract                 = {The first draft of a software architecture almost never constitutes the final picture of the system to be developed. In most cases it misses some of required properties and needs to be improved or even completely rebuilt. The software architect applies architectural transformations in order to ``repair" the system structure. In this paper we present our approach to software architecture and architectural transformations. We also discuss three related issues: ADLs, architectural styles and non-functional requirements. Some arguments for architectural change automation are also given.},
  Doi                      = {10.1145/568760.568809}
}

@Article{2005:upgrade:amor-iglesias,
  Title                    = {Measuring libre software using {D}ebian 3.1 ({S}arge) as a case study: {P}reliminary results},
  Author                   = {Juan-Jos{\'e} Amor-Iglesias and Jes{\'u}s M. Gonz{\'a}lez-Barahona and Gregorio Robles-Martinez and Israel Herr{\'a}iz-Tabernero},
  Journal                  = upgrade,
  Year                     = {2005},

  Month                    = jun,
  Number                   = {3},
  Pages                    = {13--16},
  Volume                   = {6},

  Abstract                 = {The Debian operating system is one of the most popular GNU/Linux distributions, not only among end users but also as a basis for other systems. Besides being popular, it is also one of the largest software compilations and thus a good starting point from which to analyse the current state of libre (free, open source) software. This work is a preliminary study of the new Debian GNU/Linux release (3.1, codenamed Sarge) which was officially announced recently. In it we show the size of Debian in terms of lines of code (close to 230 million source lines of code), the use of the various programming languages in which the software has been written, and the size of the packages included within the distribution. We also apply a `classical' and well-known cost estimation method which gives an idea of how much it would cost to create something on the scale of Debian from scratch (over 8 billion USD).},
  Url                      = {http://www.cepis.org/upgrade/files/full-2005-III.pdf}
}

@InProceedings{2003:itwp:anand,
  Title                    = {Intelligent Techniques for Web Personalization},
  Author                   = {Sarabjot Singh Anand and Bamshad Mobasher},
  Booktitle                = itwp,
  Year                     = {2003},
  Pages                    = {1--36},
  Series                   = lncs,
  Volume                   = {3169},

  Abstract                 = {In this chapter we provide a comprehensive overview of the topic of Intelligent Techniques for Web Personalization. Web Personalization is viewed as an application of data mining and machine learning techniques to build models of user behaviour that can be applied to the task of predicting user needs and adapting future interactions with the ultimate goal of improved user satisfaction. This chapter survey's the state-of-the-art in Web personalization. We start by providing a description of the personalization process and a classification of the current approaches to Web personalization. We discuss the various sources of data available to personalization systems, the modelling approaches employed and the current approaches to evaluating these systems. A number of challenges faced by researchers developing these systems are described as are solutions to these challenges proposed in literature. The chapter concludes with a discussion on the open challenges that must be addressed by the research community if this technology is to make a positive impact on user satisfaction with the Web.},
  Doi                      = {10.1007/11577935_1}
}

@Article{2010:ijase:andersen,
  Title                    = {Generic patch inference},
  Author                   = {Andersen, Jesper and Lawall, Julia L.},
  Journal                  = ijase,
  Year                     = {2010},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {119--148},
  Volume                   = {17},

  Abstract                 = {A key issue in maintaining Linux device drivers is the need to keep them up to date with respect to evolutions in Linux internal libraries. Currently, there is little tool support for performing and documenting such changes. In this paper we present a tool, spdiff, that identifies common changes made in a set of files and their updated versions, and extracts a generic patch performing those changes. Library developers can use our tool to extract a generic patch based on the result of manually updating a few typical driver files, and then apply this generic patch to other drivers. Driver developers can use it to extract an abstract representation of the set of changes that others have made. Our experiments on recent changes in Linux show that the inferred generic patches are more concise than the corresponding patches found in commits to the Linux source tree while being safe with respect to the changes performed in the provided driver files.},
  Doi                      = {10.1007/s10515-010-0062-z}
}

@InProceedings{2005:wcre:andreopoulos,
  Title                    = {Multiple Layer Clustering of Large Software Systems},
  Author                   = {Andreopoulos, Bill and An, Aijun and Tzerpos, Vassilios and Wang, Xiaogang},
  Booktitle                = wcre,
  Year                     = {2005},
  Pages                    = {79--88},

  Abstract                 = {Software clustering algorithms presented in the literature rarely incorporate in the clustering process dynamic information, such as the number of function invocations during runtime. Moreover, the structure of a software system is often multi-layered, while existing clustering algorithms often create flat system decompositions. This paper presents a software clustering algorithm called MULICsoft that incorporates in the clustering process both static and dynamic information. MULICsoft produces layered clusters with the core elements of each cluster assigned to the top layer. We present experimental results of applying MULICsoft to a large open-source system. Comparison with existing software clustering algorithms indicates that MULICsoft is able to produce decompositions that are close to those created by system experts.},
  Doi                      = {10.1109/WCRE.2005.24}
}

@Article{2005:tse:andritsos,
  Title                    = {Information-theoretic software clustering},
  Author                   = {Andritsos, Periklis and Tzerpos, Vassilios},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {150--165},
  Volume                   = {31},

  Abstract                 = {The majority of the algorithms in the software clustering literature utilize structural information to decompose large software systems. Approaches using other attributes, such as file names or ownership information, have also demonstrated merit. At the same time, existing algorithms commonly deem all attributes of the software artifacts being clustered as equally important, a rather simplistic assumption. Moreover, no method that can assess the usefulness of a particular attribute for clustering purposes has been presented in the literature. In this paper, we present an approach that applies information theoretic techniques in the context of software clustering. Our approach allows for weighting schemes that reflect the importance of various attributes to be applied. We introduce {LIMBO}, a scalable hierarchical clustering algorithm based on the minimization of information loss when clustering a software system. We also present a method that can assess the usefulness of any nonstructural attribute in a software clustering context. We applied {LIMBO} to three large software systems in a number of experiments. The results indicate that this approach produces clusterings that come close to decompositions prepared by system experts. Experimental results were also used to validate our usefulness assessment method. Finally, we experimented with well-established weighting schemes from information retrieval, Web search, and data clustering. We report results as to which weighting schemes show merit in the decomposition of software systems.},
  Doi                      = {10.1109/TSE.2005.25}
}

@InProceedings{2011:csmr:anquetil,
  Title                    = {Legacy Software Restructuring: Analyzing a Concrete Case},
  Author                   = {Anquetil, Nicolas and Laval, Jannik},
  Booktitle                = csmr,
  Year                     = {2011},
  Pages                    = {279--286},

  Abstract                 = {Software re-modularization is an old preoccupation of reverse engineering research. The advantages of a well structured or modularized system are well known. Yet after so much time and efforts, the field seems unable to come up with solutions that make a clear difference in practice. Recently, some researchers started to question whether some basic assumptions of the field were not overrated. The main one consists in evaluating the high-cohesion/low-coupling dogma with metrics of unknown relevance. In this paper, we study a real structuring case (on the Eclipse platform) to try to better understand if (some) existing metrics would have helped the software engineers in the task. Results show that the cohesion and coupling metrics used in the experiment did not behave as expected and would probably not have helped the maintainers reach there goal. We also measured another possible restructuring which is to decrease the number of cyclic dependencies between modules. Again, the results did not meet expectations.},
  Doi                      = {10.1109/CSMR.2011.34}
}

@InProceedings{2008:cascon:antoniol,
  Title                    = {Is it a bug or an enhancement?: A text-based approach to classify change requests},
  Author                   = {Antoniol, Giuliano and Ayari, Kamel and Di Penta, Massimiliano and Khomh, Foutse and Gu{\'e}h{\'e}neuc, Yann-Ga\"{e}l},
  Booktitle                = cascon,
  Year                     = {2008},
  Pages                    = {23:304--23:318},

  Abstract                 = {Bug tracking systems are valuable assets for managing maintenance activities. They are widely used in open-source projects as well as in the software industry. They collect many different kinds of issues: requests for defect fixing, enhancements, refactoring/restructuring activities and organizational issues. These different kinds of issues are simply labeled as ``bug" for lack of a better classification support or of knowledge about the possible kinds. This paper investigates whether the text of the issues posted in bug tracking systems is enough to classify them into corrective maintenance and other kinds of activities. We show that alternating decision trees, naive Bayes classifiers, and logistic regression can be used to accurately distinguish bugs from other kinds of issues. Results from empirical studies performed on issues for Mozilla, Eclipse, and JBoss indicate that issues can be classified with between 77\% and 82\% of correct decisions.},
  Doi                      = {10.1145/1463788.1463819}
}

@InProceedings{2005:iwpse:antoniol,
  Title                    = {Detecting groups of co-changing files in {CVS} repositories},
  Author                   = {Antoniol, Giuliano and Rollo, Vincenzo Fabio and Venturi, Gabriele},
  Booktitle                = iwpse,
  Year                     = {2005},
  Pages                    = {23--32},

  Abstract                 = {Software systems continuously evolve. CVS record almost all of the changes the system parts undergo. Hence, CVS repositories contain a great deal of information about software artifact evolution. Software artifacts of a system can evolve following similar evolution patterns as well as very different ones. A peculiar kind of similarity in evolution is the one among two or more artifacts having changed almost at the same times for a certain number of changes. We name these co-changing artifacts. Co-changing artifacts are relevant because cochanges can be inducted by not trivial dependencies among system parts. In this paper, we propose a definition of co-changes suitable of practical application. We assess the challenges arising in detection of groups of co-changing software parts, and we present a robust approach, based on Dynamic Time Warping, to detect groups of co-changing files in CVS repositories. We also report and discuss the results of a preliminary application of the approach to the Mozilla CVS repository.},
  Doi                      = {10.1109/IWPSE.2005.11}
}

@InProceedings{2006:icse:anvik,
  Title                    = {Who should fix this bug?},
  Author                   = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
  Booktitle                = icse,
  Year                     = {2006},
  Pages                    = {361--370},

  Abstract                 = {Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57\% and 64\% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.},
  Doi                      = {10.1145/1134285.1134336}
}

@Article{2011:tosem:anvik,
  Title                    = {Reducing the effort of bug report triage: Recommenders for development-oriented decisions},
  Author                   = {Anvik, John and Murphy, Gail C.},
  Journal                  = tosem,
  Year                     = {2011},

  Month                    = aug,
  Number                   = {3},
  Pages                    = {10:1--10:35},
  Volume                   = {20},

  Abstract                 = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process. To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.},
  Doi                      = {10.1145/2000791.2000794}
}

@Article{2007:ijase:apiwattanapong,
  Title                    = {{JDiff}: A differencing technique and tool for object-oriented programs},
  Author                   = {Taweesup Apiwattanapong and Alessandro Orso and Mary Jean Harrold},
  Journal                  = ijase,
  Year                     = {2007},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {3--36},
  Volume                   = {14},

  Abstract                 = {During software evolution, information about changes between different versions of a program is useful for a number of software engineering tasks. For example, configuration-management systems can use change information to assess possible conflicts among updates from different users. For another example, in regression testing, knowledge about which parts of a program are unchanged can help in identifying test cases that need not be rerun. For many of these tasks, a purely syntactic differencing may not provide enough information for the task to be performed effectively. This problem is especially relevant in the case of object-oriented software, for which a syntactic change can have subtle and unforeseen effects. In this paper, we present a technique for comparing object-oriented programs that identifies both differences and correspondences between two versions of a program. The technique is based on a representation that handles object-oriented features and, thus, can capture the behavior of object-oriented programs.We also present JDiff, a tool that implements the technique for Java programs. Finally, we present the results of four empirical studies, performed on many versions of two medium-sized subjects, that show the efficiency and effectiveness of the technique when used on real programs.},
  Doi                      = {10.1007/s10515-006-0002-0}
}

@InProceedings{2004:ase:apiwattanapong,
  Title                    = {A Differencing Algorithm for Object-Oriented Programs},
  Author                   = {Taweesup Apiwattanapong and Alessandro Orso and Mary Jean Harrold},
  Booktitle                = ase,
  Year                     = {2004},
  Pages                    = {2--13},

  Abstract                 = {During software evolution, information about changes between different versions of a program is useful for a number of software engineering tasks. For many of these tasks, a purely syntactic differencing may not provide enough information for the task to be performed effectively. This problem is especially relevant in the case of object-oriented software, for which a syntactic change can have subtle and unforeseen effects. We present a technique for comparing object-oriented programs that identifies both differences and correspondences between two versions of a program. The technique is based on a representation that handles object-oriented features and, thus, can capture the behavior of object-oriented programs. We also present JDIFF, a tool that implements the technique for Java programs, and empirical results that show the efficiency and effectiveness of the technique on a real program.},
  Doi                      = {10.1109/ASE.2004.1342719}
}

@InProceedings{2006:taicpart:apiwattanapong,
  Title                    = {{MATRIX}: Maintenance-Oriented Testing Requirements Identifier and Examiner},
  Author                   = {Apiwattanapong, Taweesup and Santelices, Raul and Chittimalli, Pavan Kumar and Orso, Alessandro and Harrold, Mary Jean},
  Booktitle                = taicpart,
  Year                     = {2006},
  Pages                    = {137--146},

  Abstract                 = {This paper presents a new test-suite augmentation technique for use in regression testing of software. Our technique combines dependence analysis and symbolic evaluation and uses information about the changes between two versions of a program to (1) identify parts of the program affected by the changes, (2) compute the conditions under which the effects of the changes are propagated to such parts, and (3) create a set of testing requirements based on the computed information. Testers can use these requirements to assess the effectiveness of the regression testing performed so far and to guide the selection of new test cases. The paper also presents MATRIX, a tool that partially implements our technique, and its integration into a regression-testing environment. Finally, the paper presents a preliminary empirical study performed on two small programs. The study provides initial evidence of both the effectiveness of our technique and the shortcomings of previous techniques in assessing the adequacy of a test suite with respect to exercising the effect of program changes.},
  Doi                      = {10.1109/TAIC-PART.2006.18}
}

@InProceedings{2009:icse:aranda,
  Title                    = {The secret life of bugs: Going past the errors and omissions in software repositories},
  Author                   = {Aranda, Jorge and Venolia, Gina},
  Booktitle                = icse,
  Year                     = {2009},
  Pages                    = {298--308},

  Abstract                 = {Every bug has a story behind it. The people that discover and resolve it need to coordinate, to get information from documents, tools, or other people, and to navigate through issues of accountability, ownership, and organizational structure. This paper reports on a field study of coordination activities around bug fixing that used a combination of case study research and a survey of software professionals. Results show that the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through automation of electronic repositories, and that such automation provides incomplete and often erroneous accounts of coordination. The paper uses rich bug histories and survey results to identify common bug fixing coordination patterns and to provide implications for tool designers and researchers of coordination in software development.},
  Doi                      = {10.1109/ICSE.2009.5070530}
}

@Article{2004:tse:arisholm,
  Title                    = {Dynamic coupling measurement for object-oriented software},
  Author                   = {Erik Arisholm and Lionel C. Briand and Audun F{\o}yen},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {491--506},
  Volume                   = {30},

  Abstract                 = {The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused (``dead'') code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at runtime. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Coupling measurement has traditionally been performed using static code analysis, because most of the existing work was done on nonobject oriented code and because dynamic code analysis is more expensive and complex to perform. For modern software systems, however, this focus on static analysis can be problematic because although dynamic binding existed before the advent of objectorientation, its usage has increased significantly in the last decade. This paper describes how coupling can be defined and precisely measured based on dynamic analysis of systems. We refer to this type of coupling as dynamic coupling. An empirical evaluation of the proposed dynamic coupling measures is reported in which we study the relationship of these measures with the change proneness of classes. Data from maintenance releases of a large Java system are used for this purpose. Preliminary results suggest that some dynamic coupling measures are significant indicators of change proneness and that they complement existing coupling measures based on static analysis.},
  Doi                      = {10.1109/TSE.2004.41}
}

@InProceedings{1993:csm:arnold,
  Title                    = {Impact Analysis: Towards a Framework for Comparison},
  Author                   = {R. S. Arnold and S. A. Bohner},
  Booktitle                = csm,
  Year                     = {1993},
  Pages                    = {292--301},

  Abstract                 = {The term impact analysis is used with many meanings. A three-part framework for characterizing and comparing diverse impact analysis approaches is defined. The parts correspond to how an approach is used to accomplish impact analysis, how an approach does impact analysis internally, and the effectiveness of the impact analysis approach. To illustrate the framework's application, five impact analysis approaches are illustrated according to it.},
  Doi                      = {10.1109/ICSM.1993.366933}
}

@InProceedings{2008:ecoop:artzi,
  Title                    = {{ReCrash}: Making software failures reproducible by preserving object states},
  Author                   = {Artzi, Shay and Kim, Sunghun and Ernst, Michael D.},
  Booktitle                = ecoop,
  Year                     = {2008},
  Pages                    = {542--565},

  Doi                      = {10.1007/978-3-540-70592-5_23}
}

@InProceedings{2010:icse:asuncion,
  Title                    = {Software traceability with topic modeling},
  Author                   = {Asuncion, Hazeline U. and Asuncion, Arthur U. and Taylor, Richard N.},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {95--104},
  Volume                   = {1},

  Abstract                 = {Software traceability is a fundamentally important task in software engineering. The need for automated traceability increases as projects become more complex and as the number of artifacts increases. We propose an automated technique that combines traceability with a machine learning technique known as topic modeling. Our approach automatically records traceability links during the software development process and learns a probabilistic topic model over artifacts. The learned model allows for the semantic categorization of artifacts and the topical visualization of the software system. To test our approach, we have implemented several tools: an artifact search tool combining keyword-based search and topic modeling, a recording tool that performs prospective traceability, and a visualization tool that allows one to navigate the software architecture and view semantic topics associated with relevant artifacts and architectural components. We apply our approach to several data sets and discuss how topic modeling enhances software traceability, and vice versa.},
  Doi                      = {10.1145/1806799.1806817}
}

@InProceedings{2007:hicss:asundi,
  Title                    = {Patch Review Processes in Open Source Software Development Communities: A Comparative Case Study},
  Author                   = {Asundi, Jai and Jayant, Rajiv},
  Booktitle                = hicss,
  Year                     = {2007},
  Pages                    = {166c:1--166c:7},

  Abstract                 = {In spite of the overwhelming success of Free/Open Source Software (F/OSS) like Apache and GNU/Linux, there is a limited understanding of the processes and methodologies that specify this form of software development. In this paper, we examine the process of patch reviews as a proxy for the extent of code-review in F/OSS projects. While existing descriptions of patch review processes are mostly narrative and based on individual experiences, we systematically analyze the email archives of five F/OSS projects to characterize this process. While doing so, we make a distinction between contributions (patches or review comments) by core members and casual contributors to grasp the role of core members in this process. Our results show that while the patch review processes are not exactly identical across various F/OSS projects, the core members across all projects play the vital role of gate-keepers to ensure a high level of review for submitted patches.},
  Doi                      = {10.1109/HICSS.2007.426}
}

@Article{2006:spe:atkinson,
  Title                    = {Effective pattern matching of source code using abstract syntax patterns},
  Author                   = {D. C. Atkinson and W. G. Griswold},
  Journal                  = spe,
  Year                     = {2006},

  Month                    = {10 } # apr,
  Number                   = {4},
  Pages                    = {413--447},
  Volume                   = {36},

  Abstract                 = {Program understanding can be assisted by tools that match patterns in the program source. Lexical pattern matchers provide excellent performance and ease of use, but have a limited vocabulary. Syntactic matchers provide more precision, but may sacrifice performance, robustness, or power. To achieve more of the benefits of both models, we extend the pattern syntax of AWK to support matching of abstract syntax trees, as demonstrated in a tool called TAWK. Its pattern syntax is language-independent, based on abstract tree patterns. As in AWK, patterns can have associated actions, which in TAWK are written in C for generality, familiarity, and performance. The use of C is simplified by high-level libraries and dynamic linking. To allow processing of program files containing non-syntactic constructs such as textual macros, mechanisms have been designed that allow matching of `language-like' macros in a syntactic fashion. We survey and apply prototypical approaches to concretely demonstrate the tradeoffs in program processing. Our results indicate that TAWK can be used to quickly and easily perform a variety of common software engineering tasks, and the extensions to accommodate non-syntactic features significantly extend the generality of syntactic matchers.},
  Doi                      = {10.1002/spe.704}
}

@InProceedings{2005:www:aula,
  Title                    = {Information search and re-access strategies of experienced web users},
  Author                   = {Aula, Anne and Jhaveri, Natalie and K\"{a}ki, Mika},
  Booktitle                = www,
  Year                     = {2005},
  Pages                    = {583--592},

  Abstract                 = {Experienced web users have strategies for information search and re-access that are not directly supported by web browsers or search engines. We studied how prevalent these strategies are and whether even experienced users have problems with searching and re-accessing information. With this aim, we conducted a survey with 236 experienced web users. The results showed that this group has frequently used key strategies (e.g., using several browser windows in parallel) that they find important, whereas some of the strategies that have been suggested in previous studies are clearly less important for them (e.g., including URLs on a webpage). In some aspects, such as query formulation, this group resembles less experienced web users. For instance, we found that most of the respondents had misconceptions about how their search engine handles queries, as well as other problems with information search and re-access. In addition to presenting the prevalence of the strategies and rationales for their use, we present concrete designs solutions and ideas for making the key strategies also available to less experienced users.},
  Doi                      = {10.1145/1060745.1060831}
}

@Article{2009:ietsoft:aversano,
  Title                    = {Relationship between design patterns defects and crosscutting concern scattering degree: An empirical study},
  Author                   = {L. Aversano and L. Cerulo and Di Penta, M.},
  Journal                  = ietsoft,
  Year                     = {2009},

  Month                    = oct,
  Number                   = {5},
  Pages                    = {395--409},
  Volume                   = {3},

  Abstract                 = {Design patterns are solutions to recurring design problems, aimed at increasing reuse, code quality and, above all, maintainability and resilience to changes. Despite such advantages, the usage of design patterns implies the presence of crosscutting code implementing the pattern usage and access from other system components. When the system evolves, the presence of crosscutting code can cause repeated changes, possibly introducing defects. This study reports an empirical study, in which it is showed that, for three open source projects, the number of defects in design-pattern classes is in several cases correlated with the scattering degree of their induced crosscutting concerns, and also varies among different kinds of patterns.},
  Doi                      = {10.1049/iet-sen.2008.0105}
}

@InCollection{2006:taosd:avgustinov,
  Title                    = {abc: An extensible {AspectJ} compiler},
  Author                   = {Avgustinov, Pavel and Christensen, Aske Simon and Hendren, Laurie and Kuzins, Sascha and Lhot{\'a}k, Jennifer and Lhot{\'a}k, Ond{\v{r}}ej and de Moor, Oege and Sereni, Damien and Sittampalam, Ganesh and Tibble, Julian},
  Booktitle                = {Transactions on Aspect-Oriented Software Development I},
  Publisher                = {Springer},
  Year                     = {2006},
  Pages                    = {293--334},
  Series                   = lncs,
  Volume                   = {3880},

  Doi                      = {10.1007/11687061_9}
}

@InProceedings{2009:wicsa_ecsa:axelsson,
  Title                    = {Evolutionary architecting of embedded automotive product lines: An industrial case study},
  Author                   = {Axelsson, Jakob},
  Booktitle                = wicsa_ecsa,
  Year                     = {2009},
  Pages                    = {101--110},

  Abstract                 = {In the automotive industry, embedded systems and software play an increasingly important role in defining the characteristics of the vehicles. Both the vehicles and the embedded systems are designed as product lines, and two distinct architecture processes can be identified. The revolutionary process develops the architecture of a new product line, and focuses on abstract quality attributes and flexibility. The evolutionary process continuously modifies the architecture due to changes, such as additions of new functionality. In this paper, the evolutionary process is investigated through a case study. The study reviews a number of changes to an existing architecture, observing the cause of the change, what quality attributes were considered, and what technical aspects were included. It is also analyzed how the interplay between the two processes can be improved through systematic feedback about what evolution actually takes place.},
  Doi                      = {10.1109/WICSA.2009.5290796}
}

@InProceedings{2007:cascon:ayari,
  Title                    = {Threats on building models from {CVS} and {B}ugzilla repositories: {T}he {M}ozilla case study},
  Author                   = {Ayari, K. and Meshkinfam, P. and Antoniol, G. and Di Penta, M.},
  Booktitle                = cascon,
  Year                     = {2007},
  Pages                    = {215--228},

  Abstract                 = {Information obtained by merging data extracted from problem reporting systems---such as Bugzilla---and versioning systems---such as Concurrent Version System (CVS)---is widely used in quality assessment approaches. This paper attempts to shed some light on threats and difficulties faced when trying to integrate information extracted from Mozilla CVS and bug repositories. Indeed, the heterogeneity of Mozilla bug reports, often dealing with non-defect issues, and lacking of traceable information may undermine validity of quality assessment approaches relying on repositories integration. In the reported Mozilla case study, we observed that available integration heuristics are unable to recover thousands of traceability links. Furthermore, Bugzilla classification mechanisms do not enforce a distinction between different kinds of maintenance activities. Obtained evidence suggests that a large amount of information is lost; we conjecture that to benefit from CVS and problem reporting systems, more systematic issue classification and more reliable traceability mechanisms are needed.},
  Doi                      = {10.1145/1321211.1321234}
}

@InProceedings{2011:simbad:aziz,
  Title                    = {Graph characterization via backtrackless paths},
  Author                   = {Aziz, Furqan and Wilson, Richard C. and Hancock, Edwin R.},
  Booktitle                = simbad,
  Year                     = {2011},
  Pages                    = {149--162},
  Series                   = lncs,
  Volume                   = {7005},

  Abstract                 = {Random walks on graphs have been extensively used for graph characterization. Positive kernels between labeled graphs have been proposed recently. In this paper we use backtrackless paths for gauging the similarity between graphs. We introduce efficient algorithms for characterizing both labeled and unlabeled graphs. First we show how to define efficient kernels based on backtrackless paths for labeled graphs. Second we show how the pattern vectors composed of backtrackless paths of different lengths can be use to characterize unlabeled graphs. The proposed methods are then applied to both labeled and unlabeled graphs.},
  Doi                      = {10.1007/978-3-642-24471-1_11}
}

@Article{2005:sosym:bezivin,
  Title                    = {On the unification power of models},
  Author                   = {B{\'{e}}zivin, Jean},
  Journal                  = sosym,
  Year                     = {2005},

  Month                    = may,
  Number                   = {2},
  Pages                    = {171--188},
  Volume                   = {4},

  Abstract                 = {In November 2000, the OMG made public the MDA initiative, a particular variant of a new global trend called MDE (Model Driven Engineering). The basic ideas of MDA are germane to many other approaches such as generative programming, domain specific languages, model-integrated computing, generic model management, software factories, etc. MDA may be defined as the realization of MDE principles around a set of OMG standards like MOF, XMI, OCL, UML, CWM, SPEM, etc. MDE is presently making several promises about the potential benefits that could be reaped from a move from code-centric to model-based practices. When we observe these claims, we may wonder when they may be satisfied: on the short, medium or long term or even never perhaps for some of them. This paper tries to propose a vision of the development of MDE based on some lessons learnt in the past 30 years in the development of object technology. The main message is that a basic principle (``Everything is an object'') was most helpful in driving the technology in the direction of simplicity, generality and power of integration. Similarly in MDE, the basic principle that ``Everything is a model'' has many interesting properties, among others the capacity to generate a realistic research agenda. We postulate here that two core relations (representation and conformance) are associated to this principle, as inheritance and instantiation were associated to the object unification principle in the class-based languages of the 80's. We suggest that this may be most useful in understanding many questions about MDE in general and the MDA approach in particular. We provide some illustrative examples. The personal position taken in this paper would be useful if it could generate a critical debate on the research directions in MDE.},
  Doi                      = {10.1007/s10270-005-0079-0}
}

@InProceedings{1991:rta:baader,
  Title                    = {Unification, weak unification, upper bound, lower bound, and generalization problems},
  Author                   = {Baader, Franz},
  Booktitle                = rta,
  Year                     = {1991},
  Pages                    = {86--97},
  Series                   = lncs,
  Volume                   = {448},

  Abstract                 = {We introduce E-unification, weak E-unification, E-upper bound, E-lower bound, and E-generalization problems, and the corresponding notions of unification, weak unification, upper bound, lower bound, and generalization type of an equational theory. When defining instantiation preorders on solutions of these problems, one can compared substitutions w.r.t. their behaviour on all variables or on finite sets of variables. We shall study the effect which these different instantiation preorders have on the existence of most general or most specific solutions of E-unification, weak E-unification, and E-generalization problems. In addition, we shall elucidate the subtle difference between most general unifiers and coequalizers, and we shall consider generalization in the class of commutative theories.},
  Doi                      = {10.1007/3-540-53904-2_88}
}

@Book{1864:book:babbage,
  Title                    = {Passages from the Life of a Philosopher},
  Author                   = {Charles Babbage},
  Publisher                = {Longman, Green, Longman, Roberts, \& Green},
  Year                     = {1864}
}

@InProceedings{2010:msr:bachmann,
  Title                    = {When process data quality affects the number of bugs: Correlations in software engineering datasets},
  Author                   = {Bachmann, Adrian and Bernstein, Abraham},
  Booktitle                = msr,
  Year                     = {2010},
  Pages                    = {62--71},

  Abstract                 = {Software engineering process information extracted from version control systems and bug tracking databases are widely used in empirical software engineering. In prior work, we showed that these data are plagued by quality deficiencies, which vary in its characteristics across projects. In addition, we showed that those deficiencies in the form of bias do impact the results of studies in empirical software engineering. While these findings affect software engineering researchers the impact on practitioners has not yet been substantiated. In this paper we, therefore, explore (i) if the process data quality and characteristics have an influence on the bug fixing process and (ii) if the process quality as measured by the process data has an influence on the product (i.e., software) quality. Specifically, we analyze six Open Source as well as two Closed Source projects and show that process data quality and characteristics have an impact on the bug fixing process: the high rate of empty commit messages in Eclipse, for example, correlates with the bug report quality. We also show that the product quality---measured by number of bugs reported---is affected by process data quality measures. These findings have the potential to prompt practitioners to increase the quality of their software process and its associated data quality.},
  Doi                      = {10.1109/MSR.2010.5463286}
}

@InProceedings{2009:iwpse_evol:bachmann,
  Title                    = {Software process data quality and characteristics: A historical view on open and closed source projects},
  Author                   = {Bachmann, Adrian and Bernstein, Abraham},
  Booktitle                = iwpse_evol,
  Year                     = {2009},
  Pages                    = {119--128},

  Abstract                 = {Software process data gathered from bug tracking databases and version control system log files are a very valuable source to analyze the evolution and history of a project or predict its future. These data are used for instance to predict defects, gather insight into a project's life-cycle, and additional tasks. In this paper we survey five open source projects and one closed source project in order to provide a deeper insight into the quality and characteristics of these often-used process data. Specifically, we first define quality and characteristics measures, which allow us to compare the quality and characteristics of the data gathered for different projects. We then compute the measures and discuss the issues arising from these observation. We show that there are vast differences between the projects, particularly with respect to the quality in the link rate between bugs and commits.},
  Doi                      = {10.1145/1595808.1595830}
}

@InProceedings{2010:fse:bachmann,
  Title                    = {The missing links: Bugs and bug-fix commits},
  Author                   = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
  Booktitle                = fse,
  Year                     = {2010},
  Pages                    = {97--106},

  Abstract                 = {Empirical studies of software defects rely on links between bug databases and program code repositories. This linkage is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories and models of the bias. To do this, we must establish ground truth: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a diffcult task, requiring an expert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sample of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache HTTP web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.},
  Doi                      = {10.1145/1882291.1882308}
}

@Article{2014:scp:bajracharya,
  Title                    = {Sourcerer: An infrastructure for large-scale collection and analysis of open-source code},
  Author                   = {Sushil Bajracharya and Joel Ossher and Cristina Lopes},
  Journal                  = scp,
  Year                     = {2014},

  Month                    = {1~} # jan,
  Pages                    = {241--259},
  Volume                   = {79},

  Abstract                 = {A large amount of open source code is now available online, presenting a great potential resource for software developers. This has motivated software engineering researchers to develop tools and techniques to allow developers to reap the benefits of these billions of lines of source code. However, collecting and analyzing such a large quantity of source code presents a number of challenges. Although the current generation of open source code search engines provides access to the source code in an aggregated repository, they generally fail to take advantage of the rich structural information contained in the code they index. This makes them significantly less useful than Sourcerer for building state-of-the-art software engineering tools, as these tools often require access to both the structural and textual information available in source code. We have developed Sourcerer, an infrastructure for large-scale collection and analysis of open source code. By taking full advantage of the structural information extracted from source code in its repository, Sourcerer provides a foundation upon which state-of-the-art search engines and related tools can easily be built. We describe the Sourcerer infrastructure, present the applications that we have built on top of it, and discuss how existing tools could benefit from using Sourcerer.},
  Doi                      = {10.1016/j.scico.2012.04.008}
}

@InProceedings{1995:wcre:baker,
  Title                    = {On finding duplication and near-duplication in large software systems},
  Author                   = {Baker, B. S.},
  Booktitle                = wcre,
  Year                     = {1995},
  Pages                    = {86--95},

  Abstract                 = {This paper describes how a program called dup can be used to locate instances of duplication or near-duplication in a software system. Dup reports both textually identical sections of code and sections that are the same textually except for systematic substitution of one set of variable names and constants for another. Further processing locates longer sections of code that are the same except for other small modifications. Experimental results from running dup on millions of lines from two large software systems show dup to be both effective at locating duplication and fast. Applications could include identifying sections of code that should be replaced by procedures, elimination of duplication during reengineering of the system, redocumentation to include references to copies, and debugging.},
  Doi                      = {10.1109/WCRE.1995.514697}
}

@InProceedings{1998:usenixatc:baker,
  Title                    = {Deducing Similarities in Java Sources from Bytecodes},
  Author                   = {Brenda S. Baker and Udi Manber},
  Booktitle                = usenixatc,
  Year                     = {1998},
  Pages                    = {15:1--15:13},

  Abstract                 = {Several techniques for detecting similarities of Java programs from bytecode files, without access to the source, are introduced in this paper. These techniques can be used to compare two files, to find similarities among thousands of files, or to compare one new file to an index of many old ones. Experimental results indicate that these techniques can be very effective. Even changes of 30\% to the source file will usually result in bytecode that can be associated with the original. Several applications are discussed.},
  Url                      = {http://static.usenix.org/publications/library/proceedings/usenix98/full_papers/baker/baker.pdf}
}

@InProceedings{1999:csss:baker,
  Title                    = {Compressing Differences of Executable Code},
  Author                   = {Brenda S. Baker and Udi Manber and Robert Muth},
  Booktitle                = csss,
  Year                     = {1999},

  Abstract                 = {Programs change often, and it is important to bring those changes to users as conveniently as possible. The two most common ways to deliver changes are to send a whole new program or to send ``patches'' that encode the differences between the two versions, requiring much less space. In this paper, we address computation of patches for executables of programs. Our techniques take into account the platform-dependent structure of executables, We identify changes in the executables that are likely to be artifacts of the compilation process, and arrange to reconstruct these when the patch is applied rather than including them in the patch; the remaining changes that must be placed in the patch are likely to be derived from source lines that changed. Our techniques should be useful for updating programs over slow data lines and should be particularly important for small devices whose programs will need to be updated through wireless communication. We have implemented our techniques for Digital UNIX Alpha executables; our experiments show our techniques to improve significantly over previous approaches to updating executables.},
  Url                      = {http://reference.kfupm.edu.sa/content/c/o/compressing_differences_of_executable_co_940257.pdf}
}

@InProceedings{2005:oopsla:balaban,
  Title                    = {Refactoring Support for Class Library Migration},
  Author                   = {Ittai Balaban and Frank Tip and Robert Fuhrer},
  Booktitle                = oopsla,
  Year                     = {2005},
  Pages                    = {265--279},

  Abstract                 = {As object-oriented class libraries evolve, classes are occasionally deprecated in favor of others with roughly the same functionality. In Java's standard libraries, for example, class Hashtable has been superseded by HashMap, and Iterator is now preferred over Enumeration. Migrating client applications to use the new idioms is often desirable, but making the required changes to declarations and allocation sites can be quite labor-intensive. Moreover, migration becomes complicated---and sometimes impossible---if an application interacts with external components, if a legacy class is not completely equivalent to its replacement, or if multiple interdependent classes must be migrated simultaneously. We present an approach in which mappings between legacy classes and their replacements are specified by the programmer. Then, an analysis based on type constraints determines where declarations and allocation sites can be updated. The method was implemented in Eclipse, and evaluated on a number of Java applications. On average, our tool could migrate more than 90\% of the references to legacy classes.},
  Doi                      = {10.1145/1094811.1094832}
}

@InProceedings{2003:icsm:balanyi,
  Title                    = {Mining Design Patterns from {C++} Source Code},
  Author                   = {Balanyi, Zsolt and Ferenc, Rudolf},
  Booktitle                = icsm,
  Year                     = {2003},
  Pages                    = {305--314},

  Abstract                 = {Design patterns are micro architectures that have proved to be reliable, easy-to implement and robust. There is a need in science and industry for recognizing these patterns. We present a new method for discovering design patterns in the source code. This method provides a precise specification of how the patterns work by describing basic structural information like inheritance, composition, aggregation and association, and as an indispensable part, by defining call delegation, object creation and operation overriding. We introduce a new {XML-based} language, the Design Pattern Markup Language {(DPML)}, which provides an easy way for the users to modify pattern descriptions to suit their needs, or even to define their own patterns or just classes in certain relations they wish to find. We tested our method on four open-source systems, and found it effective in discovering design pattern instances.},
  Doi                      = {10.1109/ICSM.2003.1235436}
}

@InProceedings{1996:apsec:balasubramaniam,
  Title                    = {Object-oriented metrics},
  Author                   = {N. V. Balasubramanian},
  Booktitle                = apsec,
  Year                     = {1996},
  Pages                    = {30--34},

  Abstract                 = {The paper introduces Class Complexity, Cohesion Ratio and Weighted Method Send out as improved/new metrics for Object-Oriented Software. Chidamber & Kemerer is used as a comparison platform. Illustrations are provided by solving a simple graphical problem using two design approaches. They are also used to validate the model proposed. Desirability of a composite complexity model is discussed and one possible approach is given.},
  Doi                      = {10.1109/APSEC.1996.566737}
}

@InProceedings{2008:oopsla:baldi,
  Title                    = {A theory of aspects as latent topics},
  Author                   = {Pierre F. Baldi and Cristina V. Lopes and Erik J. Linstead and Sushil K. Bajracharya},
  Booktitle                = oopsla,
  Year                     = {2008},
  Pages                    = {543--562},

  Abstract                 = {After more than 10 years, Aspect-Oriented Programming (AOP) is still a controversial idea. While the concept of aspects appeals to everyone's intuitions, concrete AOP solutions often fail to convince researchers and practitioners alike. This discrepancy results in part from a lack of an adequate theory of aspects, which in turn leads to the development of AOP solutions that are useful in limited situations. We propose a new theory of aspects that can be summarized as follows: concerns are latent topics that can be automatically extracted using statistical topic modeling techniques adapted to software. Software scattering and tangling can be measured precisely by the entropies of the underlying topic-over-files and files-over-topics distributions. Aspects are latent topics with high scattering entropy. The theory is validated empirically on both the large scale, with a study of 4,632 Java projects, and the small scale, with a study of 5 individual projects. From these analyses, we identify two dozen topics that emerge as generalpurpose aspects across multiple projects, as well as projectspecific topics/concerns. The approach is also shown to produce results that are compatible with previous methods for identifying aspects, and also extends them. Our work provides not only a concrete approach for identifying aspects at several scales in an unsupervised manner but, more importantly, a formulation of AOP grounded in information theory. The understanding of aspects under this newperspectivemakes additional progress toward the design of models and tools that facilitate software development.},
  Doi                      = {10.1145/1449764.1449807}
}

@InCollection{2005:book:braha:baldwin,
  Title                    = {Modularity in the Design of Complex Engineering Systems},
  Author                   = {Carliss Y. Baldwin and Kim B. Clark},
  Booktitle                = {Complex Engineered Systems: Science Meets Technology},
  Publisher                = {Springer},
  Year                     = {2005},
  Editor                   = {Dan Braha and Ali A. Minai and Yaneer Bar-Yam},
  Pages                    = {175--205},

  Abstract                 = {In the last decade, the concept of modularity has caught the attention of engineers, management researchers and corporate strategists in a number of industries. When a product or process is ``modularized," the elements of its design are split up and assigned to modules according to a formal architecture or plan. From an engineering perspective, a modularization generally has three purposes:}
}

@Book{1999:book:baldwin,
  Title                    = {Design Rules: The Power of Modularity},
  Author                   = {Baldwin, Carliss Y. and Clark, Kim B.},
  Publisher                = {MIT Press},
  Year                     = {1999}
}

@InProceedings{1990:sde:ballance,
  Title                    = {The {Pan} language-based editing system for integrated development},
  Author                   = {Robert A. Ballance and Susan L. Graham and {Van de Vanter}, Michael L.},
  Booktitle                = sde,
  Year                     = {1990},
  Pages                    = {77--93},

  Doi                      = {10.1145/99277.99286}
}

@InProceedings{2002:aosd:baniassad,
  Title                    = {Managing crosscutting concerns during software evolution tasks: An inquisitive study},
  Author                   = {Baniassad, Elisa L. A. and Murphy, Gail C. and Schwanninger, Christa and Kircher, Michael},
  Booktitle                = aosd,
  Year                     = {2002},
  Pages                    = {120--126},

  Abstract                 = {Code is modularized, for many reasons, including making it easier to understand, change, and verify. Aspect-oriented programming approaches extend the kind of code that can be modularized, enabling the modularization of crosscutting code. We conducted an inquisitive study to better understand the kinds of crosscutting code that software developers encounter and to better understand how the developers manage this code. We tracked eight participants: four from industry and four from academia. Each participant was currently evolving a non-trivial software system. We interviewed these participants three times about crosscutting concerns they had encountered and the strategies they used to deal with the concerns. We found that crosscutting concerns tended to emerge as obstacles that the developer had to consider to make the desired change. The strategy used by the developer to manage the concern depended on the form of the obstacle code. The results of this study provide empirical evidence to support the problems identified by the aspect-oriented programming community, and provide a basis on which to further assess aspect-oriented programming.},
  Doi                      = {10.1145/508386.508401}
}

@Article{1993:cacm:banker,
  Title                    = {Software complexity and maintenance costs},
  Author                   = {Rajiv D. Banker and Srikant M. Datar and Chris F. Kemerer and Dani Zweig},
  Journal                  = cacm,
  Year                     = {1993},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {81--94},
  Volume                   = {36},

  Abstract                 = {While the link between the difficulty in understanding computer software and the cost of maintaining it is appealing, prior empirical widence linking software complexity to software maintenance costs is relatively weak [21]. Many of the attempts to link software complexity to maintainability are based on experiments involving small pieces of code, or are based on analysis of software written by students. Such evidence is valuable, but several researchers have noted that such results must be applied cautiously to the large-scale commercial application systemst hat account for most software maintenance expenditures [13, 17]. Furthermore, the limited large-scale research that has been undertaken has generated either conflicting results or none at all, as, for example, on the effects of software modularity and software structure [6, 12]. Additionally, none of the previous work develops estimates ofthe actual cost of complexity, estimates that could be used by software maintenance managers to make the best use of their resources. While research supporting the statistical significance of a factor is, of court, a necessary first step in this process, practitioners must also have an understanding of the practical magnitudes of the effects of complexity if they are to be able to make informed decisions.},
  Doi                      = {10.1145/163359.163375}
}

@InProceedings{2007:www:bao,
  Title                    = {Optimizing web search using social annotations},
  Author                   = {Bao, Shenghua and Xue, Guirong and Wu, Xiaoyuan and Yu, Yong and Fei, Ben and Su, Zhong},
  Booktitle                = www,
  Year                     = {2007},
  Pages                    = {501--510},

  Abstract                 = {This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR) calculates the similarity between social annotations and webqueries; 2) SocialPageRank (SPR) captures the popularity of webpages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a webpage from the web users' perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both SSR and SPR benefit web search significantly.},
  Doi                      = {10.1145/1242572.1242640}
}

@Article{2005:nature:barabasi,
  Title                    = {The origin of bursts and heavy tails in human dynamics},
  Author                   = {Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = nature,
  Year                     = {2005},

  Month                    = {12 } # may,
  Number                   = {7039},
  Pages                    = {207--211},
  Volume                   = {435},

  Abstract                 = {The dynamics of many social, technological and economic phenomena are driven by individual human actions, turning the quantitative understanding of human behaviour into a central question of modern science. Current models of human dynamics, used from risk assessment to communications, assume that human actions are randomly distributed in time and thus well approximated by Poisson processes. In contrast, there is increasing evidence that the timing of many human activities, ranging from communication to entertainment and work patterns, follow non-Poisson statistics, characterized by bursts of rapidly occurring events separated by long periods of inactivity. Here I show that the bursty nature of human behaviour is a consequence of a decision-based queuing process: when individuals execute tasks based on some perceived priority, the timing of the tasks will be heavy tailed, with most tasks being rapidly executed, whereas a few experience very long waiting times. In contrast, random or priority blind execution is well approximated by uniform inter-event statistics. These finding have important implications, ranging from resource management to service allocation, in both communications and retail.},
  Doi                      = {10.1038/nature03459}
}

@Article{1999:science:barabasi,
  Title                    = {Emergence of scaling in random networks},
  Author                   = {Albert-L{\'a}szl{\'o} Barab{\'a}si and R{\'e}ka Albert},
  Journal                  = science,
  Year                     = {1999},

  Month                    = {15 } # oct,
  Number                   = {5439},
  Pages                    = {509--512},
  Volume                   = {286},

  Abstract                 = {Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.},
  Doi                      = {10.1126/science.286.5439.509}
}

@Article{2000:physicaa:barabasi,
  Title                    = {Scale-free characteristics of random networks: The topology of the world-wide web},
  Author                   = {Albert-L{\'a}szl{\'o} Barab{\'a}si and R{\'e}ka Albert and Hawoong Jeong},
  Journal                  = physicaa,
  Year                     = {2000},

  Month                    = {15 } # jun,
  Number                   = {1--4},
  Pages                    = {69--77},
  Volume                   = {281},

  Abstract                 = {The world-wide web forms a large directed graph, whose vertices are documents and edges are links pointing from one document to another. Here we demonstrate that despite its apparent random character, the topology of this graph has a number of universal scale-free characteristics. We introduce a model that leads to a scale-free network, capturing in a minimal fashion the self-organization processes governing the world-wide web.},
  Doi                      = {10.1016/S0378-4371(00)00018-2}
}

@Article{1999:physicaa:barabasi,
  Title                    = {Mean-field theory for scale-free random networks},
  Author                   = {Albert-L{\'a}szl{\'o} Barab{\'a}si and R{\'e}ka Albert and Hawoong Jeong},
  Journal                  = physicaa,
  Year                     = {1999},

  Month                    = {1 } # oct,
  Number                   = {1--2},
  Pages                    = {173--187},
  Volume                   = {272},

  Abstract                 = {Random networks with complex topology are common in Nature, describing systems as diverse as the world wide web or social and business networks. Recently, it has been demonstrated that most large networks for which topological information is available display scale-free features. Here we study the scaling properties of the recently introduced scale-free model, that can account for the observed power-law distribution of the connectivities. We develop a mean-field method to predict the growth dynamics of the individual vertices, and use this to calculate analytically the connectivity distribution and the scaling exponents. The mean-field method can be used to address the properties of two variants of the scale-free model, that do not display power-law scaling.},
  Doi                      = {10.1016/S0378-4371(99)00291-5}
}

@Article{2001:physicaa:barabasi,
  Title                    = {Deterministic scale-free networks},
  Author                   = {Albert-L{\'a}szl{\'o} Barab{\'a}si and Erzs{\'e}bet Ravasz and Tam{\'a}s Vicsek},
  Journal                  = physicaa,
  Year                     = {2001},

  Month                    = {15 } # oct,
  Number                   = {3--4},
  Pages                    = {559--564},
  Volume                   = {299},

  Abstract                 = {Scale-free networks are abundant in nature and society, describing such diverse systems as the world wide web, the web of human sexual contacts, or the chemical network of a cell. All models used to generate a scale-free topology are stochastic, that is they create networks in which the nodes appear to be randomly connected to each other. Here we propose a simple model that generates scale-free networks in a deterministic fashion. We solve exactly the model, showing that the tail of the degree distribution follows a power law.},
  Doi                      = {10.1016/S0378-4371(01)00369-7}
}

@InProceedings{2005:euromicro:barais,
  Title                    = {A Framework to Specify Incremental Software Architecture Transformations},
  Author                   = {Barais, Olivier and Duchien, Laurence and Le Meur, Anne-Francoise},
  Booktitle                = euromicro,
  Year                     = {2005},
  Pages                    = {62--69},

  Abstract                 = {A software architecture description facilitates the comprehension, analysis and prototyping of a piece of software. However, such a description is often monolithic and difficult to evolve. This paper proposes a framework, named TranSAT (transformations for software architecture), for incrementally integrating new concerns into a software architecture. The structural and behavioral properties of a new concern are represented by a self-sufficient component assembly description, called an architecture plan. TranSAT proposes a software architecture pattern as a means of integrating business and technical plans. Such a pattern includes not only the plan to integrate but also the preconditions that the target architecture must satisfy, and the modifications to perform on this architecture. Consequently, from a set of patterns, TranSAT allows a software architect to incrementally build complex architectures.},
  Doi                      = {10.1109/EUROMICRO.2005.5}
}

@InProceedings{2005:wicsa:barais,
  Title                    = {Providing Support for Safe Software Architecture Transformations},
  Author                   = {Barais, Olivier and Lawall, Julia and Le Meur, Anne-Francoise and Duchien, Laurence},
  Booktitle                = wicsa,
  Year                     = {2005},
  Pages                    = {201--202},

  Abstract                 = {Software architecture is a key concept in the design of a complex system. An architecture models the structure and behavior of the system, including the software elements and the relationships between them. While architectures were originally specified informally, recent years have seen the creation of a number of Architecture Description Languages (ADLs) [4]. ADLs are designed around the dimensions of composition and interaction, allowing the architect to introduce new concerns by constructing and combining increasingly complex elements.},
  Doi                      = {10.1109/WICSA.2005.54}
}

@InCollection{2008:book:mens:barais,
  Title                    = {Software Architecture Evolution},
  Author                   = {Olivier Barais and Le Meur, Anne-Fran\c{c}oise and Laurence Duchien and Julia L. Lawall},
  Booktitle                = {Software Evolution},
  Publisher                = {Springer},
  Year                     = {2008},
  Chapter                  = {10},
  Editor                   = {Tom Mens and Serge Demeyer},
  Pages                    = {233--262},

  Abstract                 = {Software architectures must frequently evolve to cope with changing requirements, and this evolution often implies integrating new concerns. Unfortunately, when the new concerns are crosscutting, existing architecture description languages provide little or no support for this kind of evolution. The software architect must modify multiple elements of the architecture manually, which risks introducing inconsistencies. This chapter provides an overview, comparison and detailed treatment of the various state-of-the-art approaches to describing and evolving software architectures. Furthermore, we discuss one particular framework named TranSAT, which addresses the above problems of software architecture evolution. TranSAT provides a new element in the software architecture descriptions language, called an architectural aspect, for describing new concerns and their integration into an existing architecture. Following the early aspect paradigm, TranSAT allows the software architect to design a software architecture stepwise in terms of aspects at the design stage. It realises the evolution as the weaving of new architectural aspects into an existing software architecture.},
  Doi                      = {10.1007/978-3-540-76440-3_10}
}

@Article{1991:software:barnes,
  Title                    = {Making Reuse Cost-Effective},
  Author                   = {Barnes, Bruce H. and Bollinger, Terry B.},
  Journal                  = software,
  Year                     = {1991},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {13--24},
  Volume                   = {8},

  Abstract                 = {Until reuse is better understood, significant reductions in the cost of building large systems will not be possible. This assertion is based primarily on the belief that the defining characteristic of good reuse is not the reuse of software per se, but the reuse of human problem solving. Analytical approaches for making good reuse investments are suggested in terms of increasing a quality-of-investment measure, Q, which is simply the ratio of reuse benefits to reuse investments. The first strategy for increasing Q is to increase the level of consumer reuse. The second technique for increasing Q is to reduce the average cost of reusing work products by making them easy and inexpensive to reuse. The third strategy is to reduce investment costs. Reuse strategies, and reuse and parameterizations, are discussed.},
  Doi                      = {10.1109/52.62928}
}

@InProceedings{2012:qosa:barnes,
  Title                    = {{NASA}'s advanced multimission operations system: A case study in software architecture evolution},
  Author                   = {Barnes, Jeffrey M.},
  Booktitle                = qosa,
  Year                     = {2012},
  Pages                    = {3--12},

  Abstract                 = {Virtually all software systems of significant size and longevity eventually undergo changes to their basic architectural structure. Such changes may be prompted by new feature requests, new quality attribute requirements, changing technology, or other reasons. Whatever the cause, software architecture evolution is commonplace in real-world software projects. However, research in this area has suffered from problems of validation; previous work has tended to make heavy use of toy examples and hypothetical scenarios and has not been well supported by real-world examples. To help address this problem, this paper presents a case study of an ongoing effort at the Jet Propulsion Laboratory to rearchitect the Advanced Multimission Operations System used to operate NASA's deep-space and astrophysics missions. Based on examination of project documents and interviews with project personnel, I describe the goals and approach of this evolution effort, then demonstrate how approaches and formal methods from previous research in architecture evolution may be applied to this evolution while using languages and tools already in place at the Jet Propulsion Laboratory.},
  Doi                      = {10.1145/2304696.2304700}
}

@Article{2000:epjb:barrat,
  Title                    = {On the properties of small-world networks},
  Author                   = {A. Barrat and M. Weigt},
  Journal                  = epjb,
  Year                     = {2000},

  Month                    = {1 } # feb,
  Number                   = {3},
  Pages                    = {547--560},
  Volume                   = {13},

  Abstract                 = {We study the small-world networks recently introduced by Watts and Strogatz [Nature 393, 440 (1998)], using analytical as well as numerical tools. We characterize the geometrical properties resulting from the coexistence of a local structure and random long-range connections, and we examine their evolution with size and disorder strength. We show that any finite value of the disorder is able to trigger a ``small-world" behaviour as soon as the initial lattice is big enough, and study the crossover between a regular lattice and a ``small-world" one. These results are corroborated by the investigation of an Ising model defined on the network, showing for every finite disorder fraction a crossover from a high-temperature region dominated by the underlying one-dimensional structure to a mean-field like low-temperature region. In particular there exists a finite-temperature ferromagnetic phase transition as soon as the disorder strength is finite.},
  Doi                      = {10.1007/s100510050067}
}

@InProceedings{2010:iwmcp:barrett,
  Title                    = {{M}irador: A synthesis of model matching strategies},
  Author                   = {Barrett, Stephen C. and Butler, Greg and Chalin, Patrice},
  Booktitle                = iwmcp,
  Year                     = {2010},
  Pages                    = {2--10},

  Abstract                 = {Mirador is a model merging tool that supports multiple model comparison strategies for the purpose of matching model elements. Capable of running either standalone, or as a Fujaba plug-in, Mirador leverages the CoObRA software versioning package to obtain model change information. The bringing together of various comparison strategies allows Mirador to solicit measures of element similarity from one or more strategies, as appropriate for a given matching context. As an addition to this strategy mix we suggest one based on model evolution, and illustrate its potential for use with some simple examples. Mirador performs operation-based merging, premised on the notion of a plane of change operations, which we have extended into the third dimension to enable the detection of cross-matching strategy conflicts. We also propose breaking this monolithic change plane up into a series of local change planes to facilitate effective, conflict free merging.},
  Doi                      = {10.1145/1826147.1826151}
}

@Article{2008:sqj:bartsch,
  Title                    = {An exploratory study of the effect of aspect-oriented programming on maintainability},
  Author                   = {Marc Bartsch and Rachel Harrison},
  Journal                  = sqj,
  Year                     = {2008},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {23--44},
  Volume                   = {16},

  Abstract                 = {In this paper we describe an exploratory assessment of the effect of aspectoriented programming on software maintainability. An experiment was conducted in which 11 software professionals were asked to carry out maintenance tasks on one of two programs. The first program was written in Java and the second in AspectJ. Both programs implement a shopping system according to the same set of requirements. A number of statistical hypotheses were tested. The results did seem to suggest a slight advantage for the subjects using the object-oriented system since in general it took the subjects less time to answer the questions on this system. Also, both systems appeared to be equally difficult to modify. However, the results did not show a statistically significant influence of aspectoriented programming at the 5\% level. We are aware that the results of this single small study cannot be generalized. We conclude that more empirical research is necessary in this area to identify the benefits of aspect-oriented programming and we hope that this paper will encourage such research.},
  Doi                      = {10.1007/s11219-007-9022-7}
}

@InProceedings{1994:icse:basili,
  Title                    = {Facts and myths affecting software reuse},
  Author                   = {Victor R. Basili},
  Booktitle                = icse,
  Year                     = {1994},
  Pages                    = {269},

  Abstract                 = {Discusses the three most important facts or myths affecting reuse. There is a great deal of misunderstanding about reuse in the software domain and it is difficult to pick out only three: there has been too much emphasis on the reuse of code; software reuse implies some form of modification of the artifact being reused; and software development processes do not explicitly support reuse, in fact they implicitly inhibit reuse},
  Doi                      = {10.1109/ICSE.1994.296786}
}

@Article{1996:cacm:basili,
  Title                    = {How reuse influences productivity in object-oriented systems},
  Author                   = {Victor R. Basili and Lionel C. Briand and Walc\'{e}lio L. Melo},
  Journal                  = cacm,
  Year                     = {1996},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {104--116},
  Volume                   = {39},

  Abstract                 = {Although reuse is assumed to be especially valuable in building high-quality software, as well as in OO development, limited empirical evidence connects reuse with productivity and quality gains. The authors' eight-system study begins to define such benefits in an OO framework, most notably in terms of reduced defect density and rework, as well as in increased productivity.},
  Doi                      = {10.1145/236156.236184}
}

@Article{1996:tse:basili,
  Title                    = {A validation of object-oriented design metrics as quality indicators},
  Author                   = {Victor R. Basili and Lionel C. Briand and Walc{\'e}lio L. Melo},
  Journal                  = tse,
  Year                     = {1996},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {751--761},
  Volume                   = {22},

  Abstract                 = {This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than ``traditional" code metrics, which can only be collected at a later phase of the software development processes},
  Doi                      = {10.1109/32.544352}
}

@Article{1986:tse:basili,
  Title                    = {Experimentation in software engineering},
  Author                   = {Victor R. Basili and Richard W. Selby and David H. Hutchens},
  Journal                  = tse,
  Year                     = {1986},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {733--743},
  Volume                   = {12},

  Abstract                 = {Experimentation in software engineering supports the advancement of the field through an iterative learning process. In this paper we present a framework for analyzing most of the experimental work performed in software engineering over the past several years. We describe a variety of experiments in the framework and discuss their contribution to the software engineering discipline. Some useful recommendations for the applicaion of the experimental process in software engineering are included.},
  Doi                      = {10.1109/TSE.1986.6312975}
}

@Article{2009:tse:basit,
  Title                    = {A Data Mining Approach for Detecting Higher-Level Clones in Software},
  Author                   = {Basit, Hamid Abdul and Jarzabek, Stan},
  Journal                  = tse,
  Year                     = {2009},

  Month                    = jul # {/} # aug,
  Number                   = {4},
  Pages                    = {497--514},
  Volume                   = {35},

  Abstract                 = {Code clones are similar program structures recurring in variant forms in software system(s). Several techniques have been proposed to detect similar code fragments in software, so-called simple clones. Identification and subsequent unification of simple clones is beneficial in software maintenance. Even further gains can be obtained by elevating the level of code clone analysis. We observed that recurring patterns of simple clones often indicate the presence of interesting higher-level similarities that we call structural clones. Structural clones show a bigger picture of similarity situation than simple clones alone. Being logical groups of simple clones, structural clones alleviate the problem of huge number of clones typically reported by simple clone detection tools, a problem that is often dealt with postdetection visualization techniques. Detection of structural clones can help in understanding the design of the system for better maintenance and in reengineering for reuse, among other uses. In this paper, we propose a technique to detect some useful types of structural clones. The novelty of our approach includes the formulation of the structural clone concept and the application of data mining techniques to detect these higher-level similarities. We describe a tool called clone miner that implements our proposed technique. We assess the usefulness and scalability of the proposed techniques via several case studies. We discuss various usage scenarios to demonstrate in what ways the knowledge of structural clones adds value to the analysis based on simple clones alone.},
  Doi                      = {10.1109/TSE.2009.16}
}

@Book{2012:book:bass,
  Title                    = {Software Architecture in Practice},
  Author                   = {Bass, Len and Clements, Paul and Kazman, Rick},
  Publisher                = {Addison-Wesley},
  Year                     = {2012},
  Edition                  = {3rd}
}

@Book{2003:book:bass,
  Title                    = {Software Architecture in Practice},
  Author                   = {Len Bass and Paul Clements and Rick Kazman},
  Publisher                = {Addison-Wesley},
  Year                     = {2003},
  Edition                  = {2nd}
}

@Article{2007:software:bassett,
  Title                    = {The Case for Frame-Based Software Engineering},
  Author                   = {Paul Bassett},
  Journal                  = software,
  Year                     = {2007},

  Month                    = jul # {/} # aug,
  Number                   = {4},
  Pages                    = {90--99},
  Volume                   = {24},

  Abstract                 = {Frame technology adapts generic components into custom information structures. Its facility for maximizing reuse and minimizing redundancy has demonstrated dramatic improvements across software's life cycle.},
  Doi                      = {10.1109/MS.2007.119}
}

@InProceedings{1997:ssr:bassett,
  Title                    = {The theory and practice of adaptive reuse},
  Author                   = {Paul G. Bassett},
  Booktitle                = ssr,
  Year                     = {1997},
  Pages                    = {2--9},

  Abstract                 = {Good morning everyone. What a pleasure it is to be among peers who are as excited about reuse as I am, and who think long and hard about how reuse works. I am sure the only reason the whole world is not as excited as we are, is because we have not yet discovered how software reuse relates to sex! For over fifteen years I have made my living by applying the tools and techniques of ``adaptive reuse" to the software life cycle of large, complex business systems of all stripes. I am pleased to say that organizations employing adaptive reuse, commonly achieve order of magnitude reductions in the time and cost of their software projects, be they new or existing systems. I want to share with you some of what I have learned, ``lessons from the real world," as the sub-title of my book is called [1]. It comes as no surprise to say that reuse is a multi-dimensional subject (with nary a hint of sex along any dimension), involving: technology, methodology, infrastructure, and culture. My talk today can but scratch the surface of these dimensions.},
  Doi                      = {10.1145/258366.258371}
}

@Article{1989:or:bates,
  Title                    = {The Design of Browsing and Berrypicking Techniques for the Online Search Interface},
  Author                   = {Bates, Marcia J.},
  Journal                  = or,
  Year                     = {1989},
  Number                   = {5},
  Pages                    = {407--424},
  Volume                   = {13},

  Abstract                 = {First, a new model of searching in online and other information systems, called ``berrypicking," is discussed. This model, it is argued, is much closer to the real behavior of information searchers than the traditional model of information retrieval is, and, consequently, will guide our thinking better in the design of effective interfaces. Second, the research literature of manual information seeking behavior is drawn on for suggestions of capabilities that users might like to have in online systems. Third, based on the new model and the research on information seeking, suggestions are made for how new search capabilities could be incorporated into the design of search interfaces. Particular attention is given to the nature and types of browsing that can be facilitated.},
  Doi                      = {10.1108/eb024320}
}

@InProceedings{1971:ifip:bauer,
  Title                    = {Software Engineering},
  Author                   = {Friedrich L. Bauer},
  Booktitle                = ifipc,
  Year                     = {1971},
  Pages                    = {530--538},
  Volume                   = {1}
}

@Article{2013:ese:bavota,
  Title                    = {Automating {E}xtract {C}lass refactoring: An improved method and its evaluation},
  Author                   = {Gabriele Bavota and De Lucia, Andrea and Andrian Marcus and Rocco Oliveto},
  Journal                  = ese,
  Year                     = {2013},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {1617--1664},
  Volume                   = {19},

  Abstract                 = {During software evolution the internal structure of the system undergoes continuous modifications. These continuous changes push away the source code from its original design, often reducing its quality, including class cohesion. In this paper we propose a method for automating the Extract Class refactoring. The proposed approach analyzes (structural and semantic) relationships between the methods in a class to identify chains of strongly related methods. The identified method chains are used to define new classes with higher cohesion than the original class, while preserving the overall coupling between the new classes and the classes interacting with the original class. The proposed approach has been first assessed in an artificial scenario in order to calibrate the parameters of the approach. The data was also used to compare the new approach with previous work. Then it has been empirically evaluated on real Blobs from existing open source systems in order to assess how good and useful the proposed refactoring solutions are considered by software engineers and how well the proposed refactorings approximate refactorings done by the original developers. We found that the new approach outperforms a previously proposed approach and that developers find the proposed solutions useful in guiding refactorings.},
  Doi                      = {10.1007/s10664-013-9256-x}
}

@InProceedings{2010:icsm:bavota,
  Title                    = {Playing with Refactoring: Identifying Extract Class Opportunities through Game Theory},
  Author                   = {Gabriele Bavota and Rocco Oliveto and De Lucia, Andrea and Giuliano Antoniol and Yann-Ga{\"e}l Gu{\'e}h{\'e}neuc},
  Booktitle                = icsm,
  Year                     = {2010},
  Pages                    = {13:1--13:5},

  Abstract                 = {In software engineering, developers must often find solutions to problems balancing competing goals, e.g., quality versus cost, time to market versus resources, or cohesion versus coupling. Finding a suitable balance between contrasting goals is often complex and recommendation systems are useful to support developers and managers in performing such a complex task. We believe that contrasting goals can be often dealt with game theory techniques. Indeed, game theory is successfully used in other fields, especially in economics, to mathematically propose solutions to strategic situation, in which an individual's success in making choices depends on the choices of others. To demonstrate the applicability of game theory to software engineering and to understand its pros and cons, we propose an approach based on game theory that recommend extract-class refactoring opportunities. A preliminary evaluation inspired by mutation testing demonstrates the applicability and the benefits of the proposed approach.},
  Doi                      = {10.1109/ICSM.2010.5609739}
}

@InProceedings{2006:oopsla:baxter,
  Title                    = {Understanding the shape of {J}ava software},
  Author                   = {Gareth Baxter and Marcus Frean and James Noble and Mark Rickerby and Hayden Smith and Matt Visser and Hayden Melton and Ewan Tempero},
  Booktitle                = oopsla,
  Year                     = {2006},
  Pages                    = {397--412},

  Abstract                 = {Large amounts of Java software have been written since the language's escape into unsuspecting software ecology more than ten years ago. Surprisingly little is known about the structure of Java programs in the wild: about the way methods are grouped into classes and then into packages, the way packages relate to each other, or the way inheritance and composition are used to put these programs together. We present the results of the first in-depth study of the structure of Java programs. We have collected a number of Java programs and measured their key structural attributes. We have found evidence that some relationships follow power-laws, while others do not. We have also observed variations that seem related to some characteristic of the application itself. This study provides important information for researchers who can investigate how and why the structural relationships we find may have originated, what they portend, and how they can be managed.},
  Doi                      = {10.1145/1167473.1167507}
}

@InProceedings{2002:iwpse:baxter,
  Title                    = {{DMS}: Program transformations for practical scalable software evolution},
  Author                   = {Ira D. Baxter},
  Booktitle                = iwpse,
  Year                     = {2002},
  Pages                    = {48--51},

  Abstract                 = {This paper describes the scaling issues and progress towards constructing a practical program transformation system to support software evolution.},
  Doi                      = {10.1109/ICSE.2004.1317484}
}

@InProceedings{1998:icsm:baxter,
  Title                    = {Clone Detection Using Abstract Syntax Trees},
  Author                   = {Ira D. Baxter and Andrew Yahin and De Moura, Leonardo M. and Marcelo Sant'Anna and Lorraine Bier},
  Booktitle                = icsm,
  Year                     = {1998},
  Pages                    = {368--377},

  Abstract                 = {Existing research suggests that a considerable fraction (5-10\%) of the source code of large-scale computer programs is duplicate code (``clones"). Detection and removal of such clones promises decreased software maintenance costs of possibly the same magnitude. Previous work was limited to detection of either near-misses differing only in single lexems, or near misses only between complete functions. This paper presents simple and practical methods for detecting exact and near miss clones over arbitrary program fragments in program source code by using abstract syntax trees. Previous work also did not suggest practical means for removing detected clones. Since our methods operate in terms of the program structure, clones could be removed by mechanical methods producing in-lined procedures or standard preprocessor macros.A tool using these techniques is applied to a C production software system of some 400K source lines, and the results confirm detected levels of duplication found by previous work. The tool produces macro bodies needed for clone removal, and macro invocations to replace the clones. The tool uses a variation of the well-known compiler method for detecting common sub-expressions. This method determines exact tree matches; a number of adjustments are needed to detect equivalent statement sequences, commutative operands, and nearly exact matches. We additionally suggest that clone detection could also be useful in producing more structured code, and in reverse engineering to discover domain concepts and their implementations.},
  Doi                      = {10.1109/ICSM.1998.738528}
}

@Book{2002:book:beck,
  Title                    = {Test Driven Development: By Example},
  Author                   = {Beck, Kent},
  Publisher                = {Addison Wesley},
  Year                     = {2002}
}

@Book{2004:book:beck,
  Title                    = {Extreme Programming Explained: Embrace Change},
  Author                   = {Beck, Kent and Andres, Cynthia},
  Publisher                = {Addison-Wesley Professional},
  Year                     = {2004},
  Edition                  = {2nd}
}

@InProceedings{2009:chase:begel,
  Title                    = {Coordination in Large-Scale Software Teams},
  Author                   = {Andrew Begel and Nachiappan Nagappan and Christopher Poile and Lucas Layman},
  Booktitle                = chase,
  Year                     = {2009},
  Pages                    = {1--7},

  Abstract                 = {Large-scale software development requires coordination within and between very large engineering teams which may be located in different buildings, on different company campuses, and in different time zones. From a survey answered by 775 Microsoft software engineers, we learned how work was coordinated within and between teams and how engineers felt about their success at these tasks. The respondents revealed that the most common objects of coordination are schedules and features, not code or interfaces, and that more communication and personal contact worked better to make interactions between teams go more smoothly.},
  Doi                      = {10.1109/CHASE.2009.5071401}
}

@Article{1976:ibmsj:belady,
  Title                    = {A Model of Large Program Development},
  Author                   = {L. A. Belady and M. M. Lehman},
  Journal                  = ibmsj,
  Year                     = {1976},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {225--252},
  Volume                   = {15},

  Abstract                 = {Discussed are observations made on the development of OS/360 and its subsequent enhancements and releases. Some modeling approaches to organizing these observations are also presented.},
  Doi                      = {10.1147/sj.153.0225}
}

@InProceedings{2011:csmr:belderrar,
  Title                    = {Sub-graph Mining: Identifying Micro-architectures in Evolving Object-Oriented Software},
  Author                   = {Belderrar, Ahmed and Kpodjedo, Segla and Gu{\'e}h{\'e}neuc, Yann-Ga{\"e}l and Antoniol, Giuliano and Galinier, Philippe},
  Booktitle                = csmr,
  Year                     = {2011},
  Pages                    = {171--180},

  Abstract                 = {Developers introduce novel and undocumented micro-architectures when performing evolution tasks on object-oriented applications. We are interested in understanding whether those organizations of classes and relations can bear, much like cataloged design and anti-patterns, potential harm or benefit to an object-oriented application. We present SGFinder, a sub-graph mining approach and tool based on an efficient enumeration technique to identify recurring micro-architectures in object-oriented class diagrams. Once SGFinder has detected instances of micro-architectures, we exploit these instances to identify their desirable properties, such as stability, or unwanted properties, such as change or fault proneness. We perform a feasibility study of our approach by applying SGFinder on the reverse-engineered class diagrams of several releases of two Java applications: ArgoUML and Rhino. We characterize and highlight some of the most interesting micro-architectures, e.g., the most fault prone and the most stable, and conclude that SGFinder opens the way to further interesting studies.},
  Doi                      = {10.1109/CSMR.2011.23}
}

@Article{2007:tse:bellon,
  Title                    = {Comparison and Evaluation of Clone Detection Tools},
  Author                   = {Bellon, S. and Koschke, R. and Antoniol, G. and Krinke, J. and Merlo, E.},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {577--591},
  Volume                   = {33},

  Abstract                 = {Many techniques for detecting duplicated source code (software clones) have been proposed in the past. However, it is not yet clear how these techniques compare in terms of recall and precision as well as space and time requirements. This paper presents an experiment that evaluates six clone detectors based on eight large C and Java programs (altogether almost 850 {KLOC).} Their clone candidates were evaluated by one of the authors as an independent third party. The selected techniques cover the whole spectrum of the state-of-the-art in clone detection. The techniques work on text, lexical and syntactic information, software metrics, and program dependency graphs.},
  Doi                      = {10.1109/TSE.2007.70725}
}

@Article{1995:cacm:berg,
  Title                    = {Lessons learned from the {OS}/400 {OO} project},
  Author                   = {William Berg and Marshall Cline and Mike Girou},
  Journal                  = cacm,
  Year                     = {1995},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {54--64},
  Volume                   = {38},

  Abstract                 = {This article describes some of the lessons learned when a team of 150 developers with a minimal prior exposure to object-oriented (OO) technology undertook a large development project. Team members became proficient in OO design, using C++ as an OO language rather than just using C++ as a better C, and developed IBM's RISC version of the AS/400 and System/36 operating systems from 1992 to 1994 in Rochester, Minnesota. The project contains 14,000 thousand classes, 90,000 thousand methods, and 2 million lines of C++ integrated into 20 million lines of total code. The result of their efforts was the development of a product that is being used daily by a substantial international customer base.},
  Doi                      = {10.1145/226239.226253}
}

@InProceedings{2007:ideal:berzal,
  Title                    = {Hierarchical program representation for program element matching},
  Author                   = {Berzal, Fernando and Cubero, Juan-Carlos and Jim{\'e}nez, A\'{\i}da},
  Booktitle                = ideal,
  Year                     = {2007},
  Pages                    = {467--476},

  Abstract                 = {Many intermediate program representations are used by compilers and other software development tools. In this paper, we propose a novel representation technique that, unlike those commonly used by compilers, has been explicitly designed for facilitating program element matching, a task at the heart of many software mining problems.},
  Doi                      = {10.1007/978-3-540-77226-2_48}
}

@InProceedings{2007:etx:bettenburg,
  Title                    = {Quality of bug reports in {E}clipse},
  Author                   = {Bettenburg, Nicolas and Just, Sascha and Schr{\"{o}}ter, Adrian and Wei{\ss}, Cathrin and Premraj, Rahul and Zimmermann, Thomas},
  Booktitle                = etx,
  Year                     = {2007},
  Pages                    = {21--25},

  Abstract                 = {The information in bug reports influences the speed at which bugs are fixed. However, bug reports differ in their quality of information. We conducted a survey among ECLIPSE developers to determine the information in reports that they widely used and the problems frequently encountered. Our results show that steps to reproduce and stack traces are most sought after by developers, while inaccurate steps to reproduce and incomplete information pose the largest hurdles. Surprisingly, developers are indifferent to bug duplicates. Such insight is useful to design new bug tracking tools that guide reporters at providing more helpful information. We also present a prototype of a quality-meter tool that measures the quality of bug reports by scanning its content.},
  Doi                      = {10.1145/1328279.1328284}
}

@InProceedings{2008:fse:bettenburg,
  Title                    = {What makes a good bug report?},
  Author                   = {Bettenburg, Nicolas and Just, Sascha and Schr{\"o}ter, Adrian and Weiss, Cathrin and Premraj, Rahul and Zimmermann, Thomas},
  Booktitle                = fse,
  Year                     = {2008},
  Pages                    = {308--318},

  Abstract                 = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difficult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. In our experiments, CUEZILLA was able to predict the quality of 31--48\% of bug reports accurately.},
  Doi                      = {10.1145/1453101.1453146}
}

@InProceedings{2009:icsm:bettenburg,
  Title                    = {Duplicate bug reports considered harmful {\ldots{}} really?},
  Author                   = {Bettenburg, Nicolas and Premraj, Rahul and Zimmermann, Thomas and Kim, Sunghun},
  Booktitle                = icsm,
  Pages                    = {337--345},

  Abstract                 = {In a survey we found that most developers have experienced duplicated bug reports, however, only few considered them as a serious problem. This contradicts popular wisdom that considers bug duplicates as a serious problem for open source projects. In the survey, developers also pointed out that the additional information provided by duplicates helps to resolve bugs quicker. In this paper, we therefore propose to merge bug duplicates, rather than treating them separately. We quantify the amount of information that is added for developers and show that automatic triaging can be improved as well. In addition, we discuss the different reasons why users submit duplicate bug reports in the first place.},
  Doi                      = {10.1109/ICSM.2008.4658082}
}

@InProceedings{2005:icpc:beyer,
  Title                    = {Clustering Software Artifacts Based on Frequent Common Changes},
  Author                   = {Beyer, Dirk and Noack, Andreas},
  Booktitle                = icpc,
  Year                     = {2005},
  Pages                    = {259--268},

  Abstract                 = {Changes of software systems are less expensive and less error-prone if they affect only one subsystem. Thus, clusters of artifacts that are frequently changed together are subsystem candidates. We introduce a two-step method for identifying such clusters. First, a model of common changes of software artifacts, called co-change graph, is extracted from the version control repository of the software system. Second, a layout of the co-change graph is computed that reveals clusters of frequently co-changed artifacts. We derive requirements for such layouts, and introduce an energy model for producing layouts that fulfill these requirements. We evaluate the method by applying it to three example systems, and comparing the resulting layouts to authoritative decompositions.},
  Doi                      = {10.1109/WPC.2005.12}
}

@InProceedings{1995:ssr:bieman,
  Title                    = {Cohesion and reuse in an object-oriented system},
  Author                   = {James M. Bieman and Byung-Kyoo Kang},
  Booktitle                = ssr,
  Year                     = {1995},
  Pages                    = {259--262},

  Abstract                 = {We define and apply two new measures of object-oriented class cohesion to a reasonably large C++ system. We find that most of the classes are quite cohesive, but that the classes that are reused more frequently via inheritance exhibit clearly lower cohesion.},
  Doi                      = {10.1145/211782.211856}
}

@Article{1987:software:biggerstaff,
  Title                    = {Reusability Framework, Assessment, and Directions},
  Author                   = {Biggerstaff, T. and Richter, C.},
  Journal                  = software,
  Year                     = {1987},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {41--49},
  Volume                   = {4},

  Abstract                 = {Reusability is widely believed to be a key to improving software development productivity and quality. The reuse of software components amplifies th software developer's capabilities. It results in fewer total symbols in a system's development and in less time spent organizing those symbols. However, while reusability is a strategy of great promise, it is one whose promise has been largely unfulfilled.},
  Doi                      = {10.1109/MS.1987.230095}
}

@Article{1998:annse:biggerstaff,
  Title                    = {A perspective of generative reuse},
  Author                   = {Biggerstaff, Ted J.},
  Journal                  = annse,
  Year                     = {1998},

  Month                    = {1 } # jan,
  Number                   = {1},
  Pages                    = {169--226},
  Volume                   = {5},

  Abstract                 = {This paper presents a perspective of generative reuse technologies as they have evolved over the last 15 years or so and a discussion of how generative reuse addresses some key reuse problems. Over that time period, a number of different reuse strategies have been tried ranging from pure component reuse to pure generation. The record of success is mixed and the evidence is sketchy. Nevertheless, the paper will use some known metric evidence plus anecdotal evidence, personal experience, and suggestive evidence to define some of the boundaries of the success envelope. Fundamentally, the paper will make the argument that the first order term in the success equation of reuse is the amount of domain-specific content and the second order term is the specific technology chosen in which to express that content. The overall payoff of any reuse system correlates well with the amount of content expressed in the domain specific elements. While not a silver bullet, technology is not without its contribution and the degree of payoff for any specific technology is sensitive to many factors. The paper will make the argument that the generative factors predominate over other technology factors. By looking closely at several successful generation systems that are exemplars for classes of related systems, the paper will examine how those classes have solved problems associated with the more convention reuse of concrete components expressed in conventional programming languages. From this analysis, it will distill the key elements of generative success and provide an opinion of approximately where each class of generative system fits in the overall picture. The result is a guide to the generative reuse technologies that appear to work best today.},
  Doi                      = {10.1023/A:1018924407841}
}

@InProceedings{1994:icsr:biggerstaff,
  Title                    = {The library scaling problem and the limits of concrete component reuse},
  Author                   = {Ted J. Biggerstaff},
  Booktitle                = icsr,
  Year                     = {1994},
  Pages                    = {102--109},

  Abstract                 = {The growth of component libraries puts them on a collision course with a key reuse problem---the difficulty in scaling reuse libraries in both component sizes and feature variations. Because of the concreteness of conventional, mainstream programming languages, one is torn between combinatorial growth of reuse libraries containing components with good run-time performance or linear growth with poor performance. The paper identifies the extensions necessary to solve the scaling problem, notably 1) factored component libraries based on a ``layers of abstraction" notion, 2) a composition operator and compile-time generator to manufacture combinatorially many custom components from compositions of factors, and 3) extra-linguistic attributes associated with individual programming constructs to make inter-factor dependencies explicit and machine processable. This paper analyses and compares existing reuse systems that contain instances of these extensions and indicates the directions for factored component libraries.},
  Doi                      = {10.1109/ICSR.1994.365806}
}

@InProceedings{1993:icse:biggerstaff,
  Title                    = {The concept assignment problem in program understanding},
  Author                   = {Biggerstaff, Ted J. and Mitbander, Bharat G. and Webster, Dallas},
  Booktitle                = icse,
  Year                     = {1993},
  Pages                    = {482--498},

  Abstract                 = {Concept assignment is a process of recognizing concepts within a computer program and building up an understanding of the program by relating the recognized concepts to portions of the program, its operational context and to one another. The problem of discovering individual human oriented concepts and assigning them to their implementation oriented counterparts for a given program is the concept assignment problem. The authors argue that the solution to this problem requires methods that have a strong plausible reasoning component. They illustrate these ideas through example scenarios using an existing design recovery system called DESIRE. DESIRE is evaluated based on its usage on real-world problems over the years.},
  Doi                      = {10.1109/ICSE.1993.346017}
}

@Article{1994:cacm:biggerstaff,
  Title                    = {Program understanding and the concept assignment problem},
  Author                   = {Biggerstaff, Ted J. and Mitbander, Bharat G. and Webster, Dallas E.},
  Journal                  = cacm,
  Year                     = {1994},

  Month                    = may,
  Number                   = {5},
  Pages                    = {72--82},
  Volume                   = {37},

  Abstract                 = {A person understands a program when able to explain the program, its structure, its behavior, its effects on its operational context, and its relationships to its application domain in terms that are qualitatively different from the tokens used to construct the source code of the program. For example, it is qualitatively different for me to claim that the program ``reserves an airline seat" than for me to assert that ``if (seat = request(flight)) && available(seat) then reserve(seat,customer).'' Apart from the obvious differences in level of detail and formality, the first case expresses computational intent in human-oriented term-terms that involve a rich context of knowledge about the world. In the second case, the vocabulary and grammar are narrowly restricted, formally controlled and do not inherently reference the human-oriented context of knowledge about the world. The first expression of computational intent is designed for succinct, intentionally ambiguous (i.e., informal) human-level communication, whereas the second is designed for automated treatment (e.g., program verification or compilation). Both forms of the information must be present for a human to manipulate programs (create, maintain, explain, reengineer, reuse, or document) in any but the most trivial way. Moreover, one must understand the association between the formal and the informal expressions of computational intent. When a person tries to develop an understanding of an unfamiliar program or portion of a program, the informal, human-oriented expression of computational intent must be created or reconstructed through a process of analysis, experimentation, guessing, and crossword puzzle-like assembly. As the informal concepts are discovered and interrelated concept by concept, they are simultaneously associated with or assigned to the specific implementation structures within the program (and its operational context) that are the concrete instances of those concepts. The problem of discovering these human-oriented concepts and assigning them to their realizations within a specific program or its context is the concept assignment problem [4]. In practice, there are several general strategies and classes of tools that can successfully address this problem. We will illustrate some of these strategies through example scenarios and some classes of tools that support them through examples of the DESIRE (DESign Information Recovery Environment) suite of tools. The problem, strategies, and tools are relevant to anyone who creates, maintains, changes, reengineers, reuses, or otherwise manages the design of a program or system. A central hypothesis of this article is that a parsing-oriented recognition approach based on formal, predominantly structural patterns of programming languages features is necessary but not sufficient for solving the general concept assignment problem. While parsing-oriented recognition schemes certainly play a role in program understanding, the signatures of most human-oriented concepts are not constrained in ways that are convenient for parsing technologies. So there is more to program understanding than parsing.},
  Doi                      = {10.1145/175290.175300}
}

@Book{1999:book:binder,
  Title                    = {Testing Object-Oriented Systems: Models, Patterns, and Tools},
  Author                   = {Robert V. Binder},
  Publisher                = {Addison-Wesley},
  Year                     = {1999}
}

@InProceedings{1992:icsm:binkley,
  Title                    = {Using semantic differencing to reduce the cost of regression testing},
  Author                   = {David Binkley},
  Booktitle                = icsm,
  Year                     = {1992},
  Pages                    = {41--50},

  Abstract                 = {An algorithm is presented that reduces the cost of regression testing by reducing the number of test cases that must be rerun and by reducing the size of the program that they must be run on. The algorithm uses dependence graphs and program slicing to partition the components of the new program into two sets: preserved points-components that have unchanged run-time behaviour; and affected points-components that have changed run-time behavior. Only test cases that test the behavior of affected points just be rerun; the behavior of the preserved points is guaranteed to be the same in the old and new versions of the program. Furthermore, the algorithm produces a program `difference', which captures the behavior of (only) the affected points. Thus, rather than restricting the (large) new program on a large number of test cases, it is possible to certify the new program by running the (smaller) program `differences' on a (smaller) number of test cases.},
  Doi                      = {10.1109/ICSM.1992.242560}
}

@InProceedings{2009:esec_fse:bird,
  Title                    = {Fair and balanced?: Bias in bug-fix datasets},
  Author                   = {Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
  Booktitle                = esec_fse,
  Year                     = {2009},
  Pages                    = {121--130},

  Abstract                 = {Software engineering researchers have long been interested in where and why bugs occur in code, and in predicting where they might turn up next. Historical bug-occurence data has been key to this research. Bug tracking systems, and code version histories, record when, how and by whom bugs were fixed; from these sources, datasets that relate file changes to bug fixes can be extracted. These historical datasets can be used to test hypotheses concerning processes of bug introduction, and also to build statistical bug prediction models. Unfortunately, processes and humans are imperfect, and only a fraction of bug fixes are actually labelled in source code version histories, and thus become available for study in the extracted datasets. The question naturally arises, are the bug fixes recorded in these historical datasets a fair representation of the full population of bug fixes? In this paper, we investigate historical data from several software projects, and find strong evidence of systematic bias. We then investigate the potential effects of ``unfair, imbalanced" datasets on the performance of prediction techniques. We draw the lesson that bias is a critical problem that threatens both the effectiveness of processes that rely on biased datasets to build prediction models and the generalizability of hypotheses tested on biased data.},
  Doi                      = {10.1145/1595696.1595716}
}

@InProceedings{2010:fse:bird,
  Title                    = {{LINKSTER}: Enabling efficient manual inspection and annotation of mined data},
  Author                   = {Bird, Christian and Bachmann, Adrian and Rahman, Foyzur and Bernstein, Abraham},
  Booktitle                = fse,
  Year                     = {2010},
  Pages                    = {369--370},

  Abstract                 = {While many uses of mined software engineering data are automatic in nature, some techniques and studies either require, or can be improved, by manual methods. Unfortunately, manually inspecting, analyzing, and annotating mined data can be difficult and tedious, especially when information from multiple sources must be integrated. Oddly, while there are numerous tools and frameworks for automatically mining and analyzing data, there is a dearth of tools which facilitate manual methods. To fill this void, we have developed LINKSTER, a tool which integrates data from bug databases, source code repositories, and mailing list archives to allow manual inspection and annotation. LINKSTER has already been used successfully by an OSS project lead to obtain data for one empirical study.},
  Doi                      = {10.1145/1882291.1882352}
}

@InProceedings{2007:msr:bird,
  Title                    = {Open Borders? Immigration in Open Source Projects},
  Author                   = {Bird, Christian and Gourley, Alex and Devanbu, Prem and Swaminathan, Anand and Hsu, Greta},
  Booktitle                = msrw,
  Year                     = {2007},
  Pages                    = {6:1--6:8},

  Abstract                 = {Open source software is built by teams of volunteers. Each project has a core team of developers, who have the authority to commit changes to the repository; this team is the elite, committed foundation of the project, selected through a meritocratic process from a larger number of people who participate on the mailing list. Most projects carefully regulate admission of outsiders to full developer privileges; some projects even have formal descriptions of this process. Understanding the factors that influence the ``who, how and when" of this process is critical, both for the sustainability of FLOSS projects, and for outside stakeholders who want to gain entry and succeed. In this paper we mount a quantitative case study of the process by which people join FLOSS projects, using data mined from the Apache web server, Postgres, and Python. We develop a theory of open source project joining, and evaluate this theory based on our data.},
  Doi                      = {10.1109/MSR.2007.23}
}

@InProceedings{2009:issre:bird,
  Title                    = {Putting it all together: Using socio-technical networks to predict failures},
  Author                   = {Bird, Christian and Nagappan, Nachiappan and Gall, Harald and Murphy, Brendan and Devanbu, Premkumar},
  Booktitle                = issre,
  Year                     = {2009},
  Pages                    = {109--119},

  Abstract                 = {Studies have shown that social factors in development organizations have a dramatic effect on software quality. Separately, program dependency information has also been used successfully to predict which software components are more fault prone. Interestingly, the influence of these two phenomena have only been studied separately. Intuition and practical experience suggests,however, that task assignment (i.e. who worked on which components and how much) and dependency structure (which components have dependencies on others) together interact to influence the quality of the resulting software. We study the influence of combined socio-technical software networks on the fault-proneness of individual software components within a system. The network properties of a software component in this combined network are able to predict if an entity is failure prone with greater accuracy than prior methods which use dependency or contribution information in isolation. We evaluate our approach in different settings by using it on Windows Vista and across six releases of the Eclipse development environment including using models built from one release to predict failure prone components in the next release. We compare this to previous work. In every case, our method performs as well or better and is able to more accurately identify those software components that have more post-release failures, with precision and recall rates as high as 85\%.},
  Doi                      = {10.1109/ISSRE.2009.17}
}

@InProceedings{2012:fse:bird,
  Title                    = {Assessing the Value of Branches with What-if Analysis},
  Author                   = {Christian Bird and Thomas Zimmermann},
  Booktitle                = fse,
  Year                     = {2012},
  Pages                    = {45:1--45:11},

  Abstract                 = {Branches within source code management systems (SCMs) allow a software project to divide work among its teams for concurrent development by isolating changes. However, this benefit comes with several costs: increased time required for changes to move through the system and pain and error potential when integrating changes across branches. In this paper, we present the results of a survey to characterize how developers use branches in a large industrial project and common problems that they face. One of the major problems mentioned was the long delay that it takes changes to move from one team to another, which is often caused by having too many branches (branchmania). To monitor branch health, we introduce a novel what-if analysis to assess alternative branch structures with respect to two properties, isolation and liveness. We demonstrate with several scenarios how our what-if analysis can support branch decisions. By removing high-cost-low-benefit branches in Windows based on our what-if analysis, changes would each have saved 8.9 days of delay and only intro-duced 0.04 additional conflicts on average.},
  Doi                      = {10.1145/2393596.2393648}
}

@InProceedings{1992:usenixcpp:bischofberger,
  Title                    = {Sniff: A Pragmatic Approach to a {C++} Programming Environment},
  Author                   = {Walter R. Bischofberger},
  Booktitle                = usenixcpp,
  Year                     = {1992},
  Pages                    = {67--82}
}

@InProceedings{1995:cbr:bisio,
  Title                    = {Cost Estimation of Software Projects through Case Base Reasoning},
  Author                   = {Rossella Bisio and Fabio Malabocchia},
  Booktitle                = cbr,
  Year                     = {1995},
  Pages                    = {11--22},
  Series                   = lncs,
  Volume                   = {1010},

  Abstract                 = {One of the most challenging goals for the software development community is the definition and assessment of techniques and tools enabling the cost estimation of projects in the early phases of the software life cycle. Despite of the increasing needs and the available tools and methods, a satisfactory solution is still to be found. During the last two years, has gained some interest in this community an approach based on analogy. Following this approach, an estimation is made starting from analogies with other software projects met in the past. The main reason for this trend is the increased flexibility with respect to other approaches. The main drawback of this method was the inadequacy of the software technologies to provide an effective and efficient implementation. The experience reported here, demonstrates that CBR techniques allow a natural implementation of the estimation by analogy paradigm, and the software estimation by analogy in particular.},
  Doi                      = {10.1007/3-540-60598-3_2}
}

@InProceedings{2010:wcre:bittencourt,
  Title                    = {Improving Automated Mapping in Reflexion Models Using Information Retrieval Techniques},
  Author                   = {Bittencourt, Roberto Almeida and Jansen de Souza Santos, Gustavo and Guerrero, Dalton Dario Serey and Murphy, Gail C.},
  Booktitle                = wcre,
  Year                     = {2010},
  Pages                    = {163--172},

  Abstract                 = {The reflexion model technique supports structural conformance checking of software systems. The scalability of the technique is limited by the requirement for developers to provide a mapping between implementation entities and high-level modules. Such mapping is usually based on regular expressions, and it must also be updated as the system evolves. Automated mapping techniques exist but are solely based on structural dependencies. In this paper, we introduce an automated mapping technique for reflexion models based on information retrieval techniques. It makes use of source code vocabulary and their similarity to the vocabulary of high-level modules. Two case studies conducted across four systems show that the nature of the module view influences which technique performs best. Results also show that combining both mapping techniques into a two step-mapping algorithm generally increases recall, while keeping precision similar to the best approach used in isolation.},
  Doi                      = {10.1109/WCRE.2010.26}
}

@InProceedings{2003:cbr:bjornestad,
  Title                    = {Analogical Reasoning for Reuse of Object-Oriented Specifications},
  Author                   = {Solveig Bj{\o}rnestad},
  Booktitle                = cbr,
  Year                     = {2003},
  Pages                    = {50--64},
  Series                   = lncs,
  Volume                   = {2689},

  Abstract                 = {Software reuse means to use again software components built successfully for previous projects. To be successful, techniques for reuse should be incorporated into the development environment. This paper presents an approach where analogical reasoning is used to identify potentially reusable analysis models. A prototype implementation with focus on the repository and analogical reasoning mechanism is presented. All models in the repository are described in terms of their structure. Semantic similarity among models is found by identifying distance in a semantic net built on WordNet, an electronic, lexical database. During retrieval of potential analogies, information about structure and semantics of models is used. During mapping, genetic algorithms are used to optimize the mapping between two models based on their structure and semantics. Experiments are described in which analogies are identified from the models in the repository. The results reported show that this approach is viable.},
  Doi                      = {10.1007/3-540-45006-8_7},
  Key                      = {Bjornestad}
}

@InProceedings{2005:chi:blackmon,
  Title                    = {Tool for accurately predicting website navigation problems, non-problems, problem severity, and effectiveness of repairs},
  Author                   = {Blackmon, Marilyn Hughes and Kitajima, Muneo and Polson, Peter G.},
  Booktitle                = chi,
  Year                     = {2005},
  Pages                    = {31--40},

  Abstract                 = {The Cognitive Walkthrough for the Web (CWW) is a partially automated usability evaluation method for identifying and repairing website navigation problems. Building on five earlier experiments [3,4], we first conducted two new experiments to create a sufficiently large dataset for multiple regression analysis. Then we devised automatable problem-identification rules and used multiple regression analysis on that large dataset to develop a new CWW formula for accurately predicting problem severity. We then conducted a third experiment to test the prediction formula and refined CWW against an independent dataset, resulting in full cross-validation of the formula. We conclude that CWW has high psychological validity, because CWW gives us (a) accurate measures of problem severity, (b) high success rates for repairs of identified problems (c) high hit rates and low false alarms for identifying problems, and (d) high rates of correct rejections and low rates of misses for identifying non-problems.},
  Doi                      = {10.1145/1054972.1054978}
}

@InProceedings{2006:oopsla:bloch,
  Title                    = {How to design a good {API} and why it matters},
  Author                   = {Bloch, Joshua},
  Booktitle                = oopslacomp,
  Year                     = {2006},
  Pages                    = {506--507},

  Abstract                 = {In lieu of a traditional abstract, I've tried to distill the essence of the talk into a collection of maxims: All programmers are API designers. Good programs are modular, and intermodular boundaries define APIs. Good modules get reused. APIs can be among your greatest assets or liabilities. Good APIs create long-term customers; bad ones create long-term support nightmares. Public APIs, like diamonds, are forever. You have one chance to get it right so give it your best. APIs should be easy to use and hard to misuse. It should be easy to do simple things; possible to do complex things; and impossible, or at least difficult, to do wrong things. APIs should be self-documenting: It should rarely require documentation to read code written to a good API. In fact, it should rarely require documentation to write it. When designing an API, first gather requirements---with a healthy degree of skepticism. People often provide solutions; it's your job to ferret out the underlying problems and find the best solutions. Structure requirements as use-cases: they are the yardstick against which you'll measure your API. Early drafts of APIs should be short, typically one page with class and method signatures and one-line descriptions. This makes it easy to restructure the API when you don't get it right the first time. Code the use-cases against your API before you implement it, even before you specify it properly. This will save you from implementing, or even specifying, a fundamentally broken API. Maintain the code for uses-cases as the API evolves. Not only will this protect you from rude surprises, but the resulting code will become the examples for the API, the basis for tutorials and tests. Example code should be exemplary. If an API is used widely, its examples will be the archetypes for thousands of programs. Any mistakes will come back to haunt you a thousand fold. You can't please everyone so aim to displease everyone equally. Most APIs are overconstrained. Expect API-design mistakes due to failures of imagination. You can't reasonably hope to imagine everything that everyone will do with an API, or how it will interact with every other part of a system. API design is not a solitary activity. Show your design to as many people as you can, and take their feedback seriously. Possibilities that elude your imagination may be clear to others. Avoid fixed limits on input sizes. They limit usefulness and hasten obsolescence. If it's hard to find good names, go back to the drawing board. Don't be afraid to split or merge an API, or embed it in a more general setting. If names start falling into place, you're on the right track. Names matter. Strive for intelligibility, consistency, and symmetry. Every API is a little language, and people must learn to read and write it. If you get an API right, code will read like prose. When in doubt, leave it out. If there is a fundamental theorem of API design, this is it. It applies equally to functionality, classes, methods, and parameters. Every facet of an API should be as small as possible, but no smaller. You can always add things later, but you can't take them away. Minimizing conceptual weight is more important than class- or method-count. Keep APIs free of implementations details. They confuse users and inhibit the flexibility to evolve. It isn't always obvious what's an implementation detail: Be wary of overspecification. Minimize mutability. Immutable objects are simple, thread-safe, and freely sharable. Documentation matters. No matter how good an API, it won't get used without good documentation. Document every exported API element: every class, method, field, and parameter. Consider the performance consequences of API design decisions, but don't warp an API to achieve performance gains. Luckily, good APIs typically lend themselves to fast implementations. APIs must coexist peacefully with the platform, so do what is customary. It is almost always wrong to ``transliterate" an API from one platform to another. Minimize accessibility; when in doubt, make it private. This simplifies APIs and reduces coupling. Subclass only if you can say with a straight face that every instance of the subclass is an instance of the superclass. Exposed classes should never subclass just to reuse implementation code. Design and document for inheritance or else prohibit it. This documentation takes the form of self-use patterns: how methods in a class use one another. Without it, safe subclassing is impossible. Don't make the client do anything the library could do. Violating this rule leads to boilerplate code in the client, which is annoying and error-prone. Obey the principle of least astonishment. Every method should do the least surprising thing it could, given its name. If a method doesn't do what users think it will, bugs will result. Fail fast. The sooner you report a bug, the less damage it will do. Compile-time is best. If you must fail at run-time, do it as soon as possible. Provide programmatic access to all data available in string form. Otherwise, programmers will be forced to parse strings, which is painful. Worse, the string forms will turn into de facto APIs. Overload with care. If the behaviors of two methods differ, it's better to give them different names. Use the right data type for the job. For example, don't use string if there is a more appropriate type. Use consistent parameter ordering across methods. Otherwise, programmers will get it backwards. Avoid long parameter lists, especially those with multiple consecutive parameters of the same type. Avoid return values that demand exceptional processing. Clients will forget to write the special-case code, leading to bugs. For example, return zero-length arrays or collections rather than nulls. Throw exceptions only to indicate exceptional conditions. Otherwise, clients will be forced to use exceptions for normal flow control, leading to programs that are hard to read, buggy, or slow. Throw unchecked exceptions unless clients can realistically recover from the failure. API design is an art, not a science. Strive for beauty, and trust your gut. Do not adhere slavishly to the above heuristics, but violate them only infrequently and with good reason.},
  Doi                      = {10.1145/1176617.1176622}
}

@Article{2004:siam:blondel,
  Title                    = {A Measure of Similarity between Graph Vertices: Applications to Synonym Extraction and Web Searching},
  Author                   = {Blondel, Vincent D. and Gajardo, Anah\'{\i} and Heymans, Maureen and Senellart, Pierre and Van Dooren, Paul},
  Journal                  = siam,
  Year                     = {2004},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {647--666},
  Volume                   = {46},

  Abstract                 = {We introduce a concept of similarity between vertices of directed graphs. Let $G_A$ and $G_B$ be two directed graphs with, respectively, $n_A$ and $n_B$ vertices. We define an $n_B \times n_A$ similarity matrix $\mathbf S$ whose real entry $s_{ij}$ expresses how similar vertex $j$ (in $G_A$) is to vertex $i$ (in $G_B$): we say that $s_{ij}$ is their similarity score. The similarity matrix can be obtained as the limit of the normalized even iterates of $S_k + 1 = BS_kA^T + B^TS_kA$, where $A$ and $B$ are adjacency matrices of the graphs and $S_0$ is a matrix whose entries are all equal to 1. In the special case where $G_A = G_B = G$, the matrix $\mathbf S$ is square and the score $s_{ij}$ is the similarity score between the vertices $i$ and $j$ of $G$. We point out that Kleinberg's ``hub and authority'' method to identify web-pages relevant to a given query can be viewed as a special case of our definition in the case where one of the graphs has two vertices and a unique directed edge between them. In analogy to Kleinberg, we show that our similarity scores are given by the components of a dominant eigenvector of a nonnegative matrix. Potential applications of our similarity concept are numerous. We illustrate an application for the automatic extraction of synonyms in a monolingual dictionary.},
  Doi                      = {10.1137/S0036144502415960},
  Issue_date               = {2004}
}

@Article{1999:computer:boehm,
  Title                    = {Managing software productivity and reuse},
  Author                   = {Barry Boehm},
  Journal                  = computer,
  Year                     = {1999},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {111--113},
  Volume                   = {32},

  Abstract                 = {Your organization can choose from three main strategies for improving its software productivity. You can work faster, using tools that automate or speed up previously labor-intensive tasks. You can work smarter, primarily through process improvements that avoid or reduce non-value-adding tasks. Or you can avoid unnecessary work by reusing software artifacts instead of custom developing each project. Which strategy will produce the highest payoff? The author performed an extensive analysis that addressed this question for the US Department of Defense. The result of this analysis showed that work avoidance via software reuse produced the highest improvement in software productivity. The article gives advice on how to manage software reuse and the pitfalls to avoid.},
  Doi                      = {10.1109/2.789755}
}

@InProceedings{2002:icsm:bohner,
  Title                    = {Software change impacts: An evolving perspective},
  Author                   = {Bohner, S. A.},
  Booktitle                = icsm,
  Year                     = {2002},
  Pages                    = {263--272},

  Abstract                 = {As software engineering practice evolves to respond to demands for distributed applications on heterogeneous platforms, software change is increasingly influenced by middleware and components. Interoperability dependency relationships now point to more relevant impacts of software change and necessarily drive the analysis. Software changes to software systems that incorporate middleware components like Web services expose these systems and the organizations they serve to unforeseen ripple effects that frequently result in failures. Current software change impact analysis models have not adequately addressed this trend. Moreover, as software systems grow in size and complexity, the dependency webs of information extend beyond most software engineers ability to comprehend them. This paper examines preliminary research for extending current software change impact analysis to incorporate interoperability dependency relationships for addressing distributed applications and explores three dimensional (3D) visualization techniques for more effective navigation of software changes.},
  Doi                      = {10.1109/ICSM.2002.1167777}
}

@Article{2009:software:booch,
  Title                    = {The Defenestration of Superfluous Architectural Accoutrements},
  Author                   = {Booch, Grady},
  Journal                  = software,
  Year                     = {2009},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {7--8},
  Volume                   = {26},

  Abstract                 = {Simple architectures have conceptual integrity and are better than more complex ones. Continuous architectural refactoring helps to converge a system to its practical and optimal simplicity.},
  Doi                      = {10.1109/MS.2009.105}
}

@Article{2003:jasist:borlund,
  Title                    = {The concept of relevance in {IR}},
  Author                   = {Borlund, Pia},
  Journal                  = jasist,
  Year                     = {2003},

  Month                    = aug,
  Number                   = {10},
  Pages                    = {913--925},
  Volume                   = {54},

  Abstract                 = {This article introduces the concept of relevance as viewed and applied in the context of IR evaluation, by presenting an overview of the multidimensional and dynamic nature of the concept. The literature on relevance reveals how the relevance concept, especially in regard to the multidimensionality of relevance, is many faceted, and does not just refer to the various relevance criteria users may apply in the process of judging relevance of retrieved information objects. From our point of view, the multidimensionality of relevance explains why some will argue that no consensus has been reached on the relevance concept. Thus, the objective of this article is to present an overview of the many different views and ways by which the concept of relevance is used---leading to a consistent and compatible understanding of the concept. In addition, special attention is paid to the type of situational relevance. Many researchers perceive situational relevance as the most realistic type of user relevance, and therefore situational relevance is discussed with reference to its potential dynamic nature, and as a requirement for interactive information retrieval (IIR) evaluation.},
  Doi                      = {10.1002/asi.10286}
}

@InCollection{2004:ewsa:bosch,
  Title                    = {Software Architecture: The Next Step},
  Author                   = {Bosch, Jan},
  Booktitle                = ewsa,
  Year                     = {2004},
  Pages                    = {194--199},
  Series                   = lncs,
  Volume                   = {3047},

  Abstract                 = {This position paper makes the following claims that, in our opinion, are worthwhile to discuss at the workshop. 1) The first phase of software architecture research, where the key concepts are components and connectors, has matured the technology to a level where industry adoption is wide-spread and few fundamental issues remain. 2) The traditional view on software architecture suffers from a number of key problems that cannot be solved without changing our perspective on the notion of software architecture. These problems include the lack of first-class representation of design decisions, the fact that these design decisions are cross-cutting and intertwined, that these problems lead to high maintenance cost, because of which design rules and constraints are easily violated and obsolete design decisions are not removed. 3) As a community, we need to take the next step and adopt the perspective that a software architecture is, fundamentally, a composition of architectural design decisions. These design decisions should be represented as first-class entities in the software architecture and it should, at least before system deployment, be possible to add, remove and change architectural design decisions against limited effort.},
  Doi                      = {10.1007/978-3-540-24769-2_14}
}

@InProceedings{2001:pfe:bosch,
  Title                    = {Variability Issues in Software Product Lines},
  Author                   = {Bosch, Jan and Florijn, Gert and Greefhorst, Danny and Kuusela, Juha and Obbink, J. Henk and Pohl, Klaus},
  Booktitle                = pfe,
  Year                     = {2002},
  Pages                    = {13--21},
  Series                   = lncs,
  Volume                   = {2290},

  Abstract                 = {Software product lines (or system families) have achieved considerable adoption by the software industry. A software product line captures the commonalities between a set of products while providing for the differences. Differences are managed by delaying design decisions, thereby introducing variation points. The whole of variation points is typically referred to as the variability of the software product line. Variability management is, however, not a trivial activity and several issues exist, both in general as well as specific to individual phases in the lifecycle. This paper identifies and describes several variability issues based on practical experiences and theoretical understanding of the problem domain.}
}

@InProceedings{1999:ecbs:bosch,
  Title                    = {Software architecture design: Evaluation and transformation},
  Author                   = {Bosch, Jan and Molin, Peter},
  Booktitle                = ecbs,
  Year                     = {1999},
  Pages                    = {4--10},

  Abstract                 = {Since the architecture of a software system constrains the quality attributes, the decisions taken during architectural design have a large impact on the resulting system. An architectural design method is presented that employs iterative evaluation and transformation of the software architecture in order to satisfy the quality requirements. Architecture evaluation is performed by using scenarios, simulation, mathematical modelling and experience-based reasoning. The architecture can be transformed by imposing an architectural style, imposing an architectural pattern, using a design pattern, converting a quality requirement to fiunctionality and by distributing quality requirements. The method has evolved through its application in several industrial projects.},
  Doi                      = {10.1109/ECBS.1999.755855}
}

@InProceedings{2004:oopsla:boshernitsan,
  Title                    = {{iXj}: Interactive source-to-source transformations for {J}ava},
  Author                   = {Boshernitsan, Marat and Graham, Susan L.},
  Booktitle                = oopslacomp,
  Year                     = {2004},
  Pages                    = {212--213},

  Abstract                 = {Manual large-scale modification or generation of source code can be tedious and error-prone. Integrating scriptable source-to-source program transformations into development environments will assist developers with this overwhelming task. We discuss various usability issues of bringing such ad-hoc transformations to end-users and describe a developer-oriented interactive source code transformation tool for Java that we are building.},
  Doi                      = {10.1145/1028664.1028755}
}

@InProceedings{2007:chi:boshernitsan,
  Title                    = {Aligning Development Tools with the Way Programmers Think about Code Changes},
  Author                   = {Marat Boshernitsan and Susan L. Graham and Marti A. Hearst},
  Booktitle                = chi,
  Year                     = {2007},
  Pages                    = {567--576},

  Abstract                 = {Software developers must modify their programs to keepup with changing requirements and designs. Often, aconceptually simple change can require numerous editsthat are similar but not identical, leading to errors andomissions. Researchers have designed programming environmentsto address this problem, but most of thesesystems are counter-intuitive and difficult to use.By applying a task-centered design process, we developeda visual tool that allows programmers to makecomplex code transformations in an intuitive manner.This approach uses a representation that aligns wellwith programmers' mental models of programming structures.The visual language combines textual and graphicalelements and is expressive enough to support a broadrange of code-changing tasks. To simplify learning thesystem, its user interface scaffolds construction and executionof transformations. An evaluation with Java programmerssuggests that the interface is intuitive, easyto learn, and effective on a representative editing task.},
  Doi                      = {10.1145/1240624.1240715}
}

@InProceedings{2006:wcre:bouktif,
  Title                    = {Extracting Change-patterns from {CVS} Repositories},
  Author                   = {Salah Bouktif and Yann-Ga{\"e}l Gu{\'e}h{\'e}neuc and Giuliano Antoniol},
  Booktitle                = wcre,
  Year                     = {2006},
  Pages                    = {221--230},

  Abstract                 = {Often, the only sources of information about the evolution of software systems are the systems themselves and their histories. Version control repositories contain information on several thousand of files and on millions of changes. We propose an approach based on dynamic time warping to discover change-patterns, which, for example, describe files that change together almost all the time. We define the synchrony change-pattern to answer the question: given a software system and one file under modification, what others files must be changed? We have applied our approach on PADL, a software system developed in Java, and on Mozilla. Interesting results are achieved even when the discovered groups of co-changing files are compared with these provided by experts},
  Doi                      = {10.1109/WCRE.2006.27}
}

@Manual{1977:manual:bourne,
  Title                    = {An Introduction to the {UNIX} Shell},
  Author                   = {S. R. Bourne},
  Organization             = {Bell Laboratories},
  Year                     = {1977}
}

@InProceedings{2007:csmr:bourquin,
  Title                    = {High-impact Refactoring Based on Architecture Violations},
  Author                   = {Bourquin, F. and Keller, R. K.},
  Booktitle                = csmr,
  Year                     = {2007},
  Pages                    = {149--158},

  Abstract                 = {Software refactoring has been identified as a key technique for the maintenance and evolution of object-oriented systems. Most interesting are high-impact refactorings, that is, refactorings that have a strong impact on the quality of the system's architecture. ``Bad smells" and code metrics have been suggested as means for identifying refactoring needs. According to our experience these techniques are useful yet, in order to spot opportunities for high-impact refactorings, they should be complemented with the analysis of architectural violations. The subject of this report is a mid-sized Java enterprise application from the telecommunications domain whose functionality had to be radically extended We show how we combined several tools and techniques to identify' opportunities for high-impact refactorings, and discuss the resulting architecture, the refactoring process, tool support as well as related experiences},
  Doi                      = {10.1109/CSMR.2007.25}
}

@InProceedings{2007:csmr:bourqun,
  Title                    = {High-impact Refactoring Based on Architecture Violations},
  Author                   = {Bourqun, Fabrice and Keller, Rudolf K.},
  Booktitle                = csmr,
  Year                     = {2007},
  Pages                    = {149--158},

  Abstract                 = {Software refactoring has been identified as a key technique for the maintenance and evolution of object-oriented systems. Most interesting are high-impact refactorings, that is, refactorings that have a strong impact on the quality of the system's architecture. ``Bad smells'' and code metrics have been suggested as means for identifying refactoring needs. According to our experience these techniques are useful yet, in order to spot opportunities for high-impact refactorings, they should be complemented with the analysis of architectural violations. The subject of this report is a mid-sized Java enterprise application from the telecommunications domain whose functionality had to be radically extended We show how we combined several tools and techniques to identify' opportunities for high-impact refactorings, and discuss the resulting architecture, the refactoring process, tool support as well as related experiences.},
  Doi                      = {10.1109/CSMR.2007.25}
}

@Article{1998:tosem:bowdidge,
  Title                    = {Supporting the restructuring of data abstractions through manipulation of a program visualization},
  Author                   = {Bowdidge, Robert W. and Griswold, William G.},
  Journal                  = tosem,
  Year                     = {1998},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {109--157},
  Volume                   = {7},

  Abstract                 = {With a meaning-preserving restructuring tool, a software engineer can change a program's structure to ease future modifications. However, deciding how to restructure the program requires a global understanding of the program's structure, which cannot be derived easily by directly inspecting the source code. We describe a manipulable program visualization---the star diagram---that supports the restructuring task of encapsulating a global data structure. The star diagram graphically displays information pertinent to encapsulation, and direct manipulation of the diagram causes the underlying program to be restructured. The visualization compactly presents all statements in the program that use the given global data structure, helping the programmer to choose the functions that completely encapsulate it. Additionally, the visualization elides code unrelated to the data structure and to the task and collapses similar expressions to help the programmer identify frequently occurring code fragments and manipulate them together. The visualization is mapped directly to the program text, so manipulation of the visualization also restructures the program. We present the star diagram concept and describe an implementation of the star diagram built upon a meaning-preserving restructuring tool for Scheme. We also describe our creation of star diagram generators for C programs, and we test the scalability of the star diagram using large C and MUMPS programs.},
  Doi                      = {10.1145/279310.279312}
}

@InProceedings{2010:icse:bragdon,
  Title                    = {{Code Bubbles}: Rethinking the user interface paradigm of integrated development environments},
  Author                   = {Bragdon, Andrew and Reiss, Steven P. and Zeleznik, Robert and Karumuri, Suman and Cheung, William and Kaplan, Joshua and Coleman, Christopher and Adeputra, Ferdi and LaViola, Jr., Joseph J.},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {455--464},
  Volume                   = {1},

  Abstract                 = {Today's integrated development environments (IDEs) are hampered by their dependence on files and file-based editing. We propose a novel user interface that is based on collections of lightweight editable fragments, called bubbles, which when grouped together form concurrently visible working sets. In this paper we describe the design of a prototype IDE user interface for Java based on working sets. A quantitative evaluation shows that developers could expect to view a sizeable number of functions concurrently with relatively few UI operations. A qualitative user evaluation with 23 professional developers indicates a high level of excitement, interest, and potential benefits and uses.},
  Doi                      = {10.1145/1806799.1806866}
}

@InProceedings{2010:chi:bragdon,
  Title                    = {{Code Bubbles}: A working set-based interface for code understanding and maintenance},
  Author                   = {Bragdon, Andrew and Zeleznik, Robert and Reiss, Steven P. and Karumuri, Suman and Cheung, William and Kaplan, Joshua and Coleman, Christopher and Adeputra, Ferdi and LaViola, Jr., Joseph J.},
  Booktitle                = chi,
  Year                     = {2010},
  Pages                    = {2503--2512},

  Abstract                 = {Developers spend significant time reading and navigating code fragments spread across multiple locations. The file-based nature of contemporary IDEs makes it prohibitively difficult to create and maintain a simultaneous view of such fragments. We propose a novel user interface metaphor for code understanding based on collections of lightweight, editable fragments called bubbles, which form concurrently visible working sets. We present the results of a qualitative usability evaluation, and the results of a quantitative study which indicates Code Bubbles significantly improved code understanding time, while reducing navigation interactions over a widely-used IDE, for two controlled tasks.},
  Doi                      = {10.1145/1753326.1753706}
}

@InProceedings{2010:chi:brandt,
  Title                    = {Example-centric programming: integrating web search into the development environment},
  Author                   = {Brandt, Joel and Dontcheva, Mira and Weskamp, Marcos and Klemmer, Scott R.},
  Booktitle                = chi,
  Year                     = {2010},
  Pages                    = {513--522},

  Abstract                 = {The ready availability of online source-code examples has fundamentally changed programming practices. However, current search tools are not designed to assist with programming tasks and are wholly separate from editing tools. This paper proposes that embedding a task-specific search engine in the development environment can significantly reduce the cost of finding information and thus enable programmers to write better code more easily. This paper describes the design, implementation, and evaluation of Blueprint, a Web search interface integrated into the Adobe Flex Builder development environment that helps users locate example code. Blueprint automatically augments queries with code context, presents a code-centric view of search results, embeds the search experience into the editor, and retains a link between copied code and its source. A comparative laboratory study found that Blueprint enables participants to write significantly better code and find example code significantly faster than with a standard Web browser. Analysis of three months of usage logs with 2,024 users suggests that task-specific search interfaces can significantly change how and when people search the Web.},
  Doi                      = {10.1145/1753326.1753402}
}

@InProceedings{2009:chi:brandt,
  Title                    = {Two studies of opportunistic programming: Interleaving web foraging, learning, and writing code},
  Author                   = {Brandt, Joel and Guo, Philip J. and Lewenstein, Joel and Dontcheva, Mira and Klemmer, Scott R.},
  Booktitle                = chi,
  Year                     = {2009},
  Pages                    = {1589--1598},

  Abstract                 = {This paper investigates the role of online resources in problem solving. We look specifically at how programmers---an exemplar form of knowledge workers---opportunistically interleave Web foraging, learning, and writing code. We describe two studies of how programmers use online resources. The first, conducted in the lab, observed participants' Web use while building an online chat room. We found that programmers leverage online resources with a range of intentions: They engage in just-in-time learning of new skills and approaches, clarify and extend their existing knowledge, and remind themselves of details deemed not worth remembering. The results also suggest that queries for different purposes have different styles and durations. Do programmers' queries ``in the wild" have the same range of intentions, or is this result an artifact of the particular lab setting? We analyzed a month of queries to an online programming portal, examining the lexical structure, refinements made, and result pages visited. Here we also saw traits that suggest the Web is being used for learning and reminding. These results contribute to a theory of online resource usage in programming, and suggest opportunities for tools to facilitate online knowledge work.},
  Doi                      = {10.1145/1518701.1518944}
}

@Article{2012:ist:breivold,
  Title                    = {A systematic review of software architecture evolution research},
  Author                   = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
  Journal                  = ist,
  Year                     = {2012},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {16--40},
  Volume                   = {54},

  Abstract                 = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
  Doi                      = {10.1016/j.infsof.2011.06.002}
}

@Article{2012:jss:breivold,
  Title                    = {Software architecture evolution through evolvability analysis},
  Author                   = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
  Journal                  = jss,
  Year                     = {2012},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {2574--2592},
  Volume                   = {85},

  Abstract                 = {Software evolvability is a multifaceted quality attribute that describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for the efficient implementation of strategic decisions, and the increasing economic value of software. For long life systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. However, designing and evolving software architectures are the challenging task. To improve the ability to understand and systematically analyze the evolution of software system architectures, in this paper, we describe software architecture evolution characterization, and propose an architecture evolvability analysis process that provides replicable techniques for performing activities to aim at understanding and supporting software architecture evolution. The activities are embedded in: (i) the application of a software evolvability model; (ii) a structured qualitative method for analyzing evolvability at the architectural level; and (iii) a quantitative evolvability analysis method with explicit and quantitative treatment of stakeholders' evolvability concerns and the impact of potential architectural solutions on evolvability. The qualitative and quantitative assessments manifested in the evolvability analysis process have been applied in two large-scale industrial software systems at ABB and Ericsson, with experiences and reflections described.},
  Doi                      = {10.1016/j.jss.2012.05.085}
}

@InProceedings{2008:seaa:breivold,
  Title                    = {Migrating Industrial Systems towards Software Product Lines: Experiences and Observations through Case Studies},
  Author                   = {Breivold, Hongyu Pei and Larsson, Stig and Land, Rikard},
  Booktitle                = seaa,
  Year                     = {2008},
  Pages                    = {232--239},

  Abstract                 = {Software product line engineering has emerged as one of the dominant paradigms for developing variety of software products based on a shared platform and shared software artifacts. An important and challenging type of software maintenance and evolution is how to cost-effectively manage the migration of legacy systems towards product lines. This paper presents a structured migration method and describes our experiences in migrating industrial legacy systems into product lines. In addition, we present a number of specific recommendations for the transition process which will be of value to organizations that are considering a product line approach to their business. The recommendations cover four perspectives: business, organization, product development processes and technology.},
  Doi                      = {10.1109/SEAA.2008.13}
}

@Article{2007:jss:brereton,
  Title                    = {Lessons from applying the systematic literature review process within the software engineering domain},
  Author                   = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
  Journal                  = jss,
  Year                     = {2007},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {571--583},
  Volume                   = {80},

  Abstract                 = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
  Doi                      = {10.1016/j.jss.2006.07.009}
}

@InProceedings{2010:cscw:breu,
  Title                    = {Investigating information needs to improve cooperation between developers and bug reporters},
  Author                   = {Silvia Breu and Rahul Premraj and Jonathan Sillito and Thomas Zimmermann},
  Booktitle                = cscw,
  Year                     = {2010},
  Pages                    = {301--310},

  Abstract                 = {For many software projects, bug tracking systems play a central role in supporting collaboration between the developers and the users of the software. To better understand this collaboration and how tool support can be improved, we have quantitatively and qualitatively analysed the questions asked in a sample of 600 bug reports from the MOZILLA and ECLIPSE projects. We categorised the questions and analysed response rates and times by category and project. Our results show that the role of users goes beyond simply reporting bugs: their active and ongoing participation is important for making progress on the bugs they report. Based on the results, we suggest four ways in which bug tracking systems can be improved.},
  Doi                      = {10.1145/1718918.1718973}
}

@InProceedings{2006:ase:breu,
  Title                    = {Mining Aspects from Version History},
  Author                   = {Breu, Silvia and Zimmermann, Thomas},
  Booktitle                = ase,
  Year                     = {2006},
  Pages                    = {221--230},

  Abstract                 = {Aspect mining identifies cross-cutting concerns in a program to help migrating it to an aspect-oriented design. Such concerns may not exist from the beginning, but emerge over time. By analysing where developers add code to a program, our history-based aspect mining (HAM) identifies and ranks cross-cutting concerns. We evaluated the effectiveness of our approach with the history of three open-source projects. HAM scales up to industrial-sized projects: for example, we were able to identify a locking concern that cross-cuts 1 284 methods in Eclipse. Additionally, the precision of HAM increases with project size and history; for Eclipse, it reaches 90\% for the top-10 candidates.},
  Doi                      = {10.1109/ASE.2006.50}
}

@InProceedings{2006:etx:breu,
  Title                    = {{HAM}: Cross-cutting concerns in {E}clipse},
  Author                   = {Breu, Silvia and Zimmermann, Thomas and Lindig, Christian},
  Booktitle                = etx,
  Year                     = {2006},
  Pages                    = {21--24},

  Abstract                 = {As programs evolve, newly added functionality sometimes no longer aligns with the original design, ending up scattered across the software system. Aspect mining tries to identify such crosscutting concerns in a program to support maintenance, or as a first step towards an aspect-oriented program. Previous approaches to aspect mining applied static or dynamic program analysis techniques to a single version of a system. We exploit all versions from a system's CVS history to mine aspect candidates; we are about to extend our research prototype to an Eclipse plug-in called HAM: when a single CVS commit adds calls to the same (small) set of methods in many unrelated locations, these method calls are likely to be cross-cutting. HAM employs formal concept analysis to identify aspect candidates. Analysing one commit operation at a time makes the approach scale to industrial-sized programs. In an evaluation we mined cross-cutting concerns from Eclipse 3.2M3 and found that up to 90\% of the top-10 aspect candidates are truly cross-cutting concerns.},
  Doi                      = {10.1145/1188835.1188840}
}

@InProceedings{1997:icse:briand,
  Title                    = {An Investigation into Coupling Measures for {C++}},
  Author                   = {Lionel Briand and Prem Devanbu and Walcelio Melo},
  Booktitle                = icse,
  Year                     = {1997},
  Pages                    = {412--421},

  Abstract                 = {This paper proposes a comprehensive suite of measures to quantify the level of class coupling during the design of object-oriented systems. This suite takes into account the different OO design mechanisms provided by the C++ language (e.g., friendship between classes, specialization, and aggregation) but it can be tailored to other OO languages. The different measures in our suite thus reflect different hypotheses about the different mechanisms of coupling in 00 systems. Based on actual project defect data, the hypotheses underlying our coupling measures are empirically validated by analyzing their relationship with the probability of fault detection across classes. The results demonstrate that some of these coupling measures may be useful early quality indicators of the design of OO systems. These measures are conceptually different from the OO design measures defined by Chidamber and Kemerer; in addition, our data suggests that they are complementary quality indicators.}
}

@InProceedings{1998:metrics:briand,
  Title                    = {A comprehensive empirical validation of design measures for object-oriented systems},
  Author                   = {Lionel C. Briand and John Daly and Victor Porter and J{\"u}rgen W{\"u}st},
  Booktitle                = metrics,
  Year                     = {1998},
  Pages                    = {246--257},

  Abstract                 = {This paper aims at empirically exploring the relationships between existing object-oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness.}
}

@Article{1998:ese:briand,
  Title                    = {A unified framework for cohesion measurement in object-oriented systems},
  Author                   = {Lionel C. Briand and John W. Daly and J{\"u}rgen W{\"u}st},
  Journal                  = ese,
  Year                     = {1998},

  Month                    = {1 } # mar,
  Number                   = {1},
  Pages                    = {65--117},
  Volume                   = {3},

  Abstract                 = {The increasing importance being placed on software measurement has led to an increased amount of research developing new software measures. Given the importance of object-oriented development techniques, one specific area where this has occurred is cohesion measurement in object-oriented systems. However, despite a very interesting body of work, there is little understanding of the motivation and empirical hypotheses behind many of these new measures. It is often difficult to determine how such measures relate to one another and for which application they can be used. As a consequence, it is very difficult for practitioners and researchers to obtain a clear picture of the state-of-the-art in order to select or define cohesion measures for object-oriented systems. This situation is addressed and clarified through several different activities. First, a standardized terminology and formalism for expressing measures is provided which ensures that all measures using it are expressed in a fully consistent and operational manner. Second, to provide a structured synthesis, a review of the existing approaches to measure cohesion in object-oriented systems takes place. Third, a unified framework, based on the issues discovered in the review, is provided and all existing measures are then classified according to this framework. Finally, a review of the empirical validation work concerning existing cohesion measures is provided. This paper contributes to an increased understanding of the state-of-the-art: a mechanism is provided for comparing measures and their potential use, integrating existing measures which examine the same concepts in different ways, and facilitating more rigorous decision making regarding the definition of new measures and the selection of existing measures for a specific goal of measurement. In addition, our review of the state-of-the-art highlights several important issues: (i) many measures are not defined in a fully operational form, (ii) relatively few of them are based on explicit empirical models as recommended by measurement theory, and (iii) an even smaller number of measures have been empirically validated; thus, the usefulness of many measures has yet to be demonstrated.},
  Doi                      = {10.1023/A:1009783721306}
}

@Article{1999:tse:briand:a,
  Title                    = {A unified framework for coupling measurement in object-oriented systems},
  Author                   = {Lionel C. Briand and John W. Daly and J{\"u}rgen K. W{\"u}st},
  Journal                  = tse,
  Year                     = {1999},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {91--121},
  Volume                   = {25},

  Abstract                 = {The increasing importance being placed on software measurement has led to an increased amount of research developing new software measures. Given the importance of object-oriented development techniques, one specific area where this has occurred is coupling measurement in object-oriented systems. However, despite a very interesting and rich body of work, there is little understanding of the motivation and empirical hypotheses behind many of these new measures. It is often difficult to determine how such measures relate to one another and for which application they can be used. As a consequence, it is very difficult for practitioners and researchers to obtain a clear picture of the state-of-the-art in order to select or define measures for object-oriented systems. This situation is addressed and clarified through several different activities. First, a standardized terminology and formalism for expressing measures is provided which ensures that all measures using it are expressed in a fully consistent and operational manner. Second, to provide a structured synthesis, a review of the existing frameworks and measures for coupling measurement in object-oriented systems takes place. Third, a unified framework, based on the issues discovered in the review, is provided and all existing measures are then classified according to this framework. This paper contributes to an increased understanding of the state-of-the-art: A mechanism is provided for comparing measures and their potential use, integrating existing measures which examine the same concepts in different ways, and facilitating more rigorous decision making regarding the definition of new measures and the selection of existing measures for a specific goal of measurement. In addition, our review of the state-of-the-art highlights that many measures are not defined in a fully operational form, and relatively few of them are based on explicit empirical models, as recommended by measurement theory.},
  Doi                      = {10.1109/32.748920}
}

@Article{1999:tse:briand:b,
  Title                    = {Defining and validating measures for object-based high-level design},
  Author                   = {Lionel C. Briand and Sandro Morasca and Victor R. Basili},
  Journal                  = tse,
  Year                     = {1999},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {722--743},
  Volume                   = {25},

  Abstract                 = {The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. In this paper, we introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, which satisfy a previously published set of mathematical properties that are necessary for any such measures to be valid. We then investigate the measures' relationship to fault-proneness on three large scale projects, to provide empirical support for their practical significance and usefulness.},
  Doi                      = {10.1109/32.815329}
}

@Article{2000:jss:briand,
  Title                    = {Exploring the relationships between design measures and software quality in object-oriented systems},
  Author                   = {Lionel C. Briand and J{\"u}rgen W{\"u}st and John W. Daly and D. Victor Porter},
  Journal                  = jss,
  Year                     = {2000},

  Month                    = may,
  Number                   = {3},
  Pages                    = {245--273},
  Volume                   = {51},

  Abstract                 = {One goal of this paper is to empirically explore the relationships between existing object-oriented (OO) coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. In other words, we wish to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The second goal is to propose an investigation and analysis strategy to make these kind of studies more repeatable and comparable, a problem which is pervasive in the literature on quality measurement. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. However, it is shown that by using a subset of measures, accurate models can be built to predict which classes most of the faults are likely to lie in. When predicting fault-prone classes, the best model shows a percentage of correct classifications higher than 80\% and finds more than 90\% of faulty classes. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness.},
  Doi                      = {10.1016/S0164-1212(99)00102-8}
}

@Article{2000:jsmrp:bril,
  Title                    = {Maintaining a legacy: Towards support at the architectural level},
  Author                   = {Reinder J. Bril and Loe M. G. Feijs and Andr{\'e} Glas and Ren{\'e} L. Krikhaar and M. (Thijs) R. M. Winter},
  Journal                  = jsmrp,
  Year                     = {2000},

  Month                    = may # {/} # jun,
  Number                   = {3},
  Pages                    = {143--170},
  Volume                   = {12},

  Abstract                 = {An organization that develops large, software intensive systems with a long lifetime will encounter major changes in the market requirements, the software development environment, including its platform, and the target platform. In order to meet the challenges associated with these changes, software development has to undergo major changes as well. Especially when these systems are successful, and hence become an asset, particular care shall be taken to maintain this legacy; large systems with a long lifetime tend to become very complex and difficult to understand. Software architecture plays a vital role in the development of large software systems. For the purpose of maintenance, an up-to-date explicit description of the software architecture of a system supports understanding and comprehension of it, amongst other things. However, many large complex systems do not have an up-to-date documented softwarearchitecture. Particularly in cases where these systems have a long lifetime, the (natural) turnover of personnel will make it very likely that many employees contributing to previous generations of the system are no longer available. A need to `recover' the software architecture of the system may become prevalent, facilitating the understanding of the system, providing ways to improve its maintainability and quality and to control architectural changes. This paper gives an overview of an on-going effort to improve the maintainability and quality of a legacy system, and describes the recent introduction of support at the architectural level for program understanding and complexity control.},
  Doi                      = {10.1002/1096-908X(200005/06)12:3<143::AID-SMR207>3.3.CO;2-Z}
}

@InProceedings{2001:iwpc:bril,
  Title                    = {An architectural connectivity metric and its support for incremental re-architecting of large legacy systems},
  Author                   = {Bril, Reinder J. and Postma, Andr{\'e}},
  Booktitle                = iwpc,
  Year                     = {2001},
  Pages                    = {269--280},

  Abstract                 = {Architectural connectivity metrics are a means of supporting incremental re-architecting of large legacy systems. These metrics provide support by giving an indication of the degree of connectivity between or within architectural entities in the system. Ideally, a connectivity metric should provide useful information in as many situations as possible. However, the existing metrics cohesion and coupling provide support only in a limited number of situations. In this paper, we present a new architectural connectivity metric, referred to as directed connectivity, together with an appropriate visualization. Directed connectivity is a measure for the relative number of connections from one architectural entity to another. The metric is applicable in a large number of situations, including ones where cohesion and coupling fall short. The metric is visualized by means of a tabular representation with browsing facilities. A description is given of initial experiences with directed connectivity and its visualization on a large industrial system.}
}

@InProceedings{2003:icsm:bril,
  Title                    = {Embedding architectural support in industry},
  Author                   = {Bril, Reinder J. and Postma, Andr{\'e} and Krikhaar, Ren{\'e} L.},
  Booktitle                = icsm,
  Year                     = {2003},
  Pages                    = {348--357},

  Abstract                 = {Software architecture plays a vital role in thedevelopment (and hence maintenance) of large complex systems with a long lifetime. It is therefore required that the software architectureis also maintained, i.e. sufficiently documented, clearly communicated, and explicitly controlled. In our experience, these requirements cannot be met without appropriate support. Commercial-off-the-shelf support for architectural maintenance is still scarcely available, if at all, implying the need to develop appropriate proprietary means. In this paper, we briefly report upon an overall approach taken within three organizations within Philips that develop professional systems. We extensively describe the experience gained with the embedding of architectural support in these three organizations. We focus on architectural support in the area of software architecture recovery, visualization, analysis, and verification. In our experience, the support must be carried by a number of elements of software development, and all of these elements have to go through a change process to ensure sustainable embedding. We distinguish four of these elements, i.e. process, organization, software development environment, and humans, and present our experience in terms of those elements.},
  Doi                      = {10.1109/ICSM.2003.1235442}
}

@Article{1994:jss:brito_e_abreu,
  Title                    = {Candidate metrics for object-oriented software within a taxonomy framework},
  Author                   = {Brito e Abreu, Fernando and Rog{\'e}rio Carapu{\c{c}}a},
  Journal                  = jss,
  Year                     = {1994},

  Month                    = jul,
  Number                   = {1},
  Pages                    = {87--96},
  Volume                   = {26},

  Abstract                 = {This article offers an overview of the state of the art in object-oriented (OO) metrics as well as some new contributions. The usefulness of metrics is reviewed. The inappropriateness of ``traditional" metrics to encompass development under the OO paradigm is discussed. A framework for classifying metrics is suggested. Metrics are classified along two vectors: category and granularity, the usefulness and rationale behind each category are presented. Candidate metrics are suggested within the proposed framework. Finally, some research directions that require further effort are identified.},
  Doi                      = {10.1016/0164-1212(94)90099-X}
}

@InProceedings{1995:icsq:brito_e_abreu,
  Title                    = {Toward the Design Quality Evaluation of Object-Oriented Software Systems},
  Author                   = {Brito e Abreu, Fernando and Miguel Goul{\~a}o and Rita Esteves},
  Booktitle                = icsq,
  Year                     = {1995},
  Pages                    = {43--57},

  Abstract                 = {MOOD (metrics for object-oriented design) can enhance the quality of software development. Improvements in maintainability and in resource allocation through better object-oriented design and estimation were the goals of this research. The MOOD set included six dimensionless factors: attribute hiding, attribute inheritance, coupling, method hiding, method inheritance, and polymorphism. Using a filter metaphor, MOOD metric analysis led to band-pass and high-pass filters for design heuristics. In a field trial, the MOODKIT extraction tool collected MOOD metrics, at a 95\% savings of the time required for manual collection. The trial analyzed 163,556 lines of code in five C++ libraries: ET++, GNU glib++, Microsoft Foundation Classes, MotifApp, and NewMat. MOODKIT measures ranged from 100\% for attribute hiding in MotifApp to 2.7\% for polymorphism in Microsoft Foundation Classes. Future work should include a new version of MOODKIT as well as a common set of metrics applicable to diverse object-oriented analysis models.}
}

@Article{1987:computer:brooks,
  Title                    = {No silver bullet: Essence and accidents of software engineering},
  Author                   = {Brooks, Jr., Frederick P.},
  Journal                  = computer,
  Year                     = {1987},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {10--19},
  Volume                   = {20},

  Abstract                 = {Of all the monsters that fill the nightmares of our folklore, none terrify more than werewolves, because they transform unexpectedly from the familiar into horrors. For these, one seeks bullets of silver that can magically lay them to rest. The familiar software project, at least as seen by the nontechnical manager, has something of this character; it is usually innocent and straightforward, but is capable of becoming a monster of missed schedules, blown budgets, and flawed products. So we hear desperate cries for a silver bullet---something to make software costs drop as rapidly as computer hardware costs do. But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed. Skepticism is not pessimism, however. Although we see no startling breakthroughs---and indeed, I believe such to be inconsistent with the nature of software---many encouraging innovations are under way. A disciplined, consistent effort to develop, propagate, and exploit these innovations should indeed yield an order-of-magnitude improvement. There is no royal road, but there is a road. The first step toward the management of disease was replacement of demon theories and humours theories by the germ theory. That very step, the beginning of hope, in itself dashed all hopes of magical solutions. It told workers that progress would be made stepwise, at great effort, and that a persistent, unremitting care would have to be paid to a discipline of cleanliness. So it is with software engineering today. },
  Doi                      = {10.1109/MC.1987.1663532}
}

@Article{1983:ijmms:brooks,
  Title                    = {Towards a theory of the comprehension of computer programs},
  Author                   = {Ruven Brooks},
  Journal                  = ijmms,
  Year                     = {1983},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {543--554},
  Volume                   = {18},

  Abstract                 = {A sufficiency theory is presented of the process by which a computer programmer attempts to comprehend a program. The theory is intended to explain four sources of variation in behavior on this task: the kind of computation the program performs, the intrinsic properties of the program text, such as language and documentation, the reason for which the documentation is needed, and differences among the individuals performing the task. The starting point for the theory is an analysis of the structure of the knowledge required when a program is comprehended which views the knowledge as being organized into distinct domains which bridge between the original problem and the final program. The program comprehension process is one of reconstructing knowledge about these domains and the relationship among them. This reconstruction process is theorized to be a top-down, hypothesis driven one in which an initially vague and general hypothesis is refined and elaborated based on inf ormation extracted from the program text and other documentation.},
  Doi                      = {10.1016/S0020-7373(83)80031-5}
}

@Article{2007:misq:browne,
  Title                    = {Cognitive stopping rules for terminating information search in online tasks},
  Author                   = {Browne, Glenn J. and Pitts, Mitzi G. and Wetherbe, James C.},
  Journal                  = misq,
  Year                     = {2007},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {89--104},
  Volume                   = {31},

  Abstract                 = {Online search has become a significant activity in the daily lives of individuals throughout much of the world. The almost instantaneous availability of billions of web pages has caused a revolution in the way people seek information. Despite the increasing importance of online search behavior in decision making and problem solving, very little is known about why people stop searching for information online. In this paper, we review the literature concerning online search and cognitive stopping rules, and then describe specific types of information search tasks. Based on this theoretical development, we generated hypotheses and conducted an experiment with 115 participants each performing three search tasks on the web. Our findings show that people utilize a number of stopping rules to terminate search, and that the stopping rule used depends on the type of task performed. Implications for online information search theory and practice are discussed.},
  Url                      = {http://misq.org/cognitive-stopping-rules-for-terminating-information-search-in-online-tasks.html?SID=fm4asfg9vsv084mccfr3b3cau5}
}

@InProceedings{2009:esec_fse:bruch,
  Title                    = {Learning from examples to improve code completion systems},
  Author                   = {Marcel Bruch and Martin Monperrus and Mira Mezini},
  Booktitle                = esec_fse,
  Year                     = {2009},
  Pages                    = {213--222},

  Abstract                 = {The suggestions made by current IDE's code completion features are based exclusively on static type system of the programming language. As a result, often proposals are made which are irrelevant for a particular working context. Also, these suggestions are ordered alphabetically rather than by their relevance in a particular context. In this paper, we present intelligent code completion systems that learn from existing code repositories. We have implemented three such systems, each using the information contained in repositories in a different way. We perform a large-scale quantitative evaluation of these systems, integrate the best performing one into Eclipse, and evaluate the latter also by a user study. Our experiments give evidence that intelligent code completion systems which learn from examples significantly outperform mainstream code completion systems in terms of the relevance of their suggestions and thus have the potential to enhance developers' productivity.},
  Doi                      = {10.1145/1595696.1595728}
}

@InProceedings{2008:rsse:bruch,
  Title                    = {On evaluating recommender systems for {API} usages},
  Author                   = {Bruch, Marcel and Sch{\"a}fer, Thorsten and Mezini, Mira},
  Booktitle                = rsse,
  Year                     = {2008},
  Pages                    = {16--20},

  Abstract                 = {To ease framework understanding, tools have been developed that analyze existing framework instantiations to extract API usage patterns and present them to the user. However, detailed quantitative evaluations of such recommender systems are lacking. In this paper we present an automated evaluation process which extracts queries and expected results from existing code bases. This enables the validation of recommendation systems with large test beds in an objective manner by means of precision and recall measures. We demonstrate the applicability of our approach by evaluating an improvement of an existing API recommender tool that takes into account the framework-method context for recommendations.}
}

@InProceedings{2011:esec_fse:brun,
  Title                    = {Proactive detection of collaboration conflicts},
  Author                   = {Brun, Yuriy and Holmes, Reid and Ernst, Michael D. and Notkin, David},
  Booktitle                = esec_fse,
  Year                     = {2011},
  Pages                    = {168--178},

  Abstract                 = {Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results. First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems. Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations. Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts.},
  Doi                      = {10.1145/2025113.2025139}
}

@InProceedings{2009:popl:brunel,
  Title                    = {A foundation for flow-based program matching: Using temporal logic and model checking},
  Author                   = {Brunel, Julien and Doligez, Damien and Hansen, Ren{\'e} Rydhof and Lawall, Julia L. and Muller, Gilles},
  Booktitle                = popl,
  Year                     = {2009},
  Pages                    = {114--126},

  Abstract                 = {Reasoning about program control-flow paths is an important functionality of a number of recent program matching languages and associated searching and transformation tools. Temporal logic provides a well-defined means of expressing properties of control-flow paths in programs, and indeed an extension of the temporal logic CTL has been applied to the problem of specifying and verifying the transformations commonly performed by optimizing compilers. Nevertheless, in developing the Coccinelle program transformation tool for performing Linux collateral evolutions in systems code, we have found that existing variants of CTL do not adequately support rules that transform subterms other than the ones matching an entire formula. Being able to transform any of the subterms of a matched term seems essential in the domain targeted by Coccinelle. In this paper, we propose an extension to CTL named CTLVW (CTL with variables and witnesses) that is a suitable basis for the semantics and implementation of the Coccinelles program matching language. Our extension to CTL includes existential quantification over program fragments, which allows metavariables in the program matching language to range over different values within different control-flow paths, and a notion of witnesses that record such existential bindings for use in the subsequent program transformation process. We formalize CTL-VW and describe its use in the context of Coccinelle. We then assess the performance of the approach in practice, using a transformation rule that fixes several reference count bugs in Linux code.},
  Doi                      = {10.1145/1594834.1480897}
}

@InProceedings{2000:sigir:buckley,
  Title                    = {Evaluating evaluation measure stability},
  Author                   = {Buckley, Chris and Voorhees, Ellen M.},
  Booktitle                = sigir,
  Year                     = {2000},
  Pages                    = {33--40},

  Abstract                 = {This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.}
}

@InProceedings{2002:wcre:bull,
  Title                    = {Semantic Grep: Regular Expressions + Relational Abstraction},
  Author                   = {R. Ian Bull and Andrew Trevors and Andrew J. Malton and Michael W. Godfrey},
  Booktitle                = wcre,
  Year                     = {2002},
  Pages                    = {267--276},

  Abstract                 = {Searching source code is one of the most common activities of software engineers. Text editors and other support tools normally provide searching based on lexical expressions (regular expressions). Some more advanced editors provide a way to add semantic direction to some of the searches. Recent research has focused on advancing the semantic options available to text-based queries. Most of these results make use of heavy weight relational database management technology. In this paper we explore the extension of lexical pattern matching by means of light weight relational queries, implemented using a tool called grok. A ``semantic grep" (sgrep) command was implemented, which translates queries in a mixed algebraic and lexical language into a combination of grok queries and grep commands. This paper presents the design decisions behind sgrep, and example queries that can be posed. The paper concludes with a case study in which sgrep was used to identify architectural anomalies in PostgreSQL, an open source Database Management System.}
}

@InProceedings{2009:iwsc:bulychev,
  Title                    = {An evaluation of duplicate code detection using anti-unification},
  Author                   = {Peter Bulychev and Marius Minea},
  Booktitle                = iwsc,
  Year                     = {2009},

  Abstract                 = {This paper describes an algorithm for finding software clones, which works at the level of abstract syntax trees and is thus conceptually independent of the source language of the analyzed programs. We use a notion of clones which captures replacement of subtrees in the program AST, and is formally based on the notion of anti-unification. This allows us to capture syntactic structural similarity with increased accuracy and express it in metrics. We have implemented this algorithm in a tool named Clone Digger, freely available under the GPL license, and which currently supports the Python, Java and Lua languages. We report on initial experimental results and comparisons with similar tools.},
  Url                      = {http://www.informatik.uni-bremen.de/st/IWSC/bulychev.pdf}
}

@InProceedings{2008:syrcose:bulychev,
  Title                    = {Duplicate code detection using anti-unification},
  Author                   = {Peter Bulychev and Marius Minea},
  Booktitle                = syrcose,
  Year                     = {2008},
  Pages                    = {51--54},

  Abstract                 = {This paper describes a new algorithm for finding software clones. It is conceptually independent of the source language of the analyzed programs, working at the level of abstract syntax trees. The algorithm considers that two sequences of statements form a clone if one of them can be obtained from the other by replacing some subtrees. To our knowledge this notion was not previously employed in the literature. It allows to take into account all information on the syntactic structure of a program. We have implemented this algorithm in the tool Clone Digger. It currently supports the Python and Java languages. Clone Digger is free and provided under the GPL license.},
  Url                      = {http://www-cad.eecs.berkeley.edu/~marius/papers/syrcose08.pdf}
}

@Article{1997:patreclet:bunke,
  Title                    = {On a relation between graph edit distance and maximum common subgraph},
  Author                   = {H. Bunke},
  Journal                  = patreclet,
  Year                     = {1997},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {689--694},
  Volume                   = {18},

  Abstract                 = {In approximate, or error-correcting, graph matching one considers a set of graph edit operations, and defines the edit distance of two graphs $g_1$ and $g_2$ as the shortest (or least cost) sequence of edit operations that transform $g_1$ into $g_2$. A maximum common subgraph of two graphs $g_1$ and $g_2$ is a subgraph of both $g_1$ and $g_2$ such that there is no other subgraph of $g_1$ and $g_2$ with more nodes. Graph edit distance and maximum common subgraph are well known concepts that have various applications in pattern recognition and machine vision. In this paper a particular cost function for graph edit distance is introduced, and it is shown that under this cost function graph edit distance computation is equivalent to the maximum common subgraph problem.},
  Doi                      = {10.1016/S0167-8655(97)00060-3}
}

@InProceedings{2005:softviz:burch,
  Title                    = {Visual data mining in software archives},
  Author                   = {Burch, Michael and Diehl, Stephan and Wei{\ss}gerber, Peter},
  Booktitle                = softviz,
  Year                     = {2005},
  Pages                    = {37--46},

  Abstract                 = {Software archives contain historical information about the development process of a software system. Using data mining techniques rules can be extracted from these archives. In this paper we discuss how standard visualization techniques can be applied to interactively explore these rules. To this end we extended the standard visualization techniques for association rules and sequence rules to also show the hierarchical order of items. Clusters and outliers in the resulting visualizations provide interesting insights into the relation between the temporal development of a system and its static structure. As an example we look at the large software archive of the MOZILLA open source project. Finally we discuss what kind of regularities and anomalies we found and how these can then be leveraged to support software engineers.}
}

@InProceedings{1997:icsm:burd,
  Title                    = {Investigating the maintenance implications of the replication of code},
  Author                   = {Burd, Elizabeth and Munro, Malcolm},
  Booktitle                = icsm,
  Year                     = {1997},
  Pages                    = {322--329},

  Abstract                 = {This paper describes an investigation into the use of code replication within legacy software systems. Two cases of replication are investigated. These are replication with an individual program and replication of an entire or part of a program across a program suite. For each of the cases an example is given from code used within the commercial sector. The instances of replication are then investigated and the implication of their occurrences within the code on the maintenance process are considered. The reasons why code replication is not a form of software reuse are discussed. Finally this paper investigates whether, with reengineering, areas of high usage of code replication are potential candidates for reuse.},
  Doi                      = {10.1109/ICSM.1997.624265}
}

@Article{2005:aij:burghardt,
  Title                    = {{E}-generalization using grammars},
  Author                   = {J. Burghardt},
  Journal                  = aij,
  Year                     = {2005},

  Month                    = jun,
  Number                   = {1},
  Pages                    = {1--35},
  Volume                   = {165},

  Abstract                 = {We extend the notion of anti-unification to cover equational theories and present a method based on regular tree grammars to compute a finite representation of E-generalization sets. We present a framework to combine Inductive Logic Programming and E-generalization that includes an extension of Plotkin's lgg theorem to the equational case. We demonstrate the potential power of E-generalization by three example applications: computation of suggestions for auxiliary lemmas in equational inductive proofs, computation of construction laws for given term sequences, and learning of screen editor command sequences.},
  Doi                      = {10.1016/j.artint.2005.01.008}
}

@InProceedings{1995:interact:burkhardt,
  Title                    = {An empirical study of software reuse by experts in object-oriented design},
  Author                   = {Burkhardt, Jean-Marie and D{\'e}tienne, Fran{\c{c}}oise},
  Booktitle                = interact,
  Year                     = {1995},
  Pages                    = {133--138},

  Abstract                 = {This paper presents an empirical study of the software reuse activity by expert designers in the context of object-oriented design. Our study focuses on the three following aspects of reuse : (1) the interaction between some design processes, e.g. constructing a problem representation, searching for and evaluating solutions, and reuse processes, i.e. retrieving and using previous solutions, (2) the mental processes involved in reuse, e.g. example-based retrieval or bottom-up versus top-down expanding of the solution, and (3) the mental representations constructed throughout the reuse activity, e.g. dynamic versus static representations. Some implications of these results for the specification of software reuse support environments are discussed.},
  Url                      = {http://arxiv.org/pdf/cs/0702005}
}

@Book{1981:book:burris,
  Title                    = {A Course in Universal Algebra},
  Author                   = {Burris, Stanley and Sankappanavar, H. P.},
  Publisher                = {Springer},
  Year                     = {1981},
  Number                   = {78},
  Series                   = {Graduate Texts in Mathematics}
}

@InProceedings{1990:compsac:burson,
  Title                    = {A program transformation approach to automating software re-engineering},
  Author                   = {Burson, S. and Kotik, G. B. and Markosian, L. Z.},
  Booktitle                = compsac,
  Year                     = {1990},
  Pages                    = {314--322},

  Abstract                 = {The authors describe a novel approach to software re-engineering that combines several technologies: object-oriented databases integrated with parser, for capturing the software to be re-engineered; specification and pattern languages for querying and analyzing a database of software; and transformation rules for automatically generating re-engineered code. The authors then describe REFINE, an environment for program representation, analysis, and transformation that provides the tools needed to implement the automation of software maintenance and re-engineering. The transformational approach is illustrated with examples taken from actual experience in re-engineering software in C, JCL and NATURAL. It is concluded that the ability to support automation in modifying large software systems by using rule-based program transformation is a key innovation of the present approach that distinguishes it from tools that focus only on automation of program analysis.}
}

@Article{2011:software:buschmann:a,
  Title                    = {Gardening Your Architecture, Part 1: Refactoring},
  Author                   = {Buschmann, Frank},
  Journal                  = software,
  Year                     = {2011},

  Month                    = jul # {/} # aug,
  Number                   = {4},
  Pages                    = {92--94},
  Volume                   = {28},

  Abstract                 = {Refactoring has a more precise definition than common practice might suggest: it's a change that improves the developmental quality of some part of a system while preserving its functional behavior. Refactoring isn't limited to code detail but can range up to the larger scale of a system's software architecture. Yet refactoring is limited in what qualities it can help improve. It can also do more harm than good when practiced informally or ad hoc or when it's used as a synonym for any form of change in a system.},
  Doi                      = {10.1109/MS.2011.76}
}

@Article{2011:software:buschmann:b,
  Title                    = {Gardening Your Architecture, Part 2: Reengineering and Rewriting},
  Author                   = {Buschmann, Frank},
  Journal                  = software,
  Year                     = {2011},

  Month                    = sep # {/} # oct,
  Number                   = {5},
  Pages                    = {21--23},
  Volume                   = {28},

  Abstract                 = {Reengineering and rewriting are two common approaches for improving system quality---in addition to refactoring, which the last installment of this column explored. Reengineering is a systematic approach to evolve existing software to exhibit new behavior, features, and operational quality. Refactoring and reengineering aren't the same, and they're also different from rewriting---the most radical change---which involves wiping the slate clean and starting over.},
  Doi                      = {10.1109/MS.2011.97}
}

@InProceedings{2002:gpce:butler,
  Title                    = {Architectural Refactoring in Framework Evolution: A Case Study},
  Author                   = {Butler, Gregory},
  Booktitle                = gpce,
  Year                     = {2002},
  Pages                    = {128--139},

  Abstract                 = {The Know-It-All Project is investigating methodologies for the development, application, and evolution of frameworks. A concrete framework for database management systems is being developed as a case study for the methodology research. The methodology revolves around a set of models for the domain, the functionality, the architecture, the design, and the code. These models reflect the common and variable features of the domain.Refactoring of source code has been studied as a preliminary step in the evolution of object-oriented software. In cascaded refactoring, we view framework evolution as a two-step process: refactoring and extension. The refactoring step is a set of refactorings, one for each model. The refactorings chosen for a model determine the rationale or constraints for the choice of refactorings of the next model.There are several issues with respect to architecture that we have encountered and are exploring. These include (1) the choice of models for the architecture; (2) the design of the architecture and its evaluation; (3) the evolution of the architecture by extending the concept of refactoring from source code to architecture; and (4) the modeling of variation in architectures across the product line. Here we focus on the refactoring of the architecture.}
}

@InCollection{1999:book:fayad:butler,
  Title                    = {Documenting frameworks},
  Author                   = {Greg Butler and Pierre D{\'e}nomm{\'e}e},
  Booktitle                = {Building Application Frameworks: Object-Oriented Foundations of Framework Design},
  Publisher                = {John Wiley and Sons},
  Year                     = {1999},
  Chapter                  = {21},
  Editor                   = {Mohamed E. Fayad and Douglas C. Schmidt and Ralph E. Johnson},
  Pages                    = {495--504},

  Abstract                 = {TBD}
}

@Article{2000:csur:butler,
  Title                    = {A Framework for Framework Documentation},
  Author                   = {Greg Butler and Rudolf K. Keller and Hafedh Mili},
  Journal                  = csur,
  Year                     = {2000},

  Month                    = mar,
  Number                   = {1es},
  Pages                    = {15:1--15:7},
  Volume                   = {32},

  Abstract                 = {Frameworks are quite difficult to understand when one first uses them: the design is very abstract, to factor out commonality; the design is incomplete, requiring additional subclasses to create an application; the design provides flexibility for several hotspots, not all of which are needed in the application at hand; and the collaborations and the resulting dependencies between classes can be indirect and obscure. Many approaches to documenting frameworks have been tried, though with different aims and audiences in mind. In this paper, we present a task-oriented framework for framework documentation. Our framework is based on first identifying a set of framework (re- )use cases, which we then decompose into a set of elementary engineering tasks. Each such task requires a set of documentation primitives, enabling us to specify a minimal set of documentation primitives for each framework usage scenario. We study some major framework documentation approaches in light of this framework, identifying possible deficiences and highlighting directions for further research.},
  Doi                      = {10.1145/351936.351951}
}

@InProceedings{2006:spin:de_la_camara,
  Title                    = {Abstract matching for software model checking},
  Author                   = {de la C\'{a}mara, Pedro and del Mar Gallardo, Mar\'{\i}a and Merino, Pedro},
  Booktitle                = spin,
  Year                     = {2006},
  Pages                    = {182--200},

  Abstract                 = {Current research in software model checking explores new techniques to handle the storage of visited states (usually called the heap). One approach consists in saving only parts or representations of the states in the heap. This paper presents a new technique to implement sound abstract matching of states. This kind of matching produces a reduction in the number of states and traces explored. With the aim of obtaining a useful result, it is necessary to establish some correctness conditions on the matching scheme. In this paper, we use static analysis to automatically construct an abstract matching function which depends on the program and the property to be verified. The soundness of the static analysis guarantees the soundness of verification. This paper describes the overall technique applied to Spin, the correctness issues and some examples which show its efficiency.},
  Doi                      = {10.1007/11691617_11}
}

@Article{2004:tpami:caelli,
  Title                    = {An Eigenspace Projection Clustering Method for Inexact Graph Matching},
  Author                   = {Caelli, Terry and Kosinov, Serhiy},
  Journal                  = tpami,
  Year                     = {2004},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {515--519},
  Volume                   = {26},

  Abstract                 = {In this paper, we show how inexact graph matching (that is, the correspondence between sets of vertices of pairs of graphs) can be solved using the renormalization of projections of the vertices (as defined in this case by their connectivities) into the joint eigenspace of a pair of graphs and a form of relational clustering. An important feature of this eigenspace renormalization projection clustering (EPC) method is its ability to match graphs with different number of vertices. Shock graph-based shape matching is used to illustrate the model and a more objective method for evaluating the approach using random graphs is explored with encouraging results.},
  Doi                      = {10.1109/TPAMI.2004.1265866}
}

@InProceedings{2011:fase:cai,
  Title                    = {An Empirical Study of Long-Lived Code Clones},
  Author                   = {Dongxiang Cai and Miryung Kim},
  Booktitle                = fase,
  Year                     = {2011},
  Pages                    = {432--446},
  Series                   = lncs,
  Volume                   = {6603},

  Abstract                 = {Previous research has shown that refactoring code clones as soon as they are formed or discovered is not always feasible or worthwhile to perform, since some clones never change during evolution and some disappear in a short amount of time, while some undergo repetitive similar edits over their long lifetime. Toward a long-term goal of developing a recommendation system that selectively identifies clones to refactor, as a first step, we conducted an empirical investigation into the characteristics of long-lived clones. Our study of 13558 clone genealogies from 7 large open source projects, over the history of 33.25 years in total, found surprising results. The size of a clone, the number of clones in the same group, and the method-level distribution of clones are not strongly correlated with the survival time of clones. However, the number of developers who modified clones and the time since the last addition or removal of a clone to its group are highly correlated with the survival time of clones. This result indicates that the evolutionary characteristics of clones may be a better indicator for refactoring needs than static or spatial characteristics such as LOC, the number of clones in the same group, or the dispersion of clones in a system.},
  Doi                      = {10.1007/978-3-642-19811-3_30}
}

@InProceedings{2006:ase:cai,
  Title                    = {Modularity Analysis of Logical Design Models},
  Author                   = {Yuanfang Cai and Sullivan, Kevin J.},
  Booktitle                = ase,
  Year                     = {2006},
  Pages                    = {91--102},

  Abstract                 = {Traditional design representations are inadequate for generalized reasoning about modularity in design and its technical and economic implications. We have developed an architectural modeling and analysis approach, and automated tool support, for improved reasoning in these terms. However, the complexity of constraint satisfaction limited the size of models that we could analyze. The contribution of this paper is a more scalable approach. We exploit the dominance relations in our models to guide a divide-andconquer algorithm, which we have implemented it in our Simon tool. We evaluate its performance in case studies. The approach reduced the time needed to analyze small but representative models from hours to seconds. This work appears to make our modeling and analysis approach practical for research on the evolvability and economic properties of software design architectures.}
}

@Article{1991:computer:caldiera,
  Title                    = {Identifying and qualifying reusable software components},
  Author                   = {Gianluigi Caldiera and Victor R. Basili},
  Journal                  = computer,
  Year                     = {1991},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {61--70},
  Volume                   = {24},

  Abstract                 = {Identification and qualification of reusable software based on software models and metrics is explored. Software metrics provide a way to automate the extraction of reusable software components from existing systems, reducing the amount of code that experts must analyze. Also, models and metrics permit feedback and improvement to make the extraction process fit a variety of environments. Some case studies are described to validate the experimental approach. They deal with only the identification phase and use a very simple model of a reusable code component, but the results show that automated techniques can reduce the amount of code that a domain expert needs to evaluate to identify reusable parts.},
  Doi                      = {10.1109/2.67210}
}

@Article{2011:ese:callo_arias,
  Title                    = {A practice-driven systematic review of dependency analysis solutions},
  Author                   = {Callo Arias, Trosky B. and van der Spek, Pieter and Avgeriou, Paris},
  Journal                  = ese,
  Year                     = {2011},

  Month                    = oct,
  Number                   = {5},
  Pages                    = {544--586},
  Volume                   = {16},

  Abstract                 = {When following architecture-driven strategies to develop large software-intensive systems, the analysis of the dependencies is not an easy task. In this paper, we report a systematic literature review on dependency analysis solutions. Dependency analysis concerns making dependencies due to interconnections between programs or system components explicit. The review is practice-driven because its research questions, execution, and reporting were influenced by the practice of a group of software architects at Philips Healthcare MRI. The review results in an overview and assessment of the state-of-the-art and applicability of dependency analysis. The overview provides insights about definitions related to dependency analysis, the sort of development activities that need dependency analysis, and the classification and description of a number of dependency analysis solutions. The contribution of this paper is for both practitioners and researchers. They can take it as a reference to learn about dependency analysis, match their own practice to the presented results, and to build similar overviews of other techniques and methods for other domains or types of systems.},
  Doi                      = {10.1007/s10664-011-9158-8}
}

@InProceedings{2005:metrics:canfora,
  Title                    = {Impact Analysis by Mining Software and Change Request Repositories},
  Author                   = {Canfora, Gerardo and Cerulo, Luigi},
  Booktitle                = metrics,
  Year                     = {2005},
  Pages                    = {29:1--29:9},

  Abstract                 = {Impact analysis is the identification of the work products affected by a proposed change request, either a bug fix or a new feature request. In many open-source projects, such as KDE, Gnome, Mozilla, Openoffice, change requests, and related data, are stored in a bug tracking system such as Bugzilla [1]. These data, together with the data stored in a versioning system, such as CVS [2], are a valuable source of information on which useful analyses can be performed. In this paper we propose a method to derive the set of source files impacted by a proposed change request. The method exploits information retrieval algorithms to link the change request description and the set of historical source file revisions impacted by similar past change requests. The method is evaluated by applying it on four open-source projects.}
}

@Article{2009:software:canfora,
  Title                    = {Tracking Your Changes: A Language-Independent Approach},
  Author                   = {Canfora, Gerardo and Cerulo, Luigi and Di Penta, Massimiliano},
  Journal                  = software,
  Year                     = {2009},

  Month                    = jan # {/} # feb,
  Number                   = {1},
  Pages                    = {50--57},
  Volume                   = {26},

  Abstract                 = {Versioning and bug-tracking systems are invaluable assets for large software projects that involve developers spread worldwide and numerous users reporting bugs and proposing enhancements. In addition to supporting development, versioning systems are a precious source of information for studying or monitoring a software system's evolution.},
  Doi                      = {10.1109/MS.2009.26}
}

@InProceedings{2007:msr:canfora,
  Title                    = {Identifying Changed Source Code Lines from Version Repositories},
  Author                   = {Canfora, Gerardo and Cerulo, Luigi and Di Penta, Massimiliano},
  Booktitle                = msrw,
  Year                     = {2007},
  Pages                    = {14:1--14:8},

  Abstract                 = {Observing the evolution of software systems at different levels of granularity has been a key issue for a number of studies, aiming at predicting defects or at studying certain phenomena, such as the presence of clones or of crosscutting concerns. Versioning systems such as CVS and SVN, however, only provide information about lines added or deleted by a contributor: any change is shown as a sequence of additions and deletions. This provides an erroneous estimate of the amount of code changed. This paper shows how the evolution of changes at source code line level can be inferred from CVS repositories, by combining information retrieval techniques and the Levenshtein edit distance. The application of the proposed approach to the ArgoUML case study indicates a high precision and recall.},
  Doi                      = {10.1109/MSR.2007.14}
}

@InProceedings{1994:icsm:canfora,
  Title                    = {Software Salvaging Based on Conditions},
  Author                   = {Canfora, Gerardo and Cimitile, Aniello and De Lucia, Andrea and Di Lucca, Giuseppe A.},
  Booktitle                = icsm,
  Year                     = {1994},
  Pages                    = {424--433},

  Abstract                 = {This paper presents algorithms for isolating reusable functions in large monolithic programs. The functions to be isolated are specified in terms of either pre-conditions or binding conditions, and these are mapped onto predicates on program's variables. Code components whose execution is triggered and/or bound by these predicates are then isolated. Each component is a candidate to implement a reusable function. The algorithms exploit a representation of the subject program in the form of a program dependence graph. This work forms part of RE 2, a research project that addresses the wider issue of software reuse. RE2 project aims to promote the reuse of software through the exploration of reverse engineering and re-engineering techniques to identify and extract reusable software components from existing systems.},
  Doi                      = {10.1109/ICSM.1994.336752}
}

@Article{1994:software:card,
  Title                    = {Why do so many reuse programs fail?},
  Author                   = {Card, Dave and Comer, Ed},
  Journal                  = software,
  Year                     = {1994},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {114--115},
  Volume                   = {11},

  Abstract                 = {Research, forecasts and government studies consistently show that reuse technology has the greatest potential to reduce the cost of software. Some reuse programs have succeeded, achieving anywhere from 30 to 80 percent reuse. Yet other programs have failed to show any clear return. How can such an obvious winner fail? Our experience as promoters and supporters of reuse and as measurers of its effectiveness suggests that two fundamental mistakes contribute to failure. The first mistake is that organizations treat reuse as a technology-acquisition problem instead of a technology-transition problem. Plenty of reuse technology is now mature enough for industrial use (although some problems remain). However, just buying technology usually does not lead to extensive reuse. The second mistake is that organizations fail to approach reuse as a business strategy. Even organizations that recognize reuse as a technology-transition issue may fail to address the business implications of reuse. We consider how the most important obstacles to reuse are economic and cultural not technological.},
  Doi                      = {10.1109/52.311078}
}

@InProceedings{1985:icse:card,
  Title                    = {Criteria for software modularization},
  Author                   = {David N. Card and Gerald T. Page and Frank E. McGarry},
  Booktitle                = icse,
  Year                     = {1985},
  Pages                    = {372--377},

  Abstract                 = {A central issue in programming practice involves determining the appropriate size and information content of a software module. This study attempted to determine the effectiveness of two widely used criteria for software modularization, strength and size, in reducing fault rate and development cost. Data from 453 FORTRAN modules developed by professional programmers were analyzed. The results indicated that module strength is a good criterion with respect to fault rate, whereas arbitrary module size limitations inhibit programmer productivity. This analysis is a first step toward defining empirically based standards for software modularization.}
}

@InProceedings{1999:wcre:carriere,
  Title                    = {Software architectural transformation},
  Author                   = {Carriere, S. J. and Woods, S. and Kazman, R.},
  Booktitle                = wcre,
  Year                     = {1999},
  Pages                    = {13--23},

  Abstract                 = {Software architecture, as a vehicle for communication and reasoning about software systems and their quality, is becoming an area of focus in both the forward- and reverse-engineering communities. In the past, we have attempted to unify these areas via a semantic model of reengineering called CORUM II. In this paper we present a concrete example of an architecturally-motivated reengineering task. In executing this task, we perform architecture reconstruction, reason about the reconstructed architecture, motivate an architectural transformation with new architectural quality requirements, and realize this architectural transformation via an automated code transformation.}
}

@InProceedings{2003:metrics:cartwright,
  Title                    = {Dealing with missing software project data},
  Author                   = {Cartwright, M. H. and Shepperd, M. J. and Song, Q.},
  Booktitle                = metrics,
  Year                     = {2003},
  Pages                    = {154--165},

  Abstract                 = {Whilst there is a general consensus that quantitative approaches are an important part of successful software project management, there has been relatively little research into many of the obstacles to data collection and analysis in the real world. One feature that characterises many of the data sets we deal with is missing or highly questionable values. Naturally this problem is not unique to software engineering, so we explore the application of two existing data imputation techniques that have been used to good effect elsewhere. In order to assess the potential value of imputation we use two industrial data sets. Both are quite problematic from an effort modelling perspective because they contain few cases, have a significant number of missing values and the projects are quite heterogeneous. We examine the quality of fit of effort models derived by stepwise regression on the raw data and data sets with values imputed by various techniques is compared. In both data sets we find that k-nearest neighbour (k-NN) and sample mean imputation (SMI) significantly improve the model fit, with k-NN giving the best results. These results are consistent with other recently published results, consequently we conclude that imputation can assist empirical software engineering.},
  Doi                      = {10.1109/METRIC.2003.1232464}
}

@InProceedings{1993:iwsr:castano,
  Title                    = {A constructive approach to reuse of conceptual components},
  Author                   = {Castano, S. and De Antonellis, V.},
  Booktitle                = iwsr,
  Year                     = {1993},
  Pages                    = {19--28},

  Abstract                 = {A methodological approach to the design-for-reuse process is presented. Reusability at the conceptual design level is considered, and tools for preparing reusable components to be exploited for designing applications not from scratch, but tailoring and adapting existing components are presented. Reusable components are defined as generic components with associated metacomponents providing guidelines for reuse in a given application},
  Doi                      = {10.1109/ASR.1993.291721}
}

@InProceedings{2009:sac:castro-herrera,
  Title                    = {A recommender system for requirements elicitation in large-scale software projects},
  Author                   = {Castro-Herrera, Carlos and Duan, Chuan and Cleland-Huang, Jane and Mobasher, Bamshad},
  Booktitle                = sac,
  Year                     = {2009},
  Pages                    = {1419--1426},

  Abstract                 = {In large and complex software projects, the knowledge needed to elicit requirements and specify the functional and behavioral properties can be dispersed across many thousands of stakeholders. Unfortunately traditional requirements engineering techniques, which were primarily designed to support face-to-face meetings, do not scale well to handle the needs of larger projects. We therefore propose a semi-automated requirements elicitation framework which uses data-mining techniques and recommender system technologies to facilitate stakeholder collaboration in a large-scale, distributed project. Our proposed recommender model is a hybrid one designed to manage the placement of stakeholders into highly focused discussion forums, where they can work collaboratively to generate requirements. In our approach, statements of need are first gathered from the project stakeholders; unsupervised clustering techniques are then used to identify cohesive and finely-grained themes and a users' profile is constructed according to the interests of the stakeholders in each of these themes. This profile feeds information to a collaborative recommender, which predicts stakeholders' interests in additional forums. The validity and effectiveness of the proposed recommendation framework is evaluated through a series of experiments using feature requests from three software systems.},
  Doi                      = {10.1145/1529282.1529601}
}

@InProceedings{2008:re:castro-herrera,
  Title                    = {Using Data Mining and Recommender Systems to Facilitate Large-Scale, Open, and Inclusive Requirements Elicitation Processes},
  Author                   = {Castro-Herrera, Carlos and Duan, Chuan and Cleland-Huang, Jane and Mobasher, Bamshad},
  Booktitle                = re,
  Year                     = {2008},
  Pages                    = {165--168},

  Abstract                 = {Requirements related problems, especially those originating from inadequacies in the human-intensive task of eliciting stakeholders' needs and desires, have contributed to many failed and challenged software projects. This is especially true for large and complex projects in which requirements knowledge is distributed across thousands of stakeholders. This short paper introduces a new process and related framework that utilizes data mining and recommender technologies to create an open, scalable, and inclusive requirements elicitation process capable of supporting projects with thousands of stakeholders. The approach is illustrated and evaluated using feature requests mined from an open source software product.},
  Doi                      = {10.1109/RE.2008.47}
}

@InProceedings{2004:ware:ceccato,
  Title                    = {Measuring the Effects of Software Aspectization},
  Author                   = {Ceccato, Mariano and Tonella, Paolo},
  Booktitle                = ware,
  Year                     = {2004},
  Note                     = {5~pages},

  Abstract                 = {The aim of Aspect Oriented Programming (AOP) is the production of code that is easier to understand and evolve, thanks to the separation of the crosscutting concerns from the principal decomposition. However, AOP languages introduce an implicit coupling between the aspects and the modules in the principal decomposition, in that the latter may be unaware of the presence of aspects that intercept their execution and/or modify their structure. These invisible connections represent the main drawback of AOP. A measuring method is proposed to investigate the trade-off between advantages and disadvantages obtained by using the AOP approach. The method that we are currently studying is based on a metrics suite that extends the metrics traditionally used with the OO paradigm.}
}

@Book{2012:book:cesare,
  Title                    = {Software Similarity and Classification},
  Author                   = {Silvio Cesare and Yang Xiang},
  Publisher                = {Springer},
  Year                     = {2012}
}

@Article{2000:spe:chae,
  Title                    = {A cohesion measure for object-oriented classes},
  Author                   = {Heung Seok Chae and Yong Rae Kwon and Doo Hwan Bae},
  Journal                  = spe,
  Year                     = {2000},

  Month                    = oct,
  Number                   = {12},
  Pages                    = {1405--1431},
  Volume                   = {30},

  Abstract                 = {In object-oriented systems, cohesion refers to the degree of the relatedness of the members in a class and strong cohesion has been recognized as a highly desirable property of classes. We note that the existing cohesion measures do not take into account some characteristics of classes, and thus often fail to properly reflect the cohesiveness of classes. To cope with such a problem, we propose a new cohesion measure where the characteristics of classes are incorporated. Our cohesion measure takes into account the members that actually have impact on the cohesiveness of a class, and is defined in terms of the degree of the connectivity among those members. We develop a cohesion measurement tool for C++ programs, and perform a case study on a well-known class library in order to demonstrate the effectiveness of our new measure. By performing principal component analysis, we also demonstrate that our measure captures a new aspect of class properties which is not captured by the existing cohesion measures.},
  Doi                      = {10.1002/1097-024X(200010)30:12<1405::AID-SPE330>3.0.CO;2-3}
}

@Article{2004:tse:chae,
  Title                    = {Improving cohesion metrics for classes by considering dependent instance variables},
  Author                   = {Heung Seok Chae and Yong Rae Kwon and Doo Hwan Bae;},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {826 -- 832},
  Volume                   = {30},

  Abstract                 = {The existing cohesion metrics for classes do not consider the characteristics of dependent instance variables that are commonly used in a class and, thus, do not properly reflect the cohesiveness of the class. This paper presents an approach for improving the cohesion metrics by considering the characteristics of the dependent instance variables in an object-oriented program.},
  Doi                      = {10.1109/TSE.2004.88}
}

@PhdThesis{2000:tr:chai,
  Title                    = {Pedagogical Framework Documentation: How to Document Object-Oriented Frameworks an Empirical Study},
  Author                   = {Ian Chai},
  School                   = {University of Illinois at Urbana-Champaign},
  Year                     = {2000},

  Address                  = {Urbana, Illinois, USA},

  Abstract                 = {Frameworks are a key to reuse in the Object-Oriented programming world, but they are hard to learn and document. Different people have proposed different ways to document frameworks, but it is unclear which ones actually are better, so we did an empirical study to see which documentation philosophy is best for documentation for new users of a framework. We found that different documentation styles are better for different situations, depending on what your goals are, and discovered guidelines that can help someone who is trying to document a framework.},
  Url                      = {http://ezproxy.lib.ucalgary.ca:2048/login?url=http://search.proquest.com/docview/304595585?accountid=9838}
}

@InProceedings{2003:cbr:champin,
  Title                    = {Measuring the Similarity of Labeled Graphs},
  Author                   = {Pierre-Antoine Champin and Christine Solnon},
  Booktitle                = cbr,
  Year                     = {2003},
  Pages                    = {1066--1067},
  Series                   = lncs,
  Volume                   = {2689},

  Abstract                 = {This paper proposes a similarity measure to compare cases represented by labeled graphs. We first define an expressive model of directed labeled graph, allowing multiple labels on vertices and edges. Then we define the similarity problem as the search of a best mapping, where a mapping is a correspondence between vertices of the graphs. A key point of our approach is that this mapping does not have to be univalent, so that a vertex in a graph may be associated with several vertices of the other graph. Another key point is that the quality of the mapping is determined by generic functions, which can be tuned in order to implement domain-dependant knowledge. We discuss some computational issues related to this problem, and we describe a greedy algorithm for it. Finally, we show that our approach provides not only a quantitative measure of the similarity, but also qualitative information which can prove valuable in the adaptation phase of CBR.},
  Doi                      = {10.1007/3-540-45006-8_9}
}

@Article{2009:dam:chandrasekaran,
  Title                    = {Structural analysis of a fractional matching problem},
  Author                   = {R. Chandrasekaran and M. Dawande},
  Journal                  = dam,
  Year                     = {2009},

  Month                    = {28 } # nov,
  Number                   = {18},
  Pages                    = {3708--3720},
  Volume                   = {157},

  Abstract                 = {Mixed Software Programming refers to a novel software development paradigm resulting from efforts to combine two different programming approaches: Solo Programming and Pair Programming. Solo Programming refers to the traditional practice of assigning a single developer to develop a software module and Pair Programming refers to a relatively new approach where two developers work simultaneously on developing a module. In Mixed Programming, given a set of modules to be developed, a chosen subset of modules may be developed using Solo Programming and the remaining modules using Pair Programming. Motivated by applications in Mixed Software Programming, we consider the following generalization of classical fractional 1-matching problem: Given an undirected simple graph $G = (V; E)$, and a positive number $F$, find values for $x_e, e \in E$, satisfying the following: 1. $x \in \left\{ 0, \frac{1}{2}, 1 \right\}\; \forall e \in E$. 2. $\sum_{e \in \delta(i)} x_e \le 1\; \forall i \in V$, where $\delta(i) = \left\{ e \in E: e = (i, j) \right\}, i \in V$. 3. Maximize $\left{ 2\sum_{e \in E} x_e - F \left| \left\{ i \in V: \sum_{e \in \delta(i)} x_e = 1 \right\} \right| \right\}$. We show that this problem is solvable in strongly polynomial time. Our primary focus in this paper is on obtaining the structure of the optimal solution for an arbitrary instance of the problem.},
  Doi                      = {10.1016/j.dam.2009.07.012}
}

@Article{2001:jsmerp:chapin,
  Title                    = {Types of software evolution and software maintenance},
  Author                   = {Chapin, Ned and Hale, Joanne E. and Khan, Khaled Md. and Ramil, Juan F. and Tan, Wui-Gee},
  Journal                  = jsmerp,
  Year                     = {2001},

  Month                    = jan # {/} # feb,
  Number                   = {1},
  Pages                    = {3--30},
  Volume                   = {13},

  Abstract                 = {The past two decades have seen increasing sophistication in software work. Now and in the future, the work of both practitioners and researchers would be helped by a more objective and finer granularity recognition of types of software evolution and software maintenance activities as actually done. To these ends, this paper proposes a clarifying redefinition of the types of software evolution and software maintenance. The paper bases the proposed classification not on people's intentions but upon objective evidence of maintainers' activities ascertainable from observation of activities and artifacts, and/or a before and after comparison of the software documentation. The classification includes taking into account in a semi-hierarchical manner evidence of the change or lack thereof in: (1) the software, (2) the documentation, (3) the properties of the software, and (4) the customer-experienced functionality. A comparison is made with other classifications and typologies. The paper provides a classified list of maintenance activities and a condensed decision tree as a summary guide to the proposed evidence-based classification of the types of software evolution and software maintenance.},
  Doi                      = {10.1002/smr.220}
}

@Book{2006:book:charmaz,
  Title                    = {Constructing Grounded Theory},
  Author                   = {Kathy Charmaz},
  Publisher                = {Sage Publications},
  Year                     = {2006}
}

@InProceedings{2001:icsm:chen,
  Title                    = {{CVSSearch}: Searching through Source Code using {CVS} Comments},
  Author                   = {Chen, Annie and Chou, Eric and Wong, Joshua and Yao, Andrew Y. and Zhang, Qing and Zhang, Shao and Michail, Amir},
  Booktitle                = icsm,
  Year                     = {2001},
  Pages                    = {364--373},

  Abstract                 = {CVSSearch is a tool that searches for fragments of source code by using CVS comments. CVS is a version control system that is widely used in the open source community [10]. Our search tool takes advantage of the fact that a CVS comment typically describes the lines of code involved in the commit and this description will typically hold for many future versions. In other words, CVSSearch allows one to better search the most recent version of the code by looking at previous versions to better understand the current version. In this paper, we describe our algorithm for mapping CVS comments to the corresponding source code, present a search tool based on this technique, and discuss preliminary feedback.}
}

@InProceedings{2000:iwpc:chen,
  Title                    = {Case study of feature location using dependence graphs},
  Author                   = {Kunrong Chen and V\'{a}clav Rajlich},
  Booktitle                = iwpc,
  Year                     = {2000},
  Pages                    = {241--247},

  Abstract                 = {Software change requests are often formulated as requests to modify or to add a specific feature or concept. To implement these changes, the features or concepts must be located in the code. We describe the scenarios of the feature and concept location. The scenarios utilize a computer-assisted search of software dependence graph. Scenarios are demonstrated by a case study of NCSA Mosaic source code.}
}

@InProceedings{2008:iccit:chen,
  Title                    = {Module-Based Large-Scale Software Evolution Based on Complex Networks},
  Author                   = {Tao Chen and Qing Gu and Shusen Wang and Xiaoan Chen and Daoxu Chen},
  Booktitle                = iccit,
  Year                     = {2008},
  Pages                    = {798--803},

  Abstract                 = {Large-scale software systems usually consist of a huge number of modules, and have a series of releases along with these modules. This can be seen as software evolution. In recent years, researchers have put forward several models of software evolution by employing the theory of complex networks. In this paper, we put forward a refined model of software evolution based on the BA model: module-based evolution. We theoretically prove that the power-law degree distribution can be held in our model. We also build a tool to construct and analyze the class diagrams of JDK (Java Development Kits) evolved from version 1.2 to 1.6. The class diagrams can be seen as complex networks under evolution. We apply the module-based evolution model to these complex networks and simulate the evolution of key network features such as average clustering coefficient and average path length. Compared with real networks, our model can precisely describe the evolution of these features, and be used to help developers understand the characteristics of large-scale software evolution.}
}

@Article{1990:tse:chen,
  Title                    = {The {C} Information Abstraction System},
  Author                   = {Yih-Farn Chen and Michael Y. Nishimoto and C. V. Ramamoorthy},
  Journal                  = tse,
  Year                     = {1990},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {325--334},
  Volume                   = {16},

  Abstract                 = {A system for analyzing program structures is described. The system extracts relational information from C programs according to a conceptual model and stores the information in a database. It is shown how several interesting software tasks can be performed by using the relational views. These tasks include generation of graphical views, subsystem extraction, program layering, dead code elimination and binding analysis.},
  Doi                      = {10.1109/32.48940}
}

@InProceedings{1995:icsm:chen,
  Title                    = {Ciao: A graphical navigator for software and document repositories},
  Author                   = {Yih-Farn R. Chen and Glenn S. Fowler and Eleftherios Koutsofios and Ryan S. Wallach},
  Booktitle                = icsm,
  Year                     = {1995},
  Pages                    = {66--75},

  Abstract                 = {Programmers frequently have to retrieve and link information from various software documents to accomplish a maintenance task. Ciao is a graph-based navigator that helps programmers query and browse structural connections embedded in different software and document repositories. A repository consists of a collection of source documents with an associated database that describes their structure. Ciao supports repositories organized in an architecture style called Aero, which exploits the duality between a class of entity-relationship (ER) databases and directed attributed graphs (DAG). Database queries and graph analysis operators in Aero are plug-compatible because they all take an ER database and produce yet another ER database by default. Various presentation filters generate graph views, source views, and relational views from any compatible ER database. The architecture promotes the construction of successively more complex operators using a notion of virtual database pipelines. Ciao has been instantiated for C and C++ program databases, and program difference databases. The latter allows programmers to explore program structure changes by browsing and expanding graphs that highlight changed, deleted, and added entities and relationships. The unifying ER model under ciao also allows users to navigate different software repositories and make necessary connections. We have linked program difference databases and modification request (MR) databases so that users can investigate the connections between MRs and affected entities. Ciao has been applied to several large communications software projects and we report experiences and lessons learned from these applications.}
}

@Article{1954:ams:chernoff,
  Title                    = {The use of maximum likelihood estimates in {$\chi^2$} tests for goodness-of-fit},
  Author                   = {Chernoff, Herman and Lehmann, E. L.},
  Journal                  = ams,
  Year                     = {1954},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {579--586},
  Volume                   = {25},

  Abstract                 = {The usual test that a sample comes from a distribution of given form is performed by counting the number of observations falling into specified cells and applying the $\chi^2$ test to these frequencies. In estimating the parameters for this test, one may use the maximum likelihood (or equivalent) estimate based (1) on the cell frequencies, or (2) on the original observations. This paper shows that in (2), unlike the well known result for (1), the test statistic does not have a limiting $\chi^2$-distribution, but that it is stochastically larger than would be expected under the $\chi^2$ theory. The limiting distribution is obtained and some examples are computed. These indicate that the error is not serious in the case of fitting a Poisson distribution, but may be so for the fitting of a normal.},
  Doi                      = {10.1214/aoms/1177728726}
}

@InProceedings{2007:vlhcc:cherubini,
  Title                    = {Building an Ecologically Valid, Large-Scale Diagram to Help Developers Stay Oriented in Their Code},
  Author                   = {Cherubini, Mauro and Venolia, Gina and DeLine, Rob},
  Booktitle                = vlhcc,
  Year                     = {2007},
  Pages                    = {157--162},

  Abstract                 = {This paper presents the creation, deployment, and evaluation of a large-scale, spatially-stable, paper-based visualization of a software system. The visualization was created for a single team, who were involved systematically in its initial design and subsequent design iterations. The evaluation indicates that the visualization supported the "onboarding" scenario but otherwise failed to realize the research team's expectations. We present several lessons learned, and cautions to future research into large-scale, spatially-stable visualizations of software systems.},
  Doi                      = {10.1109/VLHCC.2007.19}
}

@InProceedings{2003:chi:chi,
  Title                    = {The {B}loodhound {P}roject: Automating discovery of web usability issues using the {InfoScent} simulator},
  Author                   = {Chi, Ed H. and Rosien, Adam and Supattanasiri, Gesara and Williams, Amanda and Royer, Christiaan and Chow, Celia and Robles, Erica and Dalal, Brinda and Chen, Julie and Cousins, Steve},
  Booktitle                = chi,
  Year                     = {2003},
  Pages                    = {505--512},

  Abstract                 = {According to usability experts, the top user issue for Web sites is difficult navigation. We have been developing automated usability tools for several years, and here we describe a prototype service called InfoScent Bloodhound Simulator, a push-button navigation analysis system, which automatically analyzes the information cues on a Web site to produce a usability report. We further build upon previous algorithms to create a method called Information Scent Absorption Rate, which measures the navigability of a site by computing the probability of users reaching the desired destinations on the site. Lastly, we present a user study involving 244 subjects over 1385 user sessions that show how Bloodhound correlates with real users surfing for in-formation on four Web sites. The hope is that, by using a simulation of user surfing behavior, we can reduce the need for human labor during usability testing, thus dramatically lower testing costs, and ultimately improving user experience. The Bloodhound Project is unique in that we apply a concrete HCI theory directly to a real-world problem. The lack of empirically validated HCI theoretical model has plagued the development of our field, and this is a step toward that direction.},
  Doi                      = {10.1145/642611.642699}
}

@Article{1994:tse:chidamber,
  Title                    = {A metrics suite for object oriented design},
  Author                   = {S. R. Chidamber and C. F. Kemerer},
  Journal                  = tse,
  Year                     = {1994},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {476--493},
  Volume                   = {20},

  Abstract                 = {Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber (1989), the theoretical base chosen for the metrics was the ontology of Bunge (1977). Six design metrics are developed, and then analytically evaluated against Weyuker's (1988) proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement.},
  Doi                      = {10.1109/32.295895}
}

@Article{1990:software:chikofsky,
  Title                    = {Reverse Engineering and Design Recovery: A Taxonomy},
  Author                   = {Chikofsky, Elliot J. and Cross, II, James H.},
  Journal                  = software,
  Year                     = {1990},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {13--17},
  Volume                   = {7},

  Doi                      = {10.1109/52.43044}
}

@InProceedings{2003:iwpc:chiricota,
  Title                    = {Software components capture using graph clustering},
  Author                   = {Chiricota, Yves and Jourdan, Fabien and Melan{\c{c}}on, Guy},
  Booktitle                = iwpc,
  Year                     = {2003},
  Pages                    = {217--226},

  Abstract                 = {We describe a simple, fast computing and easy to implement method for finding relatively good clusterings of software systems. Our method relies on the ability to compute the strength of an edge in a graph by applying a straightforward metric defined in terms of the neighborhoods of its end vertices. The metric is used to identify the weak edges of the graph, which are momentarily deleted to break it into several components. We study the quality metric MQ introduced by S. Mancoridis et al. (1998) and exhibit mathematical properties that make it a good measure for clustering quality. Letting the threshold weakness of edges vary defines a path, i.e. a sequence of clusterings in the solution space (of all possible clustering of the graph). This path is described in terms of a curve linking MQ to the weakness of the edges in the graph.},
  Doi                      = {10.1109/WPC.2003.1199205}
}

@InProceedings{1996:icsm:chow,
  Title                    = {Semi-automatic Update of Applications in Response to Library Changes},
  Author                   = {Kingsum Chow and David Notkin},
  Booktitle                = icsm,
  Year                     = {1996},
  Pages                    = {359--368},

  Abstract                 = {Software libraries provide leverage largely because they are used by many applications. As Parnas (1972, 1979), Lampson (1984) and others have noted, stable interfaces to libraries isolate the application from changes in the libraries. That is, as long as there is no change in a library's syntax or semantics, applications can use updated libraries simply by importing and linking the new version. However, libraries are indeed changed from time to time and the tedious job of adapting the application source to the library interface changes becomes a burden to multitudes of programmers. The paper introduces an approach and a toolset intended to reduce these costs. Specifically, in the authors' approach, a library maintainer annotates changed functions with rules that are used to generate tools that will update the applications that use the updated libraries. Thus, in exchange for a small added amount of work by the library maintainer, costs for each application maintainer can be reduced. They present the basic approach, describe the tools that support the approach, and discuss the strengths and limitation of the approach.},
  Doi                      = {10.1109/ICSM.1996.565039}
}

@InProceedings{2012:icse:chowdhury,
  Title                    = {Assisting End-User Development in Browser-Based Mashup Tools},
  Author                   = {Soudip Roy Chowdhury},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {1625--1627},

  Abstract                 = {Despite the recent progresses in end-user development and particularly in mashup application development, developing even simple mashups is still non-trivial and requires intimate knowledge about the functionality of web APIs and services, their interfaces, parameter settings, data mappings, and so on. We aim to assist less skilled developers in composing own mashups by interactively recommending composition knowledge in the form of modeling patterns and fostering knowledge reuse. Our prototype system demonstrates our idea of interactive recommendation and automated pattern weaving, which involves recommending relevant composition patterns to the users during development, and once selected, applying automatically the changes as suggested in the selected pattern to the mashup model under development. The experimental evaluation of our prototype implementation demonstrates that even complex composition patterns can be efficiently stored, queried and weaved into the model under development in browser-based mashup tools.},
  Doi                      = {10.1109/ICSE.2012.6227222}
}

@TechReport{2009:tr:christensen,
  Title                    = {Software Architectural Evolution in the {D}ragon Project},
  Author                   = {Michael Christensen and Christian Heide Damm and Klaus Marius Hansen and Elmer Sandvad and Michael Thomsen},
  Institution              = {University of Aarhus},
  Year                     = {2009},

  Address                  = {Aarhus, Denmark},

  Abstract                 = {Our experience from the first two major phases of a software development project suggests that explicit focus on software architecture in these phases was an important key to success. More specifically: Demands for stability, flexibility and proper work organisation in an initial prototyping phase of a project are facilitated by having an explicit architecture, which, however should also allow for certain degrees of freedom for experimentation. Furthermore, in a following evolutionary development phase, architectural redesign is necessary and should be firmly based on experience gained from working within the prototype architecture. Finally, to get it right, the architecture needs to be prototyped, or iterated upon, throughout evolutionary development cycles. In this architectural prototyping process, we address the difficult issue of identifying and evolving functional components in the architecture and point to an architectural strategy: a set of architectures, their context and evolution, which was helpful in this respect.},
  Url                      = {www.cit.dk/cot/reports/reports/Case5/09/cot-5-09.pdf}
}

@InProceedings{1999:tools:christensen,
  Title                    = {Design and Evolution of Software Architecture in Practice},
  Author                   = {Christensen, Michael and Damm, Christian Heide and Hansen, Klaus Marius and Sandvad, Elmer and Thomsen, Michael},
  Booktitle                = tools,
  Year                     = {1999},
  Pages                    = {2--15},

  Abstract                 = {With special focus on software architectural issues, we report from the first two major phases of a software development project. Our experience suggests that explicit focus on software architecture in these phases was an important key to success. More specifically: Demands for stability, flexibility and proper work organization in an initial prototyping phase of a project are facilitated by having an explicit architecture. However, the architecture should also allow for certain degrees of freedom for experimentation. Furthermore, in a following evolutionary development phase, architectural redesign is necessary and should be firmly based on experience gained from working within the prototype architecture. Finally, to get it right, the architecture needs to be prototyped, or iterated upon, throughout evolutionary development cycles. In this architectural prototyping process, we address the difficult issue of identifying and evolving functional components in the architecture and point to an architectural strategy---a set of architectures, their context and evolution---that was helpful in this respect.}
}

@InProceedings{2009:ppcp:le_clement,
  Title                    = {Constraint-based graph matching},
  Author                   = {le Cl{\'e}ment, Vianney and Deville, Yves and Solnon, Christine},
  Booktitle                = ppcp,
  Year                     = {2009},
  Pages                    = {274--288},
  Series                   = lncs,
  Volume                   = {5732},

  Abstract                 = {Measuring graph similarity is a key issue in many applications. We propose a new constraint-based modeling language for defining graph similarity measures by means of constraints. It covers measures based on univalent matchings, such that each node is matched with at most one node, as well as multivalent matchings, such that a node may be matched with a set of nodes. This language is designed on top of Comet, a programming language supporting both Constraint Programming (CP) and Constraint-Based Local Search (CBLS). Starting from the constraint-based description of the measure, we automatically generate a Comet program for computing the measure. Depending on the measure characteristics, this program either uses CP---which is better suited for computing exact measures such as (sub)graph isomorphism---or CBLS---which is better suited for computing error-tolerant measures such as graph edit distances. First experimental results show the feasibility of our approach.},
  Doi                      = {10.1007/978-3-642-04244-7_23}
}

@Article{2004:ddj:clarke,
  Title                    = {Measuring {API} usability},
  Author                   = {Steven Clarke},
  Journal                  = ddj,
  Year                     = {2004},

  Month                    = {1 } # may,
  Pages                    = {S6--S9},

  Abstract                 = {The Visual Studio usability group at Microsoft, of which I'm a part, is responsible for helping design and build the user experience for developers using Visual Studio and the .NET platform. Much of our efforts at improving the developer experience have focused on improving the usability of the Visual Studio development tool itself. For example, we run extensive empirical studies in usability labs focusing on particular features of Visual Studio (such as the debugger), gathering data to help feature teams improve their designs. In this way, we can understand how best to present complex information, such as the structure of application data, to support developers better when they are debugging code. But while improving the design of Visual Studio is a necessary component of improving the developer experience, it is not sufficient. The development tool is only one tool that developers routinely use when building applications. The other major tools are the programming language they are coding in (C++, C#, VB, and so on) and any class libraries or APIs that they are coding against (MFC, .NET Frameworks, and the like). The usability of the languages and libraries that developers use have a significant impact on their ability to successfully complete a set of development tasks. In this article, I describe some of the techniques we use to design and evaluate the usability of the APIs that ship with .NET, and discuss ways in which you can use them for APIs you are designing.},
  Url                      = {http://www.drdobbs.com/windows/measuring-api-usability/184405654},
  Voulme                   = {29}
}

@Article{2009:siam:clauset,
  Title                    = {Power-law distributions in empirical data},
  Author                   = {Aaron Clauset and Cosma Rohilla Shalizi and M. E. J. Newman},
  Journal                  = siam,
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {661--703},
  Volume                   = {51},

  Abstract                 = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution---the part of the distribution representing large but rare events---and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a powerlaw distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
  Doi                      = {10.1137/070710111}
}

@Article{2009:ese:cleary,
  Title                    = {An empirical analysis of information retrieval based concept location techniques in software comprehension},
  Author                   = {Cleary, Brendan and Exton, Chris and Buckley, Jim and English, Michael},
  Journal                  = ese,
  Year                     = {2009},

  Month                    = feb,
  Number                   = {1},
  Pages                    = {93--130},
  Volume                   = {14},

  Abstract                 = {Abstract Concept location, the problem of associating human oriented concepts with their counterpart solution domain concepts, is a fundamental problem that lies at the heart of software comprehension. Recent research has attempted to alleviate the impact of the concept location problem through the application of methods drawn from the information retrieval {(IR)} community. Here we present a new approach based on a complimentary {IR} method which also has a sound basis in cognitive theory. We compare our approach to related work through an experiment and present our conclusions. This research adapts and expands upon existing language modelling frameworks in {IR} for use in concept location, in software systems. In doing so it is novel in that it leverages implicit information available in system documentation. Surprisingly, empirical evaluation of this approach showed little performance benefit overall and several possible explanations are forwarded for this finding.},
  Doi                      = {10.1007/s10664-008-9095-3}
}

@Article{1988:atttj:cleaveland,
  Title                    = {Tools for building application generators},
  Author                   = {Cleaveland, J. Craig and Kintala, Chandra M. R.},
  Journal                  = atttj,
  Year                     = {1988},

  Month                    = jul # {--} # aug,
  Number                   = {4},
  Pages                    = {46--58},
  Volume                   = {67},

  Doi                      = {10.1002/j.1538-7305.1988.tb00637.x}
}

@InProceedings{2006:aosd:coelho,
  Title                    = {Presenting crosscutting structure with active models},
  Author                   = {Coelho, Wesley and Murphy, Gail C.},
  Booktitle                = aosd,
  Year                     = {2006},
  Pages                    = {158--168},

  Abstract                 = {When modifying or debugging a software system, among other tasks, developers must often understand and manipulate source code that crosscuts the system's structure. These tasks are made more difficult by limitations of the two approaches currently used to present details of crosscutting structure: tree views and structural diagrams. Tree views force the developer to manually synthesize information from multiple views; structure diagrams quickly suffer from graphical complexity. We introduce an active model as a means of presenting the right information about crosscutting structure to a developer at the right time. An active model is produced as a result of three automated operations---projection, expansion, and abstraction. Combined with particular user interaction features during display, these operations enable a view of the model to be presented to the developer without suffering from the complexity of existing approaches. We have implemented an active model tool, called ActiveAspect, for presenting crosscutting structure described by AspectJ aspects. We report on the results of a case study in which the tool was used effectively by two subjects to implement a modification task to a non-trivial AspectJ system.}
}

@InProceedings{1995:icml:cohen,
  Title                    = {Fast Effective Rule Induction},
  Author                   = {William W. Cohen},
  Booktitle                = icml,
  Year                     = {1995},
  Pages                    = {115--123}
}

@InProceedings{2006:scam:collard,
  Title                    = {Factoring changes for iterative change management},
  Author                   = {Collard, Michael L. and Kagdi, Huzefa and Maletic, Jonathan I.},
  Booktitle                = scam,
  Year                     = {2006},
  Pages                    = {217--226},

  Abstract                 = {An approach for factoring source-code differences is presented. A single large difference between two versions of a program is decomposed into factors (i.e., smaller changes). The application of all the factors is equivalent to the application of the single large difference. The factors are obtained by user-defined criteria. They include changes that are limited to a specific syntactic construct or ones that occur throughout a file. The factors can be applied individually or in combination to generate intermediate forms of the large change. This directly supports iterative software change management by decomposing large changes into smaller factors. The approach uses srcDiff, an XML representation of multiple versions of a source-code file and their differences. XML transformations are used to factor a change according to an XPath expression. The approach is applied to changes made to a large opensource system. The results indicate the approach is flexible and efficient, and can be integrated with common differencing tools.},
  Doi                      = {10.1109/SCAM.2006.15}
}

@InProceedings{2010:simutools:collinson,
  Title                    = {Semantics for structured systems modelling and simulation},
  Author                   = {Collinson, Matthew and Monahan, Brian and Pym, David},
  Booktitle                = simutools,
  Year                     = {2010},
  Pages                    = {34:1--34:8},

  Abstract                 = {Simulation modelling is an important tool for exploring and reasoning about complex systems. Many supporting languages are available. Commonly occurring features of these languages are constructs capturing concepts such as process, resource, and location. We describe a mathematical framework that supports a modelling idiom based on these core concepts, and which adopts stochastic methods for representing the environments within which systems exist. We explain how this framework can be used to give a semantics to a simulation modelling language, Core Gnosis, that includes basic constructs for process, resource, and location. We include a brief discussion of a logic for reasoning about models that is compositional with respect to their structure. Our mathematical analysis of systems in terms of process, resource, location, and stochastic environment, together with a language that captures these concepts quite directly, yields an efficient and robust modelling framework within which natural mathematical reasoning about systems is captured.},
  Doi                      = {10.4108/ICST.SIMUTOOLS2010.8631}
}

@Article{2007:tse:concas,
  Title                    = {Power-laws in a large object-oriented software system},
  Author                   = {Giulio Concas and Michele Marchesi and Sandro Pinna and Nicola Serra},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {687--708},
  Volume                   = {33},

  Abstract                 = {We present a comprehensive study of an implementation of the Smalltalk object oriented system, one of the first and purest object-oriented programming environment, searching for scaling laws in its properties. We study ten system properties, including the distributions of variable and method names, inheritance hierarchies, class and method sizes, system architecture graph. We systematically found Pareto---or sometimes log-normal---distributions in these properties. This denotes that the programming activity, even when modeled from a statistical perspective, can in no way be simply modeled as a random addition of independent increments with finite variance, but exhibits strong organic dependencies on what has been already developed. We compare our results with similar ones obtained for large Java systems, reported in the literature or computed by ourselves for those properties never studied before, showing that the behavior found is similar in all studied object oriented systems. We show how the Yule process is able to stochastically model the generation of several of the power-laws found, identifying the process parameters and comparing theoretical and empirical tail indexes. Lastly, we discuss how the distributions found are related to existing object-oriented metrics, like Chidamber and Kemerer's, and how they could provide a starting point for measuring the quality of a whole system, versus that of single classes. In fact, the usual evaluation of systems based on mean and standard deviation of metrics can be misleading. It is more interesting to measure differences in the shape and coefficients of the data's statistical distributions.},
  Doi                      = {10.1109/TSE.2007.1019}
}

@InProceedings{1993:popl:consel,
  Title                    = {Tutorial notes on partial evaluation},
  Author                   = {Consel, Charles and Danvy, Olivier},
  Booktitle                = popl,
  Year                     = {1993},
  Pages                    = {493--501},

  Abstract                 = {The last years have witnessed a flurry of new results in the area of partial evaluation. These tutorial notes survey the field and present a critical assessment of the state of the art.}
}

@Article{1995:vldbj:constantopoulos,
  Title                    = {The Software Information Base: A Server for Reuse},
  Author                   = {Panos Constantopoulos and Matthias Jarke and John Mylopoulos and Yannis Vassiliou},
  Journal                  = vldbj,
  Year                     = {1995},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1--43},
  Volume                   = {4},

  Abstract                 = {We present an experimental software repository system that provides organization, storage, management, and access facilities for reusable software components. The system, intended as part of an applications development environment, supports the representation of information about requirements, designs and implementations of software, and offers facilities for visual presentation of the software objects. This article details the features and architecture of the repository system, the technical challenges and the choices made for the system development along with a usage scenario that illustrates its functionality. The system has been developed and evaluated within the context of the ITHACA project, a technology integration/software engineering project sponsored by the European Communities through the ESPRIT program, aimed at developing an integrated reuse-centered application development and support environment based on object-oriented techniques.},
  Doi                      = {10.1007/BF01232471}
}

@InProceedings{1971:stoc:cook,
  Title                    = {The complexity of theorem-proving procedures},
  Author                   = {Stephen A. Cook},
  Booktitle                = stoc,
  Year                     = {1971},
  Pages                    = {151--158},

  Abstract                 = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be ``reduced'' to the problem of determining whether a given propositional formula is a tautology. Here ``reduced'' means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
  Doi                      = {10.1145/800157.805047}
}

@Article{1995:ddj:coppieters,
  Title                    = {A Cross-Platform Binary Diff},
  Author                   = {Kris Coppieters},
  Journal                  = ddj,
  Year                     = {1995},

  Abstract                 = {Binary file comparison is useful for many applications. One example is sending updates of large files over a communications line: Instead of sending a complete update each time, you could send the complete file once, then create a diff file containing the differences between the original file and the updated file. At the receiving side, this diff file could be used to update the original file. Creating a diff file is processor and memory intensive. Under DOS, such a process can easily exceed the 640K limit. On the other hand, using a diff file to update the file is very lightweight and fast. In such cases, it may be desirable to create the diff file on another platform and use the resulting file under DOS. Such were the requirements when I created BinDiff, a utility that intelligently compares two versions of a binary file and creates a diff file. The algorithm in BinDiff tries to find equal chunks in the two files being compared. BinDiff then uses an indexing algorithm to find matching chunks so equal chunks need not be in the same sequence in both files. BinDiff is built from a single C source file that compiles on UNIX, OS/2, DOS, and the Macintosh. A command-line user interface is used on the first three platforms, while a point-and-click interface is used on the Mac. Because BinDiff is insensitive to Endianness, diff files created on one platform can be used on another.},
  Url                      = {http://www.drdobbs.com/embedded-systems/a-cross-platform-binary-diff/184409550?pgno=3}
}

@Article{1990:qs:corbin,
  Title                    = {Grounded theory research: Procedures, canons, and evaluative criteria},
  Author                   = {Juliet Corbin and Anselm Strauss},
  Journal                  = qs,
  Year                     = {1990},

  Month                    = {1 } # mar,
  Number                   = {1},
  Pages                    = {3--21},
  Volume                   = {13},

  Abstract                 = {Using grounded theory as an example, this paper examines three methodological questions that are generally applicable to all qualitative methods. How should the usual scientific canons be reinterpreted for qualitative research? How should researchers report the procedures and canons used in their research? What evaluative criteria should be used in judging the research products? We propose that the criteria should be adapted to fit the procedures of the method. We demonstrate how this can be done with grounded theory and suggest criteria for evaluating studies following this approach. We argue that other qualitative researchers might be similarly specific about their procedures and evluative criteria.},
  Doi                      = {10.1007/BF00988593}
}

@Article{2006:scp:cordy,
  Title                    = {The {TXL} source transformation language},
  Author                   = {James R. Cordy},
  Journal                  = scp,
  Year                     = {2006},

  Month                    = aug,
  Number                   = {3},
  Pages                    = {190--210},
  Volume                   = {61},

  Abstract                 = {TXL is a special-purpose programming language designed for creating, manipulating and rapidly prototyping language descriptions, tools and applications. TXL is designed to allow explicit programmer control over the interpretation, application, order and backtracking of both parsing and rewriting rules. Using first order functional programming at the higher level and term rewriting at the lower level, TXL provides for flexible programming of traversals, guards, scope of application and parameterized context. This flexibility has allowed TXL users to express and experiment with both new ideas in parsing, such as robust, island and agile parsing, and new paradigms in rewriting, such as XML mark-up, rewriting strategies and contextualized rules, without any change to TXL itself. This paper outlines the history, evolution and concepts of TXL with emphasis on its distinctive style and philosophy, and gives examples of its use in expressing and applying recent new paradigms in language processing.},
  Doi                      = {10.1016/j.scico.2006.04.002}
}

@Article{2004:entcs:cordy,
  Title                    = {{TXL}: A Language for Programming Language Tools and Applications},
  Author                   = {Cordy, James R.},
  Journal                  = entcs,
  Year                     = {2004},

  Month                    = {31 } # dec,
  Pages                    = {3--31},
  Volume                   = {110},

  Abstract                 = {TXL is a special-purpose programming language designed for creating, manipulating and rapidly prototyping language descriptions, tools and applications. TXL is designed to allow explicit programmer control over the interpretation, application, order and backtracking of both parsing and rewriting rules. Using first order functional programming at the higher level and term rewriting at the lower level, TXL provides for flexible programming of traversals, strategies, guards, scope of application and parameterized context. This flexibility has allowed TXL users to express and experiment with both new ideas in parsing, such as robust, island and agile parsing, and new paradigms in rewriting, such as XML markup, rewriting strategies and contextualized rules, without any change to TXL itself. In this paper I outline the history, evolution and concepts of TXL with emphasis on what makes it different from other language manipulation tools, and give examples of its use in expressing and applying recent new paradigms in language processing.},
  Doi                      = {10.1016/j.entcs.2004.11.006}
}

@InProceedings{2003:iwpc:cordy,
  Title                    = {Comprehending reality: Practical barriers to industrial adoption of software maintenance automation},
  Author                   = {James R. Cordy},
  Booktitle                = iwpc,
  Year                     = {2003},
  Pages                    = {196--205},

  Abstract                 = {Recent years have seen many significant advances in program comprehension and software maintenance automation technology. In spite of the enormous potential savings in software maintenance costs, for the most part adoption of these ideas in industry remains at the experimental prototype stage. In this paper I explore some of the practical reasons for industrial resistance to adoption of software maintenance automation. Based on the experience of six years of software maintenance automation services to the financial industry involving more than 4.5 Gloc of code at Legasys Corporation, I discuss some of the social, technical and business realities that lie at the root of this resistance, outline various Legasys attempts overcome these barriers, and suggest some approaches to software maintenance automation that may lead to higher levels of industrial acceptance in the future.},
  Doi                      = {10.1109/WPC.2003.1199203}
}

@Book{2009:book:cormen,
  Title                    = {Introduction to Algorithms},
  Author                   = {Thomas H. Cormen and Charles E. Leiserson and Ronald L. Rivest and Clifford Stein},
  Publisher                = {MIT Press},
  Year                     = {2009},
  Edition                  = {3rd}
}

@InProceedings{2007:acom:costa,
  Title                    = {Semantic Dependencies and Modularity of Aspect-Oriented Software},
  Author                   = {Costa Neto, Alberto and de Medeiros Ribeiro, Marcio and Dosea, Marcos and Bonifacio, Rodrigo and Borba, Paulo and Soares, S\'{e}rgio},
  Booktitle                = acom,
  Year                     = {2007},
  Pages                    = {11:1--11:3},

  Abstract                 = {Modularization of crosscutting concerns is the main benejtprovided by Aspect-Oriented constructs. In order to rigorously assess the overall impact of this kind of modularization, we use Design Structure Matrixes (DSMs) to analyze different versions (OO and AO) of a system. This is supported by the concept of semantic dependencies between classes and aspects, leading to a more faithful notion of coupling for AO systems. We also show how design rules can make those dependencies explicit and, consequently, yield a more modular design.}
}

@InProceedings{2002:iwpc:counsell,
  Title                    = {Comprehension of object-oriented software cohesion: The empirical quagmire},
  Author                   = {Steve Counsell and Emilia Mendes and Stephen Swift},
  Booktitle                = iwpc,
  Year                     = {2002},
  Pages                    = {33--42},

  Abstract                 = {Chidamber and Kemerer (1991) proposed an object-oriented (OO) metric suite which included the Lack of Cohesion Of Methods (LCOM) metric. Despite considerable effort both theoretically and empirically since then, the software engineering community is still no nearer finding a generally accepted definition or measure of OO cohesion. Yet, achieving highly cohesive software is a cornerstone of software comprehension and hence, maintainability. In this paper, we suggest a number of suppositions as to why a definition has eluded (and we feel will continue to elude) us. We support these suppositions with empirical evidence from three large C++ systems and a cohesion metric based on the parameters of the class methods; we also draw from other related work. Two major conclusions emerge from the study. Firstly, any sensible cohesion metric does at least provide insight into the features of the systems being analysed. Secondly however, and less reassuringly, the deeper the investigative search for a definitive measure of cohesion, the more problematic its understanding becomes; this casts serious doubt on the use of cohesion as a meaningful feature of object-orientation and its viability as a tool for software comprehension.}
}

@Article{2006:sen:counsell,
  Title                    = {Object-oriented cohesion subjectivity amongst experienced and novice developers: An empirical study},
  Author                   = {Steve Counsell and Stephen Swift and Allan Tucker and Emilia Mendes},
  Journal                  = sen,
  Year                     = {2006},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {3:1--3:10},
  Volume                   = {31},

  Abstract                 = {The concept of software cohesion in both the procedural and object-oriented paradigm is well known and documented. What is not so well known or documented is the perception of what empirically constitutes a cohesive `unit' by software engineers. In this paper, we describe an empirical investigation using object-oriented (OO) classes as a basis. Twenty-four subjects (drawn from IT experienced and novice groups) were asked to rate ten classes sampled from two industrial systems in terms of their overall cohesiveness; a class environment was used to carry out the study. Three hypotheses were investigated as part of the study, relating to class size, the role of comment lines and the differences between the two groups in terms of how they rated cohesion. Several key results were observed. Firstly, class size (when expressed in terms of number of methods) only influenced the perception of cohesion by novice subjects. Secondly, well-commented classes were rated more highly amongst IT experienced than novice subjects. Thirdly, results suggest strongly that cohesion comprises a combination of various class factors including low coupling, small numbers of attributes and well-commented methods, rather than any single, individual class feature per se. Finally, if the research supports the view that cohesion is a subjective concept reflecting a cognitive combination of class features then cohesion is also a surrogate for class comprehension.},
  Doi                      = {10.1145/1163514.1163530}
}

@InProceedings{1977:popl;cousot,
  Title                    = {Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints},
  Author                   = {Cousot, Patrick and Cousot, Radhia},
  Booktitle                = popl,
  Year                     = {1977},
  Pages                    = {238--252},

  Abstract                 = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text $-1515 * 17$ may be understood to denote computations on the abstract universe $\{(+), (-), (\pm)\}$ where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution $-1515 * 17 \rightarrow -(+) * (+) \rightarrow (-) * (+) \rightarrow (-)$, proves that $-1515 * 17$ is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. $-1515 + 17 \rightarrow -(+) + (+) \rightarrow (-) + (+) \rightarrow (\pm))$. Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, \ldots).}
}

@InProceedings{2003:iwpc:cox,
  Title                    = {Syntactic Approximation Using Iterative Lexical Analysis},
  Author                   = {Anthony Cox and Charles Clarke},
  Booktitle                = iwpc,
  Year                     = {2003},
  Pages                    = {154--164},

  Abstract                 = {Syntactic irregularities, which often occur in source-code undergoing maintenance, prevent the application of analysis and comprehension tools that employ traditional parsing techniques. As an alternative to parsing, we have developed an iterative lexical technique that is based on the repetitive application of regular expressions using a shortest-match strategy. The approach recognizes syntactic elements using iterative refinement, where unambiguous constructs are identified to provide contextual cues for the identification of more ambiguous constructs. The use of a shortest-match strategy supports the bottom up construction of a syntax tree by identifying smaller subtrees first. To examine the technique's effectiveness, we present the results of an experiment comparing iterative lexical analysis against parsing. The measures of precision and recall are used to evaluate and compare the two approaches.},
  Doi                      = {10.1109/WPC.2003.1199199}
}

@Article{1958:jrssb:cox,
  Title                    = {The regression analysis of binary sequences},
  Author                   = {D. R. Cox},
  Journal                  = jrssb,
  Year                     = {1958},
  Number                   = {2},
  Pages                    = {215--242},
  Volume                   = {20},

  Abstract                 = {A sequence of 0's and 1's is observed and it is suspected that the chance that a particular trial is a 1 depends on the value of one or more independent variables. Tests and estimates for such situations are considered, dealing first with problems in which the independent variable is preassigned and then with independent variables that are functions of the the sequences. There is a considerable amount of earlier work, which is reviewed.},
  Url                      = {http://www.jstor.org/stable/2983890}
}

@Book{1946:book:cramer,
  Title                    = {Mathematical Models of Statistics},
  Author                   = {Harald Cram{\'e}r},
  Publisher                = {Princeton University Press},
  Year                     = {1946}
}

@InProceedings{2005:hicss:crowston,
  Title                    = {Effective work practices for {FLOSS} development: A model and propositions},
  Author                   = {Kevin Crowston and Hala Annabi and James Howison and Chengetai Masango},
  Booktitle                = hicss,
  Year                     = {2005},
  Pages                    = {197a:1--197a:9},

  Abstract                 = {We review the literature on Free/Libre Open Source Software (FLOSS) development and on software development, distributed work and teams more generally to develop a theoretical model to explain the performance of FLOSS teams. The proposed model is based on Hackman's [1] model of effectiveness of work teams, with coordination theory [2] and collective mind [3] to extend Hackman's model by elaborating team practices relevant to effectiveness in software development. We propose a set of propositions to guide further research.}
}

@InProceedings{2004:csac:crowston,
  Title                    = {Coordination practices within {FLOSS} development teams: The bug fixing process},
  Author                   = {Kevin Crowston and Barbara Scozzi},
  Booktitle                = csac,
  Year                     = {2004},
  Pages                    = {21--30},

  Abstract                 = {Free/Libre Open Source Software (FLOSS) is primarily developed by distributed teams. Developers contribute from around the world and coordinate their activity almost exclusively by means of email and bulletin boards. FLOSS development teams some how profit from the advantages and evade the challenges of distributed software development. Despite the relevance of the FLOSS both for research and practice, few studies have investigated the work practices adopted by these development teams. In this paper we investigate the structure and the coordination practices adopted by development teams during the bug-fixing process, which is considered one of main areas of FLOSS project success. In particular, based on a codification of the messages recorded in the bug tracking system of four projects, we identify the accomplished tasks, the adopted coordination mechanisms, and the role undertaken by both the FLOSS development team and the FLOSS community. We conclude with suggestions for further research.}
}

@Article{2008:csur:crowston,
  Title                    = {Free/Libre open-source software development: What we know and what we do not know},
  Author                   = {Crowston, Kevin and Wei, Kangning and Howison, James and Wiggins, Andrea},
  Journal                  = csur,
  Year                     = {2008},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {7/1--7/35},
  Volume                   = {44},

  Doi                      = {10.1145/2089125.2089127}
}

@InProceedings{2006:hicss:crowston,
  Title                    = {Core and Periphery in Free/Libre and Open Source Software Team Communications},
  Author                   = {Crowston, Kevin and Wei, Kangning and Li, Qing and Howison, James},
  Booktitle                = hicss,
  Year                     = {2006},
  Pages                    = {118a:1--118a:7},

  Abstract                 = {The concept of the core group of developers is important and often discussed in empirical studies of FLOSS projects. This paper examines the question, ``how does one empirically distinguish the core?" Being able to identify the core members of a FLOSS development project is important because many of the processes necessary for successful projects likely involve core members differently than peripheral members, so analyses that mix the two groups will likely yield invalid results. We compare 3 analysis approaches to identify the core: the named list of developers, a Bradford's law analysis that takes as the core the most frequent contributors and a social network analysis of the interaction pattern that identifies the core in a core-and-periphery structure. We apply these measures to the interactions around bug fixing for 116 SourceForge projects. The 3 techniques identify different individuals as core members; examination of which individuals are identified leads to suggestions for refining the measures. All 3 measures though suggest that the core of FLOSS projects is a small fraction of the total number of contributors.}
}

@Article{2004:spe:csallner,
  Title                    = {J{C}rasher: An automatic robustness tester for {J}ava},
  Author                   = {Csallner, Christoph and Smaragdakis, Yannis},
  Journal                  = spe,
  Year                     = {2004},

  Month                    = sep,
  Number                   = {11},
  Pages                    = {1025--1050},
  Volume                   = {34},

  Abstract                 = {JCrasher is an automatic robustness testing tool for Java code. JCrasher examines the type information of a set of Java classes and constructs code fragments that will create instances of different types to test the behavior of public methods under random data. JCrasher attempts to detect bugs by causing the program under test to `crash', that is, to throw an undeclared runtime exception. Although in general the random testing approach has many limitations, it also has the advantage of being completely automatic: o supervision is required except for off-line inspection of the test ases that have caused a crash. Compared to other similar commercial and research tools, JCrasher offers several novelties: it transitively analyzes methods, determines the size of each tested method's parameter-space and selects parameter combinations and therefore test cases at random, taking into account the time allocated for testing; it defines heuristics for determining whether a Java exception should be considered as a program bug or whether the JCrasher supplied inputs have violated the code's preconditions; it includes support for efficiently undoing all the state changes introduced by previous tests; it produces test files for JUnit, a popular Java testing tool; and it can be integrated in the Eclipse IDE.},
  Doi                      = {10.1002/spe.602}
}

@Article{2008:tosem:csallner,
  Title                    = {{DSD-Crasher}: A hybrid analysis tool for bug finding},
  Author                   = {Csallner, Christoph and Smaragdakis, Yannis and Xie, Tao},
  Journal                  = tosem,
  Year                     = {2008},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {8:1--8:36},
  Volume                   = {17},

  Doi                      = {10.1145/1348250.1348254}
}

@InProceedings{2008:ainaw:cuadrado,
  Title                    = {A Case Study on Software Evolution towards Service-Oriented Architecture},
  Author                   = {Cuadrado, F{\'e}lix and Garcia, Boni and Due{\~n}as, Juan C. and Parada, Hugo A.},
  Booktitle                = ainaw,
  Year                     = {2008},
  Pages                    = {1399--1404},

  Abstract                 = {The evolution of any software product over its lifetime is unavoidable, caused both by bugs to be fixed and by new requirements appearing in the later stages of the product's lifecycle. Traditional development and architecture paradigms have proven to be not suited for these continual changes, resulting in large maintenance costs. This has caused the rise of approaches such as Service Oriented Architectures (SOA), based on loosely coupled, interoperable services, aiming to address these issues. This paper describes a case study of the evolution of an existing legacy system towards a more maintainable SOA. The proposed process includes the recovery of the legacy system architecture, as a first step to define the specific evolution plan to be executed and validated. The case study has been applied to a medical imaging system, evolving it into a service model.}
}

@Article{2003:software:cusumano,
  Title                    = {Software development worldwide: The state of the practice},
  Author                   = {Cusumano, M. and MacCormack, A. and Kemerer, C. F. and Crandall, B.},
  Journal                  = software,
  Year                     = {2003},

  Month                    = nov # {--} # dec,
  Number                   = {6},
  Pages                    = {28--34},
  Volume                   = {20},

  Doi                      = {10.1109/MS.2003.1241363}
}

@InProceedings{2010:msr:dambros,
  Title                    = {An Extensive Comparison of Bug Prediction Approaches},
  Author                   = {Marco D'Ambros and Michele Lanza and Romain Robbes},
  Booktitle                = msr,
  Year                     = {2010},
  Pages                    = {31--41},

  Abstract                 = {Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available data set consisting of several software systems, and provide an extensive comparison of the explanative and predictive power of well-known bug prediction approaches, together with novel approaches we devised. Based on the results, we discuss the performance and stability of the approaches with respect to our benchmark and deduce a number of insights on bug prediction models.},
  Doi                      = {10.1109/MSR.2010.5463279}
}

@Book{2001:book:detienne,
  Title                    = {Software Design: Cognitive Aspects},
  Author                   = {D{\'e}tienne, Fran{\c{c}}oise},
  Publisher                = {Springer},
  Year                     = {2001}
}

@InProceedings{2008:oopsla:dagenais,
  Title                    = {Enabling static analysis for partial {J}ava programs},
  Author                   = {Dagenais, Barth\'{e}l\'{e}my and Hendren, Laurie},
  Booktitle                = oopsla,
  Year                     = {2008},
  Pages                    = {313--328},

  Abstract                 = {Software engineering tools often deal with the source code of programs retrieved from the web or source code repositories. Typically, these tools only have access to a subset of a program's source code (one file or a subset of files) which makes it difficult to build a complete and typed intermediate representation (IR). Indeed, for incomplete object-oriented programs, it is not always possible to completely disambiguate the syntactic constructs and to recover the declared type of certain expressions because the declaration of many types and class members are not accessible. We present a framework that performs partial type inference and uses heuristics to recover the declared type of expressions and resolve ambiguities in partial Java programs. Our framework produces a complete and typed IR suitable for further static analysis. We have implemented this framework and used it in an empirical study on four large open source systems which shows that our system recovers most declared types with a low error rate, even when only one class is accessible.},
  Doi                      = {10.1145/1449764.1449790}
}

@InProceedings{2008:fse:dagenais,
  Title                    = {Automatically locating framework extension examples},
  Author                   = {Dagenais, Barth\'{e}l\'{e}my and Ossher, Harold},
  Booktitle                = fse,
  Year                     = {2008},
  Pages                    = {203--213},

  Abstract                 = {Using and extending a framework is a challenging task whose difficulty is exacerbated by the poor documentation that generally comes with the framework. Even in the presence of documentation, developers often desire implementation examples for concrete guidance. We propose an approach that automatically locates implementation examples from a code base given lightweight documentation of a framework. Based on our experience with concern-oriented documentation, we devised an approach that uses the framework documentation as a template and that finds instances of this template in a code base. The concern instances represent self-contained and structured implementation examples: the relationships and the roles of parts composing the examples are uncovered and explained. We implemented our approach in a tool and conducted a study comparing the results of our tool with results provided by Eclipse committers, showing that our approach can locate examples with high precision.}
}

@Article{2011:tosem:dagenais,
  Title                    = {Recommending Adaptive Changes for Framework Evolution},
  Author                   = {Barth{\'e}l{\'e}my Dagenais and Martin Robillard},
  Journal                  = tosem,
  Year                     = {2011},

  Month                    = sep,
  Number                   = {4},
  Pages                    = {19:1--19:35},
  Volume                   = {20},

  Abstract                 = {In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library.},
  Doi                      = {10.1145/2000799.2000805}
}

@InProceedings{2010:fse:dagenais,
  Title                    = {Creating and evolving developer documentation: Understanding the decisions of open source contributors},
  Author                   = {Barth{\'e}l{\'e}my Dagenais and Martin Robillard},
  Booktitle                = fse,
  Year                     = {2010},
  Pages                    = {127--136},

  Abstract                 = {Developer documentation helps developers learn frameworks and libraries. To better understand how documentation in open source projects is created and maintained, we performed a qualitative study in which we interviewed core contributors who wrote developer documentation and developers who read documentation. In addition, we studied the evolution of 19 documents by analyzing more than 1500 document revisions. We identified the decisions that contributors make, the factors influencing these decisions and the consequences for the project. Among many findings, we observed how working on the documentation could improve the code quality and how constant interaction with the projects' community positively impacted the documentation.},
  Doi                      = {10.1145/1882291.1882312}
}

@InProceedings{2008:icse:dagenais,
  Title                    = {Recommending adaptive changes for framework evolution},
  Author                   = {Dagenais, Barth{\'e}l{\'e}my and Robillard, Martin P.},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {481--490},

  Abstract                 = {In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library.},
  Comment                  = {Revised in 2011:tosem:dagenais}
}

@Article{1966:cacm:dahl,
  Title                    = {{SIMULA}: An {ALGOL}-based simulation language},
  Author                   = {Ole-Johan Dahl and Kristen Nygaard},
  Journal                  = cacm,
  Year                     = {1966},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {671--678},
  Volume                   = {9},

  Abstract                 = {This paper is an introduction to SIMULA, a programming language designed to provide a systems analyst with unified concepts which facilitate the concise description of discrete event systems. A system description also serves as a source language simulation program. SIMULA is an extension of ALGOL 60 in which the most important new concept is that of quasi-parallel processing.},
  Doi                      = {10.1145/365813.365819}
}

@Misc{2012:book:dallal,
  Title                    = {The Little Handbook of Statistical Practice},

  Author                   = {Gerald E. Dallal},
  HowPublished             = {http://www.jerrydallal.com/LHSP/LHSP.htm},
  Year                     = {2012},

  Url                      = {http://www.jerrydallal.com/LHSP/LHSP.htm}
}

@Article{2005:tse:darcy,
  Title                    = {The Structural Complexity of Software: An Experimental Test},
  Author                   = {David P. Darcy and Chris F. Kemerer and Sandra Slaughter and James E. Tomayko},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {982--995},
  Volume                   = {31},

  Abstract                 = {This research examines the structural complexity of software and, specifically, the potential interaction of the two dominant dimensions of structural complexity, coupling and cohesion. Analysis based on an information processing view of developer cognition results in a theoretically driven model with cohesion as a moderator for a main effect of coupling on effort. An empirical test of the model was devised in a software maintenance context utilizing both procedural and object-oriented tasks, with professional software engineers as participants. The results support the model in that there was a significant interaction effect between coupling and cohesion on effort, even though there was no main effect for either coupling or cohesion. The implication of this result is that, when designing, implementing, and maintaining software to control complexity, both coupling and cohesion should be considered jointly, instead of independently. By providing guidance on structuring software for software professionals and researchers, these results enable software to continue as the solution of choice for a wider range of richer, more complex problems.},
  Doi                      = {10.1109/TSE.2005.130}
}

@InProceedings{2006:icml:davis,
  Title                    = {The relationship between precision--recall and {ROC} curves},
  Author                   = {Davis, Jesse and Goadrich, Mark},
  Booktitle                = icml,
  Year                     = {2006},
  Pages                    = {233--240},

  Abstract                 = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
  Doi                      = {10.1145/1143844.1143874}
}

@Article{1995:tse:dean,
  Title                    = {A Syntactic Theory of Software Architecture},
  Author                   = {Dean, Thomas R. and Cordy, James R.},
  Journal                  = tse,
  Year                     = {1995},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {302--313},
  Volume                   = {21},

  Abstract                 = {In this paper we introduce a general, extensible diagrammatic syntax for expressing software architectures based on typed nodes and connections and formalized using set theory. The syntax provides a notion of abstraction corresponding to the concept of a subsystem, and exploits this notion in a general mechanism for pattern matching over architectures. We demonstrate these ideas using a small example architecture language with a limited number of types of nodes and connectors, and a small taxonomy of architectures characterized as sets of patterns in the language.},
  Doi                      = {10.1109/32.385969}
}

@InProceedings{2009:icpc:dekel,
  Title                    = {Reading the documentation of invoked {API} functions in program comprehension},
  Author                   = {Dekel, Uri and Herbsleb, James D.},
  Booktitle                = icpc,
  Year                     = {2009},
  Pages                    = {168--177},

  Abstract                 = {Comprehending an unfamiliar code fragment requires an awareness of explicit usage directives that may be present in the documentation of some invoked functions. Since it is not practical for developers to thoroughly investigate every call, directives may be missed and errors may occur. We previously reported on a tool called eMoose, which highlights calls to methods with associated directives, and on a controlled comparative lab study in which eMoose users were more successful at fixing bugs in given code fragments. In this paper we attempt to shed light on the factors behind these differences with a detailed analysis of videos from the study. We argue that information foraging theory may explain the subjects' reading choices and the impact of our tool. We also suggest ways to structure documentation to increase the prospects of knowledge acquisition.}
}

@Article{2009:jsmerp:rosso,
  Title                    = {Comprehend and analyze knowledge networks to improve software evolution},
  Author                   = {Del Rosso, Christian},
  Journal                  = jsmerp,
  Year                     = {2009},

  Month                    = may # {/} # jun,
  Number                   = {3},
  Pages                    = {189--215},
  Volume                   = {21},

  Abstract                 = {When a set of people are connected by a set of meaningful social relationships we talk of a social network. A social network represents a social structure and the underlying structural patterns can be used to analyze and comprehend how people relate to each other and their emergent behavior as a group. Developing software is fundamentally a human activity. Developers cooperate and exchange knowledge and information, creating in fact, a particular type of social network that we call knowledge network. In this paper we investigate knowledge networks in software development teams by applying social network analysis and we use the Apache web server as a case study. By analyzing the structural communication and coordination patterns in Apache we have been able to identify the Apache knowledge network, highlight potential communication bottlenecks, and find brokers and important coordination points in the software development team. Furthermore, our work enables a software architect to analyze and maintain the organization and the software architecture aligned during software evolution. An important lesson that we have is that the analysis of knowledge networks constitutes an additional tool to be added to the traditional software architecture assessment methods.},
  Doi                      = {10.1002/smr.408}
}

@InProceedings{2012:icse:deline,
  Title                    = {{Debugger Canvas}: Industrial experience with the {Code Bubbles} paradigm},
  Author                   = {DeLine, Robert and Bragdon, Andrew and Rowan, Kael and Jacobsen, Jens and Reiss, Steven P.},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {1064--1073},

  Abstract                 = {At ICSE 2010, the Code Bubbles team from Brown University and the Code Canvas team from Microsoft Research presented similar ideas for new user experiences for an integrated development environment. Since then, the two teams formed a collaboration, along with the Microsoft Visual Studio team, to release Debugger Canvas, an industrial version of the Code Bubbles paradigm. With Debugger Canvas, a programmer debugs her code as a collection of code bubbles, annotated with call paths and variable values, on a two-dimensional pan-and-zoom surface. In this experience report, we describe new user interface ideas, describe the rationale behind our design choices, evaluate the performance overhead of the new design, and provide user feedback based on lab participants, post-release usage data, and a user survey and interviews. We conclude that the code bubbles paradigm does scale to existing customer code bases, is best implemented as a mode in the existing user experience rather than a replacement, and is most useful when the user has a long or complex call paths, a large or unfamiliar code base, or complex control patterns, like factories or dynamic linking.},
  Url                      = {http://dl.acm.org/citation.cfm?id=2337223.2337362}
}

@InProceedings{2010:icse:deline,
  Title                    = {{Code Canvas}: Zooming towards better development environments},
  Author                   = {DeLine, Robert and Rowan, Kael},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {207--210},
  Volume                   = {2},

  Abstract                 = {The user interfaces of today's development environments have a ``bento box" design that partitions information into separate areas. This design makes it difficult to stay oriented in the open documents and to synthesize information shown in different areas. Code Canvas takes a new approach by providing an infinite zoomable surface for software development. A canvas both houses editable forms of all of a project's documents and allows multiple layers of visualization over those documents. By uniting the content of a project and information about it onto a single surface, Code Canvas is designed to leverage spatial memory to keep developers oriented and to make it easy to synthesize information.},
  Doi                      = {10.1145/1810295.1810331}
}

@Article{2010:cacm:deline,
  Title                    = {Software development with {Code Maps}},
  Author                   = {DeLine, Robert and Venolia, Gina and Rowan, Kael},
  Journal                  = cacm,
  Year                     = {2010},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {48--54},
  Volume                   = {53},

  Abstract                 = {Could ubiquitous hand-drawn code map diagrams become a thing of the past?},
  Doi                      = {10.1145/1787234.1787250}
}

@Article{2010:queue:deline,
  Title                    = {Software Development with {Code Maps}},
  Author                   = {DeLine, Robert and Venolia, Gina and Rowan, Kael},
  Journal                  = queue,
  Year                     = {2010},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {10:1--10:9},
  Volume                   = {8},

  Abstract                 = {Could those ubiquitous hand-drawn code diagrams become a thing of the past?},
  Doi                      = {10.1145/1831327.1831329}
}

@InProceedings{2000:oopsla:demeyer,
  Title                    = {Finding refactorings via change metrics},
  Author                   = {Demeyer, Serge and Ducasse, St{\'e}phane and Nierstrasz, Oscar},
  Booktitle                = oopsla,
  Year                     = {2000},
  Pages                    = {166--177},

  Abstract                 = {Reverse engineering is the process of uncovering the design and the design rationale from a functioning software system. Reverse engineering is an integral part of any successful software system, because changing requirements lead to implementations that drift from their original design. In contrast to traditional reverse engineering techniques---which analyse a single snapshot of a system---we focus the reverse engineering effort by determining where the implementation has changed. Since changes of object-oriented software are often phrased in terms of refactorings, we propose a set of heuristics for detecting refactorings by applying lightweight, object-oriented metrics to successive versions of a software system. We validate our approach with three separate case studies of mature object-oriented software systems for which multiple versions are available. The case studies suggest that the heuristics support the reverse engineering process by focusing attention on the relevant parts of a software system.}
}

@InProceedings{2010:seaa:dersten,
  Title                    = {Analysis of the Business Effects of Software Architecture Refactoring in an Automotive Development Organization},
  Author                   = {Dersten, Sara and Froberg, Joakim and Axelsson, Jakob and Land, Rikard},
  Booktitle                = seaa,
  Pages                    = {269--278},

  Abstract                 = {This paper presents an exploratory study of an automotive manufacturer, which develops embedded software for over 150 products and has adopted a company-wide software product-line approach. The company is facing the introduction of a new software architecture in all products in near time. This architecture introduces new paradigms more explicitly, such as explicit software components and signal-based communication, newer technologies, and adheres to new standards. Concretely, the architecture consists of common infrastructure and other generic components. Such a fundamental and drastic technology change can be expected to have far-reaching consequences, both of technical and non-technical nature. In this study we systematically investigate the introduction of the new software architecture, by mapping individual elements of the architectural change to system properties and company functions. The study implies that the whole organization is affected, and the new architecture also influences the cooperation with suppliers.},
  Doi                      = {10.1109/SEAA.2010.13}
}

@InProceedings{2004:wicsa:vandeursen,
  Title                    = {Symphony: View-driven software architecture reconstruction},
  Author                   = {van Deursen, Arie and Hofmeister, Christine and Koschke, Rainer and Moonen, Leon and Riva, Claudio},
  Booktitle                = wicsa,
  Year                     = {2004},
  Pages                    = {122--132},

  Abstract                 = {Authentic descriptions of a software architecture are required as a reliable foundation for any but trivial changes to a system. Far too often, architecture descriptions of existing systems are out of sync with the implementation. If they are, they must be reconstructed. There are many existing techniques for reconstructing individual architecture views, but no information about how to select views for reconstruction, or about process aspects of architecture reconstruction in general. In this paper we describe view-driven process for reconstructing software architecture that fills this gap. To describe Symphony, we present and compare different case studies, thus serving a secondary goal of sharing real-life reconstruction experience. The Symphony process incorporates the state of the practice, where reconstruction is problem-driven and uses a rich set of architecture views. Symphony provides a common framework for reporting reconstruction experiences and for comparing reconstruction approaches. Finally, it is a vehicle for exposing and demarcating research problems in software architecture reconstruction.},
  Doi                      = {10.1109/WICSA.2004.1310696}
}

@Article{1999:tosem:devanbu,
  Title                    = {{GENOA}: A Customizable, Front-end-Retargetable Source Code
 Analysis Framework},
  Author                   = {Premkumar T. Devanbu},
  Journal                  = tosem,
  Year                     = {1999},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {177--212},
  Volume                   = {8},

  Doi                      = {10.1145/304399.304402}
}

@InProceedings{2011:icsm:dhaliwal,
  Title                    = {Classifying field crash reports for fixing bugs: A case study of {M}ozilla {F}irefox},
  Author                   = {Tejinder Dhaliwal and Foutse Khomh and Ying Zou},
  Booktitle                = icsm,
  Year                     = {2011},
  Pages                    = {333--342},

  Abstract                 = {Many software systems support automatic collection of field crash-reports which record the stack traces and other runtime information when crashes occur. Analysis of field crash-reports can help developers to locate and fix bugs. However, the amount of crash-reports collected is often too large to handle. To reduce the amount of data for the analysis, the existing approaches group similar crash-reports together. A bug can trigger a crash in different usage scenarios. Therefore, the crash-reports triggered by the same bug may not be identical. Using the existing approaches, the crash-reports triggered by the same bugs can be distributed into different groups and one group may contain crash-reports triggered by different bugs. We perform an empirical study of crash-reports collected for Mozilla Firefox to analyze the impact of crash-report grouping and identify the characteristics of an efficient grouping. We observe that when a group contains crash-reports triggered by multiple bugs, it takes longer time to fix the bugs in comparison to the bugs where crash-reports triggered by each bug are grouped separately. To effectively reduce the bug fixing time, we propose a grouping approach, such that, each group contains the crash-reports triggered by only one bug. The case study shows that an effective grouping can reduce the bug fix time by more than 5\%.},
  Doi                      = {10.1109/ICSM.2011.6080800}
}

@Article{1995:jss:dhama,
  Title                    = {Quantitative models of cohesion and coupling in software},
  Author                   = {Harpal Dhama},
  Journal                  = jss,
  Year                     = {1995},

  Month                    = apr,
  Number                   = {1},
  Pages                    = {65--74},
  Volume                   = {29},

  Abstract                 = {Our project goal is to specify, implement, and verify quantitative models for measuring cohesion and coupling (C \& C) in software modules. This article is our project interim report on the specification of the C \& C quantitative models and preliminary verification effort. To quantify cohesion, we subdivided it into four categories and then quantified each category. Coupling is subdivided into four categories, followed by the quantification of each category. Although the C \& C concepts are applicable to any procedural language such as FORTRAN, PASCAL, or Ada, we chose to apply the C \& C formulas to Ada programs. We have hand-calculated C \& C values for a number of programs, but here we report and discuss in detail only a typical result of our calculations obtained by applying the C \& C formulas to two different implementations of an algorithm. We have found that the formulas are sensitive enough to distinguish between the two implementations, and the obtained quantitative values agree with the qualitative assessment of the implementations.},
  Doi                      = {10.1016/0164-1212(94)00128-A}
}

@InProceedings{1992:popl:di_cosmo,
  Title                    = {Type isomorphisms in a type-assignment framework: From library searches using types to the completion of the {ML} type checker},
  Author                   = {Di Cosmo, Roberto},
  Booktitle                = popl,
  Year                     = {1992},
  Pages                    = {200--210},

  Abstract                 = {This paper contains a full treatment of isomorphic types for languages equipped with an ML style polymorphic type inference mechanism. Surprisingly enough, the results obtained contradict the commonplace feeling that (the core of) ML is a subset of second order $\lambda$-calculus: we can provide an isomorphism of types that holds in the core ML language, but not in second order $\lambda$-calculus. This new isomorphism not only allows to provide a complete (and decidable) axiomatisation of all the types isomorphic in ML style languages, a relevant issue for the type as specifications paradigm in library searches, but also suggest a natural extension that in a sense completes the type inference mechanism in ML. This extension is easy to implement and allows to get a further insight in the nature of the \emph{let} polymorphic construct.},
  Doi                      = {10.1145/143165.143208}
}

@InProceedings{2012:acsc:dietrich,
  Title                    = {On the Existence of High-Impact Refactoring Opportunities in Programs},
  Author                   = {Dietrich, Jens and McCartin, Catherine and Tempero, Ewan and Shah, Syed M. Ali},
  Booktitle                = acsc,
  Year                     = {2012},
  Pages                    = {37--48},

  Abstract                 = {The refactoring of large systems is difficult, with the possibility of many refactorings having to be done before any useful benefit is attained. We present a novel approach to detect starting points for the architectural refactoring of large and complex systems based on the analysis and manipulation of the type dependency graph extracted from programs. The proposed algorithm is based on the simultaneous analysis of multiple architectural antipatterns, and outputs dependencies between artefacts that participate in large numbers of instances of these antipatterns. If these dependencies can be removed, they represent high-impact refactoring opportunities: a small number of changes that have a major impact on the overall quality of the system, measured by counting architectural antipattern instances. The proposed algorithm is validated using an experiment where we analyse a set of 95 open-source Java programs for instances of four architectural patterns representing modularisation problems. We discuss some examples demonstrating how the computed dependencies can be removed from programs. This research is motivated by the emergence of technologies such as dependency injection frameworks and dynamic component models. These technologies try to improve the maintainability of systems by removing dependencies between system parts from program source code and managing them explicitly in configuration files.},
  Url                      = {http://crpit.com/confpapers/CRPITV122Dietrich.pdf}
}

@InProceedings{2010:qosa:dietrich,
  Title                    = {Barriers to modularity: An empirical study to assess the potential for modularisation of {J}ava programs},
  Author                   = {Dietrich, Jens and McCartin, Catherine and Tempero, Ewan and Shah, Syed M. Ali},
  Booktitle                = qosa,
  Year                     = {2010},
  Pages                    = {135--150},
  Series                   = lncs,
  Volume                   = {6093},

  Abstract                 = {To deal with the challenges when building large and complex systems modularisation techniques such as component-based software engineering and aspect-oriented programming have been developed. In the Java space these include dependency injection frameworks and dynamic component models such as OSGi. The question arises as to how easy it will be to transform existing systems to take advantage of these new techniques. Anecdotal evidence from industry suggests that the presence of certain patterns presents barriers to refactoring of monolithic systems into a modular architecture. In this paper, we present such a set of patterns and analyse a large set of open-source systems for occurrences of these patterns. We use a novel, scalable static analyser that we have developed for this purpose. The key findings of this paper are that almost all programs investigated have a significant number of these patterns, implying that modularising will be therefore difficult and expensive.},
  Doi                      = {10.1007/978-3-642-13821-8_11}
}

@InProceedings{2006:ecoop:dig,
  Title                    = {Automated Detection of Refactorings in Evolving Components},
  Author                   = {Danny Dig and Can Comertoglu and Darko Marinov and Ralph Johnson},
  Booktitle                = ecoop,
  Year                     = {2006},
  Pages                    = {404--428},
  Series                   = lncs,
  Volume                   = {4067},

  Abstract                 = {One of the costs of reusing software components is updating applications to use the new version of the components. Updating an application can be error-prone, tedious, and disruptive of the development process. Our previous study showed that more than 80\% of the disruptive changes in five different components were caused by refactorings. If the refactorings that happened between two versions of a component could be automatically detected, a refactoring tool could replay them on applications. We present an algorithm that detects refactorings performed during component evolution. Our algorithm uses a combination of a fast syntactic analysis to detect refactoring candidates and a more expensive semantic analysis to refine the results. The experiments on components ranging from 17 KLOC to 352 KLOC show that our algorithm detects refactorings in real-world components with accuracy over 85\%.}
}

@Article{2006:jsmerp:dig,
  Title                    = {How do {API}s evolve? {A} story of refactoring},
  Author                   = {Danny Dig and Ralph Johnson},
  Journal                  = jsmerp,
  Year                     = {2006},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {83--107},
  Volume                   = {18},

  Abstract                 = {Frameworks and libraries change their APIs. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and ideas have been proposed to solve the evolution of APIs, most updates are done manually. To better understand the requirements for migration tools, we studied the API changes of four frameworks and one library. We discovered that the changes that break existing applications are not random, but tend to fall into particular categories. Over 80\% of these changes are refactorings. This suggests that refactoring-based migration tools should be used to update applications.},
  Doi                      = {10.1002/smr.v18:2}
}

@InProceedings{2005:icsm:dig,
  Title                    = {The role of refactorings in {API} evolution},
  Author                   = {Dig, Danny and Johnson, Ralph},
  Booktitle                = icsm,
  Year                     = {2005},
  Pages                    = {389-- 398},

  Abstract                 = {Frameworks and libraries change their APIs. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and ideas have been proposed to solve the evolution of APIs, most updates are done manually. To better understand the requirements for migration tools we studied the API changes of three frameworks and one library. We discovered that the changes that break existing applications are not random, but they tend to fall into particular categories. Over 80\% of these changes are refactorings. This suggests that refactoring-based migration tools should be used to update applications.}
}

@Article{2008:tse:dig,
  Title                    = {Effective Software Merging in the Presence of Object-Oriented Refactorings},
  Author                   = {Dig, Danny and Manzoor, Kashif and Johnson, Ralph E. and Nguyen, Tien N.},
  Journal                  = tse,
  Year                     = {2008},

  Month                    = may,
  Number                   = {3},
  Pages                    = {321--335},
  Volume                   = {34},

  Abstract                 = {Current text based Software Configuration Management (SCM) systems have trouble with refactorings. Refactorings result in global changes which lead to merge conflicts. A refactoring-aware SCM system reduces merge conflicts. This paper describes MolhadoRef, a refactoring-aware SCM system and the merge algorithm at its core. MolhadoRef records change operations (refactorings and edits) used to produce one version, and replays them when merging versions. Since refactorings are change operations with well defined semantics, MolhadoRef treats them intelligently. A case study and a controlled experiment show that MolhadoRef automatically solves more merge conflicts than CVS while resulting in fewer merge errors.},
  Doi                      = {10.1109/TSE.2008.29}
}

@Article{2006:sen:dig,
  Title                    = {Refactoring-aware software merging and configuration management},
  Author                   = {Dig, Danny and Manzoor, Kashif and Nguyen, Tien N. and Johnson, Ralph},
  Journal                  = sen,
  Year                     = {2006},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {16:1--16:2},
  Volume                   = {31},

  Abstract                 = {Refactoring tools allow programmers to change their source code quicker than before. However, the complexity of these changes cause versioning tools that operate at a file level to lose the history of entities and be unable to merge refactored entities. This problem can be solved by semantic, operation-based SCM with persistent IDs. MolhadoRef, our proto-type, can successfully merge edit and refactoring operations which were performed on different development branches, preserves program history better and makes it easier to understand program evolution.},
  Doi                      = {10.1145/1218776.1218797}
}

@InProceedings{2008:icse:dig,
  Title                    = {{ReBA}: Refactoring-aware binary adaptation of evolving libraries},
  Author                   = {Dig, Danny and Negara, Stas and Mohindra, Vibhu and Johnson, Ralph},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {441--450},

  Abstract                 = {Although in theory the APIs of software libraries and frameworks should be stable, they change in practice. This forces clients of the library API to change as well, making software maintenance expensive. Changing a client might not even be an option if its source code is missing or certain policies forbid its change. By giving a library both the old and the new API, clients can be shielded from API changes and can run with the new version of the library. This paper presents our solution and a tool, ReBA, that automatically generates compatibility layers between new library APIs and old clients. In the first stage, ReBA generates another version of the library, called adapted-library, that supports both the old and the new APIs. In the second stage, ReBA shrinks the adapted-library into a minimal, client-specific compatibility layer containing only classes truly required by the client. Evaluations on controlled experiments and case studies using Eclipse core libraries shows that our approach effectively adapts clients to new library versions, and is efficient.},
  Doi                      = {10.1145/1368088.1368148}
}

@InCollection{1970:nato:dijkstra,
  Title                    = {Structured programming},
  Author                   = {E. W. Dijkstra},
  Booktitle                = {Software Engineering Techniques},
  Publisher                = {NATO Scientific Affairs Division},
  Year                     = {1970},
  Editor                   = {J. N. Buxton and B. Randell},
  Pages                    = {84--87},

  Abstract                 = {This working document reports on experience and insights gained in programming experiments performed by the author in the last year. The leading question was if it was conceivable to increase our programming ability by an order of magnitude and what techniques (mental, organizational or mechanical) could be applied in the process of program composition to produce this increase. The programming experiments were undertaken to shed light upon these questions.},
  Url                      = {http://homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1969.PDF}
}

@InCollection{1972:book:dijkstra:dijkstra,
  Title                    = {Notes on structured programming},
  Author                   = {Edsger W. Dijkstra},
  Booktitle                = {Structured Programming},
  Publisher                = {Academic Press Ltd.},
  Year                     = {1972},
  Chapter                  = {1},
  Pages                    = {1--82},

  Abstract                 = {I am faced with a basic problem of presentation. What I am really concerned about is the composition of large programs, the text of which may be, say, of the same size as the whole text of this chapter. Also I have to include examples to illustrate the various techniques. For practical reasons, the demonstration programs must be small, many times smaller than the ``life-size programs'' I have in mind. My basic problem is that precisely this difference in scale is one of the major sources of our difficulties in programming! It would be very nice if I could illustrate the various techniques with small demonstration programs and could conclude with ``... and when faced with a program a thousand times as large, you compose it in the same way." This common educational device, however, would be self-defeating as one of my central themes will be that any two things that differ in some respect by a factor of already a hundred or more, are utterly incomparable. History has shown that this truth is very hard to believe. Apparently we are too much trained to disregard differences in scale, to treat them as ``gradual differences that are not essential". We tell ourselves that what we can do once, we can also do twice and by induction we fool ourselves into believing that we can do it as many times as needed, but this is just not true! A factor of a thousand is already far beyond our powers of imagination! Let me give you two examples to rub this in. A one-year old child will crawl on all fours with a speed of, say, one mile per hour. But a speed of a thousand miles per hour is that of a supersonic jet. Considered as objects with moving ability the child and the jet are incomparable, for whatever one can do the other cannot and vice versa. Also: one can close one's eyes and imagine how it feels to be standing in an open place, a prairie or a sea shore, while far away a big, reinless horse is approaching at a gallop, one can ``see" it approaching and passing. To do the same with a phalanx of a thousand of these big beasts is mentally impossible: your heart would miss a number of beats by pure panic, if you could! To complicate matters still further, problems of size do not only cause me problems of presentation, but they lie at the heart of the subject: widespread underestimation of the specific difficulties of size seems one of the major underlying causes of the current software failure. To all this I can see only one answer, viz. to treat problems of size as explicitly as possible. Hence the title of this section. To start with, we have the ``size" of the computation, i.e. the amount of information and the number of operations involved in it. It is essential that this size is large, for if it were really small, it would be easier not to use the computer at all and to do it by hand. The automatic computer owes its right to exist, its usefulness, precisely to its ability to perform large computations where we humans cannot. We want the computer to do what we could never do ourselves and the power of present-day machinery is such that even small computations are by their very size already far beyond the powers of our unaided imagination. Yet we must organise the computations in such a way that our limited powers are sufficient to guarantee that the computation will establish the desired effect. This organising includes the composition of the program and here we are faced with the next problem of size, viz. the length of the program text, and we should give this problem also explicit recognition. We should remain aware of the fact that the extent to which we can read or write a text is very much dependent on its size. In my country the entries in the telephone directory are grouped by town or village and within each such group the subscribers are listed by name in alphabetical order. I myself live in a small village and given a telephone number I have only to scan a few columns to find out to whom the telephone number belongs, but to do the same in a large city would be a major data processing task! It is in the same mood that I should like to draw the reader's attention to the fact that ``clarity" has pronounced quantitative aspects, a fact many mathematicians, curiously enough, seem to be unaware of. A theorem stating the validity of a conclusion when ten pages full of conditions are satisfied is hardly a convenient tool, as all conditions have to be verified whenever the theorem is appealed to. In Euclidean geometry, Pythagoras' Theorem holds for any three points $A$, $B$ and $C$ such that through $A$ and $C$ a straight line can be drawn orthogonal to a straight line through $B$ and $C$. How many mathematicians appreciate that the theorem remains applicable when some or all of the points $A$, $B$ and $C$ coincide? Yet this seems largely responsible for the convenience with which Pythagoras' Theorem can be used. Summarizing: as a slow-witted human being I have a very small head and I had better learn to live with it and to respect my limitations and give them full credit, rather than to try to ignore them, for the latter vain effort will be punished by failure.},
  Url                      = {http://www.cs.utexas.edu/~EWD/ewd02xx/EWD249.PDF}
}

@InProceedings{2008:icpc:dong,
  Title                    = {Identifying Architectural Change Patterns in Object-Oriented Systems},
  Author                   = {Xinyi Dong and Michael W. Godfrey},
  Booktitle                = icpc,
  Year                     = {2008},
  Pages                    = {33--42},

  Abstract                 = {As an object-oriented system evolves, its architecture tends to drift away from the original design. Knowledge of how the system has changed at coarse-grained levels is key to understanding the de facto architecture, as it helps to identify potential architectural decay and can provide guidance for further maintenance activities. However, current studies of object-oriented software changes are mostly targeted at the class or method level. In this paper, we propose a new approach to modeling object-oriented software changes at coarse-grained levels. We take snapshots of an object-oriented system, represent each version of the system as a hybrid model, and detect software changes at coarse-grained level by comparing two hybrid models. Based on this approach, we further identify a collection of change patterns, which help interpret how system changes at the architecture level. Finally, we present an exploratory case study to show how our approach can help maintainers capture and better comprehend architectural evolution of object-oriented software systems.}
}

@InProceedings{2011:icsm:dragan:b,
  Title                    = {Emergent laws of method and class stereotypes in object oriented software},
  Author                   = {Dragan, Natalia},
  Booktitle                = icsm,
  Year                     = {2011},
  Pages                    = {550--555},

  Abstract                 = {The paper investigates concepts of method and class stereotypes, focusing on understanding OO design abstraction at a lower level than design patterns. Stereotypes are powerful semantic mechanisms and represent generalizations that reflect an intrinsic or atomic behavior of a method or a class. First, we present a mechanism to automatically reverse engineer these stereotypes from existing systems along with a means to re-document methods and classes with their corresponding stereotypes. Second, based on the distribution of method stereotypes we propose techniques for automatic identification of class stereotypes, systems classification, and the characterization of changes during software evolution. We anticipate that these new techniques will better support program comprehension and design recovery, and could be used to build smarter reverse engineering tools. The results of this paper are based on the author's doctoral dissertation.}
}

@InProceedings{2011:icsm:dragan:a,
  Title                    = {Using stereotypes to help characterize commits},
  Author                   = {Dragan, Natalia and Collard, Michael L. and Hammad, Maen and Maletic, Jonathan I.},
  Booktitle                = icsm,
  Year                     = {2011},
  Pages                    = {520--523},

  Abstract                 = {Individual commits to a version control system are automatically characterized based on the stereotypes of added and deleted methods. The stereotype of each method is automatically reverse engineered using a previously defined taxonomy. Method stereotypes reflect intrinsic atomic behavior of a method and its role in the class. The stereotypes of the added and deleted methods form a descriptors are then used to categorize commits, into types, based on the impact of the changes to a class (or classes). The goal is to gain a better understanding of the design changes to a system over its history and provide a means for documenting the commit.}
}

@Article{1996:computer:drake,
  Title                    = {Measuring software quality: A case study},
  Author                   = {Thomas Drake},
  Journal                  = computer,
  Year                     = {1996},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {78--87},
  Volume                   = {29},

  Abstract                 = {The National Security Agency's (NSA) mission is to provide support for the security of the United States. Over the years, the Agency has become extremely dependent on the software that makes up its information technology infrastructure. NSA has come to view software as a critical resource upon which much of the world's security, prosperity, and economic competitiveness increasingly rests. To ensure cost effective delivery of high quality software, NSA has analyzed effective quality measures applied to a sample code base of 25 million lines. This case study dramatically illustrates the benefits of code level measurement activities.},
  Doi                      = {v}
}

@InProceedings{2011:ecoop:duala-ekoko,
  Title                    = {Using structure-based recommendations to facilitate discoverability in {API}s},
  Author                   = {Duala-Ekoko, Ekwa and Robillard, Martin P.},
  Booktitle                = ecoop,
  Year                     = {2011},
  Pages                    = {79--104},

  Abstract                 = {Empirical evidence indicates that developers face significant hurdles when the API elements necessary to implement a task are not accessible from the types they are working with. We propose an approach that leverages the structural relationships between API elements to make API methods or types not accessible from a given API type more discoverable. We implemented our approach as an extension to the content assist feature of the Eclipse IDE, in a tool called API Explorer. API Explorer facilitates discoverability in APIs by recommending methods or types, which although not directly reachable from the type a developer is currently working with, may be relevant to solving a programming task. In a case study evaluation, participants experienced little difficulty selecting relevant API elements from the recommendations made by API Explorer, and found the assistance provided by API Explorer helpful in surmounting discoverability hurdles in multiple tasks and various contexts. The results provide evidence that relevant API elements not accessible from the type a developer is working with could be efficiently located through guidance based on structural relationships.}
}

@Article{2010:tosem:duala-ekoko,
  Title                    = {Clone region descriptors: Representing and tracking duplication in source code},
  Author                   = {Duala-Ekoko, Ekwa and Robillard, Martin P.},
  Journal                  = tosem,
  Year                     = {2010},

  Month                    = jun,
  Number                   = {1},
  Pages                    = {3:1--3:31},
  Volume                   = {20},

  Abstract                 = {Source code duplication, commonly known as code cloning, is considered an obstacle to software maintenance because changes to a cloned region often require consistent changes to other regions of the source code. Research has provided evidence that the elimination of clones may not always be practical, feasible, or cost-effective. We present a clone management approach that describes clone regions in a robust way that is independent from the exact text of clone regions or their location in a file, and that provides support for tracking clones in evolving software. Our technique relies on the concept of abstract clone region descriptors (CRDs), which describe clone regions using a combination of their syntactic, structural, and lexical information. We present our definition of CRDs, and describe a clone tracking system capable of producing CRDs from the output of different clone detection tools, notifying developers of modifications to clone regions, and supporting updates to the documented clone relationships. We evaluated the performance and usefulness of our approach across three clone detection tools and five subject systems, and the results indicate that CRDs are a practical and robust representation for tracking code clones in evolving software.},
  Doi                      = {10.1145/1767751.1767754}
}

@Article{2006:jsmerp:ducasse,
  Title                    = {On the effectiveness of clone detection by string matching},
  Author                   = {Ducasse, St{\'e}phane and Nierstrasz, Oscar and Rieger, Matthias},
  Journal                  = jsmerp,
  Year                     = {2006},

  Month                    = jan # {/} # feb,
  Number                   = {1},
  Pages                    = {37--58},
  Volume                   = {18},

  Abstract                 = {Although duplicated code is known to pose severe problems for software maintenance, it is difficult to identify in large systems. Many different techniques have been developed to detect software clones, some of which are very sophisticated, but are also expensive to implement and adapt. Lightweight techniques based on simple string matching are easy to implement, but how effective are they? We present a simple string-based approach which we have successfully applied to a number of different languages such COBOL, Java, C++, Pascal, Python, Smalltalk, C and PDP-11 Assembler. In each case the maximum time to adapt the approach to a new language was less than 45 minutes. In this paper we investigate a number of simple variants of string-based clone detection that normalize differences due to common editing operations, and assess the quality of clone detection for very different case studies. Our results confirm that this inexpensive clone detection technique generally achieves high recall and acceptable precision. Over-zealous normalization of the code before comparison, however, can result in an unacceptable numbers of false positives.},
  Doi                      = {10.1002/smr.317}
}

@InProceedings{1999:icsm:ducasse,
  Title                    = {A Language Independent Approach for Detecting Duplicated Code},
  Author                   = {Ducasse, St{\'e}phane and Rieger, Matthias and Demeyer, Serge},
  Booktitle                = icsm,
  Year                     = {1999},
  Pages                    = {109--118},

  Abstract                 = {Code duplication is one of the factors that severely complicates the maintenance and evolution of large software systems. Techniques for detecting duplicated code exist but rely mostly on parsers, technology that has proven to be brittle in the face of different languages and dialects. In this paper we show that is possible to circumvent this hindrance by applying a language independent and visual approach, i.e. a tool that requires no parsing, yet is able to detect a significant amount of code duplication. We validate our approach on a number of case studies, involving four different implementation languages and ranging from 256 K up to 13Mb of source code size.}
}

@InProceedings{2004:oopsla:dufour,
  Title                    = {Measuring the dynamic behaviour of {AspectJ} programs},
  Author                   = {Dufour, Bruno and Goard, Christopher and Hendren, Laurie and De Moor, Oege and Sittampalam, Ganesh and Verbrugge, Clark},
  Booktitle                = oopsla,
  Year                     = {2004},
  Pages                    = {150--169},

  Doi                      = {10.1145/1028976.1028990}
}

@InProceedings{1995:ssr:dusink,
  Title                    = {Reuse dimensions},
  Author                   = {Dusink, Liesbeth and van Katwijk, Jan},
  Booktitle                = ssr,
  Year                     = {1995},
  Pages                    = {137--149},

  Abstract                 = {In recent years, there have been much publications on reuse. In order to bet an overview of the whole field and also a good impression of the state of the reuse art, we studied reuse literature of the last few years. As basis for comparison, we classified literature according to four (more or less orthogonal) dimensions, based on the actions and knowledge of the reuser, i.e. the software engineer. The dimensions are: actions to be taken to get an existing reusable item; knowledge to be applied to find an existing reusable item; actions to be taken to build the complete system needed; knowledge to be applied to get the complete system needed.The survey shows that research on reuse from the viewpoint of needed system, receives far less attention than research from the viewpoint of reusable artifacts. We expect reuse to live up to its promise if this topic is addressed was well.},
  Doi                      = {10.1145/211782.211828}
}

@InProceedings{2007:acom:eaddy,
  Title                    = {Identifying, Assigning, and Quantifying Crosscutting Concerns},
  Author                   = {Eaddy, Marc and Aho, Alfred and Murphy, Gail C.},
  Booktitle                = acom,
  Year                     = {2007},
  Pages                    = {2:1--2:6},

  Abstract                 = {Crosscutting concerns degrade software quality. Before we can modularize the crosscutting concerns in our programs to increase software quality, we must first be able to find them. Unfortunately, accurately locating the code related to a concern is difficult, and without proper metrics, determining how much the concern is crosscutting is impossible. We propose a systematic methodology for identifying which code is related to which concern, and a suite of metrics for quantifying the amount of crosscutting code. Our concern identification and assignment guidelines resolve some of the ambiguity issues encountered by other researchers. We applied this approach to systematically identify all the requirement concerns in a 13,531 line program. We found that 95\% of the concerns were crosscutting---indicating a significant potential for improving modularity---and that our metrics were better able to determine which concerns would benefit the most from reengineering.}
}

@Article{2008:tse:eaddy,
  Title                    = {Do Crosscutting Concerns Cause Defects?},
  Author                   = {Eaddy, Marc and Zimmermann, Thomas and Sherwood, Kaitlin D. and Garg, Vibhav and Murphy, Gail C. and Nagappan, Nachiappan and Aho, Alfred V.},
  Journal                  = tse,
  Year                     = {2008},

  Month                    = jul # {/} # aug,
  Number                   = {4},
  Pages                    = {497--515},
  Volume                   = {34},

  Abstract                 = {There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, ``How much does the amount that a concern is crosscutting affect the number of defects in a program?'' We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.},
  Doi                      = {10.1109/TSE.2008.36}
}

@TechReport{1995:tr:eder,
  Title                    = {Coupling and cohesion in object-oriented systems},
  Author                   = {Johann Eder and Gerti Kappel and Michael Schrefl},
  Institution              = {Institut f{\"u}r Informationssysteme, University of Linz},
  Year                     = {1995},

  Address                  = {Linz, Austria},

  Abstract                 = {Object-oriented system development is gaining wide attention both in research environments and in industry. A severe problem encountered, however, is the quickly increasing complexity of such systems and the lack of adequate criteria and guidelines for ``good" designs. To cope with this problem, it is imperative to better understand the properties and characteristics of object-oriented systems. In this paper, we extend the concepts of coupling and cohesion developed initially for procedure-oriented systems to object-oriented systems. Coupling describes the interdependency between methods and between object classes, respectively. Cohesion describes the binding of the elements within one method and within one object class, respectively. We introduce a comprehensive taxonomy of coupling and cohesion properties of object-oriented systems and provide guidelines for improving these properties. },
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5819&rep=rep1&type=pdf}
}

@Misc{2003:misc:edgar:a,
  Title                    = {Bug 36967: Enable {E}clipse to be used as a rich client platform},

  Author                   = {Nick Edgar},
  HowPublished             = {\url{https://bugs.eclipse.org/bugs/show_bug.cgi?id=36967}},
  Month                    = apr,
  Year                     = {2003},

  LastChecked              = {December 2008}
}

@Misc{2003:misc:edgar:b,
  Title                    = {Need a trimmed-down org.eclipse.help},

  Author                   = {Nick Edgar},
  HowPublished             = {\url{https://bugs.eclipse.org/bugs/show_bug.cgi?id=40050}},
  Month                    = jul,
  Note                     = {Eclipse Bug~40050},
  Year                     = {2003}
}

@Misc{2003:misc:edgar:c,
  Title                    = {{E}clipse {R}ich {C}lient {P}latform {UI}},

  Author                   = {Nick Edgar},
  Month                    = dec,
  Year                     = {2003}
}

@InCollection{2010:book:aggarwal:eichinger,
  Title                    = {Software-Bug Localization with Graph Mining},
  Author                   = {Frank Eichinger and Klemens Bohm},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {17},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {515--546},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {In the recent past, a number of frequent subgraph mining algorithms has been proposed They allow for analyses in domains where data is naturally graphstructured. However, caused by scalability problems when dealing with large graphs, the application of graph mining has been limited to only a few domains. In software engineering, debugging is an important issue. It is most challenging to localize bugs automatically, as this is expensive to be done manually. Several approaches have been investigated, some of which analyze traces of repeated program executions. These traces can be represented as call graphs. Such graphs describe the invocations of methods during an execution. This chapter is a survey of graph mining approaches for bug localization based on the analysis of dynamic call graphs. In particular, this chapter first introduces the subproblem of reducing the size of call graphs, before the different approaches to localize bugs based on such reduced graphs are discussed. Finally, we compare selected techniques experimentally and provide an outlook on future issues.},
  Doi                      = {10.1007/978-1-4419-6045-0_17}
}

@Article{2001:tse:eick,
  Title                    = {Does Code Decay? Assessing the Evidence from Change Management Data},
  Author                   = {Eick, Stephen G. and Graves, Todd L. and Karr, Alan F. and Marron, J. S. and Mockus, Audris},
  Journal                  = tse,
  Year                     = {2001},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1--12},
  Volume                   = {27},

  Abstract                 = {A central feature of the evolution of large software systems is that change---which is necessary to add new functionality, accommodate new hardware, and repair faults---becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed.},
  Doi                      = {10.1109/32.895984}
}

@Article{2003:tse:eisenbarth,
  Title                    = {Locating features in source code},
  Author                   = {Thomas Eisenbarth and Rainer Koschke and Daniel Simon},
  Journal                  = tse,
  Year                     = {2003},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {210--224},
  Volume                   = {29},

  Abstract                 = {Understanding the implementation of a certain feature of a system requires identification of the computational units of the system that contribute to this feature. In many cases, the mapping of features to the source code is poorly documented. In this paper, we present a semiautomatic technique that reconstructs the mapping for features that are triggered by the user and exhibit an observable behavior. The mapping is in general not injective; that is, a computational unit may contribute to several features. Our technique allows for the distinction between general and specific computational units with respect to a given set of features. For a set of features, it also identifies jointly and distinctly required computational units. The presented technique combines dynamic and static analyses to rapidly focus on the system's parts that relate to a specific set of features. Dynamic information is gathered based on a set of scenarios invoking the features. Rather than assuming a one-to-one correspondence between features and scenarios as in earlier work, we can now handle scenarios that invoke many features. Furthermore, we show how our method allows incremental exploration of features while preserving the ``mental map" the analyst has gained through the analysis.},
  Doi                      = {10.1109/TSE.2003.1183929}
}

@InProceedings{2006:fse:elbaum,
  Title                    = {Carving differential unit test cases from system test cases},
  Author                   = {Elbaum, Sebastian and Chin, Hui Nee and Dwyer, Matthew B. and Dokulil, Jonathan},
  Booktitle                = fse,
  Year                     = {2006},
  Pages                    = {253--264},

  Abstract                 = {Unit test cases are focused and efficient. System tests are effective at exercising complex usage patterns. Differential unit tests (DUT) are a hybrid of unit and system tests. They are generated by carving the system components, while executing a system test case, that influence the behavior of the target unit, and then re-assembling those components so that the unit can be exercised as it was by the system test. We conjecture that DUTs retain some of the advantages of unit tests, can be automatically and inexpensively generated, and have the potential for revealing faults related to intricate system executions. In this paper we present a framework for automatically carving and replaying DUTs that accounts for a wide-variety of strategies, we implement an instance of the framework with several techniques to mitigate test cost and enhance flexibility, and we empirically assess the efficacy of carving and replaying DUTs.}
}

@Article{2009:tse:elbaum,
  Title                    = {Carving and replaying differential unit test cases from system test cases},
  Author                   = {Sebastian Elbaum and Hui Nee Chin and Matthew B. Dwyer and Matthew Jorde},
  Journal                  = tse,
  Year                     = {2009},

  Month                    = jan # {/} # feb,
  Number                   = {1},
  Pages                    = {29--45},
  Volume                   = {35},

  Abstract                 = {Unit test cases are focused and efficient. System tests are effective at exercising complex usage patterns. Differential unit tests (DUT) are a hybrid of unit and system tests that exploits their strengths. They are generated by carving the system components, while executing a system test case, that influence the behavior of the target unit, and then re-assembling those components so that the unit can be exercised as it was by the system test. In this paper we show that DUTs retain some of the advantages of unit tests, can be automatically generated, and have the potential for revealing faults related to intricate system executions. We present a framework for carving and replaying DUTs that accounts for a wide variety of strategies and tradeoffs, we implement an automated instance of the framework with several techniques to mitigate test cost and enhance flexibility and robustness, and we empirically assess the efficacy of carving and replaying DUTs on three software artifacts.},
  Doi                      = {10.1109/TSE.2008.103}
}

@InProceedings{2007:icse:ellis,
  Title                    = {The {F}actory pattern in {API} design: A usability evaluation},
  Author                   = {Ellis, Brian and Stylos, Jeffrey and Myers, Brad},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {302--312},

  Abstract                 = {The usability of software APIs is an important and infrequently researched topic. A user study comparing the usability of the factory pattern and constructors in API designs found highly significant results indicating that factories are detrimental to API usability in several varied situations. The results showed that users require significantly more time (p = 0.005) to construct an object with a factory than with a constructor while performing both context-sensitive and context-free tasks. These results suggest that the use of factories can and should be avoided in many cases where other techniques, such as constructors or class clusters, can be used instead.},
  Doi                      = {10.1109/ICSE.2007.85}
}

@Article{2007:jcmc:ellison,
  Title                    = {The Benefits of Facebook ``Friends'': Social Capital and College Students' Use of Online Social Network Sites},
  Author                   = {Nicole B. Ellison and Charles Steinfield and Cliff Lampe},
  Journal                  = jcmc,
  Year                     = {2007},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {1143--1168},
  Volume                   = {12},

  Abstract                 = {This study examines the relationship between use of Facebook, a popular online social network site, and the formation and maintenance of social capital. In addition to assessing bonding and bridging social capital, we explore a dimension of social capital that assesses one's ability to stay connected with members of a previously inhabited community, which we call maintained social capital. Regression analyses conducted on results from a survey of undergraduate students (N = 286) suggest a strong association between use of Facebook and the three types of social capital, with the strongest relationship being to bridging social capital. In addition, Facebook usage was found to interact with measures of psychological well-being, suggesting that it might provide greater benefits for users experiencing low self-esteem and low life satisfaction.},
  Url                      = {http://jcmc.indiana.edu/vol12/issue4/ellison.html}
}

@Article{2009:ivc:emms,
  Title                    = {Graph matching using the interference of discrete-time quantum walks},
  Author                   = {David Emms and Richard C. Wilson and Edwin R. Hancock},
  Journal                  = ivc,
  Year                     = {2009},

  Month                    = {4 } # jun,
  Number                   = {7},
  Pages                    = {934--949},
  Volume                   = {27},

  Abstract                 = {In this paper, we consider how discrete-time quantum walks can be applied to graph-matching problems. The matching problem is abstracted using an auxiliary graph that connects pairs of vertices from the graphs to be matched by way of auxiliary vertices. A discrete-time quantum walk is simulated on this auxiliary graph and the quantum interference on the auxiliary vertices indicates possible matches. When dealing with graphs for which there is no exact match, the interference amplitudes together with edge consistencies are used to define a consistency measure. We also explore the use of the method for inexact graph-matching problems. We have tested the algorithm on graphs derived from the NCI molecule database and found it to significantly reduce the space of possible permutation matchings, typically by a factor of $10^{-20}$--$10^{-30}$, thereby allowing the graphs to be matched directly. An analysis of the quantum walk in the presence of structural errors between graphs is used as the basis of the consistency measure. We test the performance of this measure on graphs derived from images in the COIL-100 database.},
  Doi                      = {10.1016/j.imavis.2008.10.013}
}

@Article{2009:patrecog:emms,
  Title                    = {Graph matching using the interference of continuous-time quantum walks},
  Author                   = {Emms, David and Wilson, Richard C. and Hancock, Edwin R.},
  Journal                  = patrecog,
  Year                     = {2009},

  Month                    = may,
  Number                   = {5},
  Pages                    = {985--1002},
  Volume                   = {42},

  Abstract                 = {We consider how continuous-time quantum walks can be used for graph matching. We focus in detail on both exact and inexact graph matching, and consider in depth the problem of measuring graph similarity. We commence by constructing an auxiliary graph, in which the two graphs to be matched are co-joined by a layer of indicator vertices (one for each potential correspondence between a pair of vertices). We simulate a continuous-time quantum walk in parallel on the two graphs. The layer of connecting indicator vertices in the auxiliary graph allow quantum interference to take place between the two walks. The interference amplitudes on the indicator vertices are determined by differences in the two walks, and can be used to calculate probabilities for matches between pairs of vertices from the graphs. By applying the Hungarian (Kuhn-Munkres) algorithm to these probabilities, we recover a correspondence mapping between the graphs. To calculate graph similarity, we combine these probabilities with edge-consistency information to give a consistency measure. Based on the consistency measure, we define two graph similarity measures, one of which requires correspondence matches while the second does not. We analyse our approach experimentally using synthetic and real-world graphs. This reveals that our method gives results that are intermediate between the most sophisticated iterative techniques available, and simpler less complex ones.},
  Doi                      = {10.1016/j.patcog.2008.09.001}
}

@Article{1960:pmihas:erdos,
  Title                    = {On the evolution of random graphs},
  Author                   = {P. Erd{\H{o}}s and A. R{\'e}nyi},
  Journal                  = pmihas,
  Year                     = {1960},
  Number                   = {1--2},
  Pages                    = {17--61},
  Volume                   = {5},

  Abstract                 = {Our aim is to study the probable structure of a random graph $\Gamma_{n,N}$ which has $n$ given labelled vertices $P_1, P_2, \ldots, P_n$ and $N$ edges; we suppose that these $N$ edges are chosen at random among the $\binom{n}{2}$ possible edges, so that all $\binom{\binom{n}{2}}{N} = C_{n,N}$ possible choices are supposed to be equiprobable. Thus if $G_{n,N}$ denotes any one of the $C_{n,N}$ graphs formed from $n$ given labelledpoints and having $N$ edges, the probability that the random graph $\Gamma_{n,N}$ is identical with $G_{n,N}$ is $\frac{1}{C_{n,N}}$. If $A$ is a property which a graph may or may not possess, we denote by $\mathbf{P}_{n,N}(A)$ the probability that the random graph $\Gamma_{n,N}$ possesses the property $A$, i.e. we put $\mathbf{P}_{n,N}(A) = \frac{A_{n,N}}{C_{n,N}}$ where $A_{n,N}$ denotes the number of those $G_{n,N}$ which have the property $A$. An other equivalent formulation is the following: Let us suppose that $n$ labelled vertices $P_1, P_2, \ldots, P_n$ are given. Let us choose at random an edge among the $\binom{n}{2}$possible edges, so that all these edges are equiprobable. After this let us choose an other edge among the remaining $\binom{n}{2} - 1$ edges, and continue this process so that if already $k$ edges are fixed, any of the remaining $\binom{n}{2} - k$ edges have equal probabilities to be chosen as the next one. We shall study the ``evolution" of such a random graph if $N$ is increased. In this investigationwe endeavour to find what is the ``typical" structure at a given stage of evolution (i.e. if $N$ is equal, or asymptotically equal, to a given function$N(n)$ of $n$). By a ``typical" structure we mean such a structure the probability of which tends to 1 if $n \to +\infty$ when $N = N(n)$. If $A$ is such a property that $\lim_{n \to +\infty} \mathbf{P}_{n,N(n)}(A) = 1$, we shall say that ``almost all'' graphs $G_{n,N(n)}$ possess this property.},
  Url                      = {http://ftp.math-inst.hu/~p_erdos/1960-10.pdf}
}

@Article{2006:itpro:erder,
  Title                    = {Transitional architectures for enterprise evolution},
  Author                   = {Erder, Murat and Pureur, Pierre},
  Journal                  = itpro,
  Year                     = {2006},

  Month                    = jan # {/} # feb,
  Number                   = {3},
  Pages                    = {10--17},
  Volume                   = {8},

  Abstract                 = {Most IT architecture projects start with long, expensive effort to define the current state followed by the production of an abundant set of future-state blueprints. This paper introduced the idea of change within the architecture process. By following this approach, you will quickly learn which areas you need to focus on before moving forward with the architecture models and blueprints. The method also yields several important benefits, including a focus on delivering architecture features that matter most to the business; effective mitigation, expectation management, and requirements traceability; and lower costs.},
  Doi                      = {10.1109/MITP.2006.77}
}

@InProceedings{1998:cascon:erdogmus,
  Title                    = {Representing architectural evolution},
  Author                   = {Erdogmus, Hakan},
  Booktitle                = cascon,
  Year                     = {1998},
  Pages                    = {11:1--11:18},

  Abstract                 = {Software engineers informally use block diagrams with boxes and lines to express system architectures. Diagrammatic representations of this type are also found in many specification techniques. However, rarely are architectural documents containing such representations systematically maintained; as a system evolves, architectural documents become obsolete, and the design history of the system is ultimately lost. Additionally, box-and-line representations used in these documents do not possess a precise semantics invariant across the different techniques that rely on them. This paper addresses expression of system evolution at the architectural level based on a formal model of box-and-line diagrams. The formal model (a) provides semantic uniformity and precision; and (b) allows evolutionary steps to be represented as structural transformations. Interesting classes of such transformations are characterized in terms of the underlying operators. With these tools, the architectural evolution of a system is captured as a directed acyclic graph of baselines, where each baseline consists of a system of box-and-line diagrams, and is mapped to a successor baseline by a set of structural transformations. It is also shown how familiar design concepts---such as extension, abstraction, and structural refinement---can be formalized in simple terms within the framework developed.}
}

@Article{2000:itpro:erlikh,
  Title                    = {Leveraging legacy system dollars for e-business},
  Author                   = {Len Erlikh},
  Journal                  = itpro,
  Year                     = {2000},

  Month                    = may # {/} # jun,
  Number                   = {3},
  Pages                    = {17--23},
  Volume                   = {2},

  Abstract                 = {Although many firms have rapidly and enthusiastically adopted distributed architectures, many more are stuck with mainframe based mission-critical systems that continue to isolate them from their partner, supplier, and customer systems. Most companies want to transform their applications to meet new business demands, but because legacy systems tend to be unwieldy, monolithic, and inflexible, many firms regard modernization as somewhere between improbable and impossible. Reeling from the Y2K debacle and saddled with years of application backlog, the most these companies can hope for is to keep their legacy system alive. And keeping it alive is getting more expensive. It is also becoming harder to find qualified personnel to do the maintenance. All of this makes it difficult to add new functionality and keep up with business requirements. The ideal solution is to transform legacy systems to newer, more productive platforms so that companies can exploit faster and cheaper development technologies, like Java and XML (Extensible Markup Language). The focus then shifts to functionality, not the infrastructure, which means a company can respond more quickly to its changing business requirements and technology enhancements. RescueWare, legacy transformation software from Relativity Technologies, breaks business knowledge into stand-alone pieces, or e-components. The e-components are basically collections of objects that perform specific business services, have clearly defined application program interfaces (APIs), and are accessible through modern industry-standard protocols.},
  Doi                      = {10.1109/6294.846201}
}

@Article{1997:qq:erzberger,
  Title                    = {Triangulation: Validity and empirically-based hypothesis construction},
  Author                   = {Christian Erzberger and Gerald Prein},
  Journal                  = qq,
  Year                     = {1997},

  Month                    = may,
  Number                   = {2},
  Pages                    = {141--154},
  Volume                   = {31},

  Abstract                 = {The essay focuses on the opportunities and strengths of a multi-method approach, widely called methodological triangulation, in which different investigative methods are applied to one research ubject. In practice, this can be realized with the coupling of quantitative structural data concerning the life course and the interpretation and evaluation of life course data collected with qualitative methods. This approach is examined in order to shed light on the problem that research findings often show different phenomena and not the different aspects of one phenomenon. The discussion of the relationships of the findings to one another (congruent, complementary or divergent) shows that in this context a multi-method approach can nevertheless be used to increase validity and to test hypotheses. Further, its particular strengths are the empirically induced modification of existing models and theories, as well as the development of new explanations.},
  Doi                      = {10.1023/A:1004249313062}
}

@InProceedings{2005:esec_fse:estublier,
  Title                    = {Reuse and variability in large software applications},
  Author                   = {Jacky Estublier and German Vega},
  Booktitle                = esec_fse,
  Year                     = {2005},
  Pages                    = {316--325},

  Abstract                 = {Reuse has always been a major goal in software engineering, since it promises large gains in productivity, quality and time to market reduction. Practical experience has shown that substantial reuse has only successfully happened in two cases: libraries, where many generic and small components can be found; and product lines, where domains-specific components can be assembled in different ways to produce variations of a given product. In this paper we examine how product lines have successfully achieved reuse of coarse-grained components, and the underlying factors limiting this approach to narrowly scoped domains. We then build on this insight to present an approach, called software federation, which proposes a mechanism to overcome the identified limitations, and therefore makes reuse of coarse-grained components possible over a larger range of applications. Our approach extends and generalizes the product line approach, extending the concepts and mechanisms available to manage variability. The system is in use in different companies, validating the claims made in this paper.},
  Doi                      = {10.1145/1081706.1081757}
}

@Article{2004:ist:etzkorn,
  Title                    = {A comparison of cohesion metrics for object-oriented systems},
  Author                   = {Letha H. Etzkorn and Sampson E. Gholston and Julie L. Fortune and Cara E. Stein and Dawn Utley and Phillip A. Farrington and Glenn W. Cox},
  Journal                  = ist,
  Year                     = {2004},

  Month                    = aug,
  Number                   = {10},
  Pages                    = {677--687},
  Volume                   = {46},

  Abstract                 = {Cohesion is the degree to which the elements of a class or object belong together. Many different object-oriented cohesion metrics have been developed; many of them are based on the notion of degree of similarity of methods. No consensus has yet arisen as to which of these metrics best measures cohesion; this is a problem for software developers since there are so many suggested metrics, it is difficult to make an informed choice. This research compares various cohesion metrics with ratings of two separate teams of experts over two software packages, to determine which of these metrics best match human-oriented views of cohesion. Additionally, the metrics are compared statistically, to determine which tend to measure the same kinds of cohesion. Differences in results for different object-oriented metrics tools are discussed.},
  Doi                      = {10.1016/j.infsof.2003.12.002}
}

@TechReport{2000:tr:evans,
  Title                    = {Why object serialization is inappropriate for providing persistence in {J}ava},
  Author                   = {Evans, Huw},
  Institution              = {Deptartment of Computer Science, University of Glasgow},
  Year                     = {2000}
}

@InProceedings{2000:ase:fahmy,
  Title                    = {Using Graph Rewriting to Specify Software Architectural Transformations},
  Author                   = {Hoda Fahmy and Richard C. Holt},
  Booktitle                = ase,
  Year                     = {2000},
  Pages                    = {187--196},

  Abstract                 = {In order to understand, analyze and possibly modify software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction or repair the architecture if it has deviated from our mental model of the software. We can view such manipulations as architectural transformations, and more specifically, as graph transformations. In this paper, we use graph rewriting to specify these transformations so that we can work towards automating them. Specifically, we use the PROGRES tool to formulate executable graph-rewriting specifications for various architectural transformations in order to demonstrate the strengths of using graph rewriting. We have applied our executable specifications to small graphs and our results confirm the following: Graph rewriting offers a high-level, visual notation that can be used to neatly specify graph modifications as well as support prototype implementation. It also provides a convenient and intuitive framework for exploring various architectural transformations.}
}

@InProceedings{2000:icsm:fahmy,
  Title                    = {Software architecture transformations},
  Author                   = {Fahmy, Hoda and Holt, Richard C.},
  Booktitle                = icsm,
  Year                     = {2000},
  Pages                    = {88--96},

  Abstract                 = {In order to understand and improve software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction or zoom-in on one portion of the system. We may discover that the extracted architecture has deviated from our mental model of the software and hence we may want to repair it. The paper identifies the commonality between these architectural transformation actions, i.e., by manipulating the architecture in order to understand analyze, and modify the software structure, we are in fact performing graph transformations. We categorize useful architectural transformations and describe them within the framework of graph transformations. By describing them in a unified way, we gain a better understanding of the transformations and thus, can work towards modeling, specifying and automating them.}
}

@InProceedings{2001:ase:fahmy,
  Title                    = {Wins and Losses of Algebraic Transformations of Software Architectures},
  Author                   = {Fahmy, Hoda M. and Holt, Richard C. and Cordy, James R.},
  Booktitle                = ase,
  Year                     = {2001},
  Pages                    = {51--62},

  Abstract                 = {In order to understand, analyze and modify software, we commonly examine and manipulate its architecture. For example, we may want to examine the architecture at different levels of abstraction. We can view such manipulations as architectural transformations, and more specifically, as graph transformations. In this paper, we evaluate relational algebra as a way of specifying and automating the architectural transformations. Specifically, we examine Grok, a relational calculator that is part of the PBS toolkit. We show that relational algebra is practical in that we are able to specify many ofthe transformations commonly occurring during software maintenance and, using a tool like Grok, we are able to manipulate, quite efficiently, large software graphs; thi sis a ``win". However, this approach is not well suited to express some types of transforms involving patterns of edges and nodes; this is a ``loss". By means of a set of examples, the paper makes clear when the approach wins and when it loses.}
}

@Article{2008:ese:falke,
  Title                    = {Empirical evaluation of clone detection using syntax suffix trees},
  Author                   = {Falke, Raimar and Frenzel, Pierre and Koschke, Rainer},
  Journal                  = ese,
  Year                     = {2008},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {601--643},
  Volume                   = {13},

  Abstract                 = {Abstract Reusing software through copying and pasting is a continuous plague in software development despite the fact that it creates serious maintenance problems. Various techniques have been proposed to find duplicated redundant code (also known as software clones). A recent study has compared these techniques and shown that token-based clone detection based on suffix trees is fast but yields clone candidates that are often not syntactic units. Current techniques based on abstract syntax trees---on the other hand---find syntactic clones but are considerably less efficient. This paper describes how we can make use of suffix trees to find syntactic clones in abstract syntax trees. This new approach is able to find syntactic clones in linear time and space. The paper reports the results of a large case study in which we empirically compare the new technique to other techniques using the Bellon benchmark for clone detectors. The Bellon benchmark consists of clone pairs validated by humans for eight software systems written in C or Java from different application domains. The new contributions of this paper over the conference paper are the additional analysis of Java programs, the exploration of an alternative path that uses parse trees instead of abstract syntax trees, and the investigation of the impact on recall and precision when clone analyses insist on consistent parameter renaming.},
  Doi                      = {10.1007/s10664-008-9073-9}
}

@InProceedings{2008:pods:fan,
  Title                    = {Dependencies revisited for improving data quality},
  Author                   = {Fan, Wenfei},
  Booktitle                = pods,
  Year                     = {2008},
  Pages                    = {159--170},

  Abstract                 = {Dependency theory is almost as old as relational databases themselves, and has traditionally been used to improve the quality of schema, among other things. Recently there has been renewed interest in dependencies for improving the quality of data. The increasing demand for data quality technology has also motivated revisions of classical dependencies, to capture more inconsistencies in real-life data, and to match, repair and query the inconsistent data. This paper aims to provide an overview of recent advances in revising classical dependencies for improving data quality.},
  Doi                      = {10.1145/1376916.1376940}
}

@InCollection{1989:book:biggerstaff:feather,
  Title                    = {Reuse in the context of a transformation-based methodology},
  Author                   = {Martin S. Feather},
  Booktitle                = {Software Reusability},
  Publisher                = {Addison--Wesley},
  Year                     = {1989},
  Chapter                  = {14},
  Editor                   = {Ted J. Biggerstaff and Alan J. Perlis},
  Pages                    = {337--359},
  Volume                   = {1: Concepts and Models},

  Abstract                 = {The research group at ISI aims to improve the program development process by applying program transformation to develop implementations for specifications. Following this methodology, the development of a piece of software involves its specification in a formal specification language, and subsequent machine-assisted transformation of that specification into an implementation (conventional program). Subsequent maintenance and modification of software developed in this manner is achieved by modifying the specification, and reperforming the transformational development to derive a new implementation. Thus reuse occurs through reusing the original specification, and reusing the original transformational development of that specification. The approach is distinguished by the nature of our specification language, which has been designed to minimize the gap between informal conceptualization and formal specification. A beneficial result of this is that maintenance and modification at the specification level is relatively straightforward. Further, the techniques that are applied in transforming specifications into implementations are themselves applied repeatedly, and serve to capture our programming knowledge in a conveniently reusable manner. Consideration of an example drawn from the domain of process control illustrates these points.}
}

@InProceedings{2009:icpc:feilkas,
  Title                    = {The loss of architectural knowledge during system evolution: An industrial case study},
  Author                   = {Martin Feilkas and Daniel Ratiu and Elmar J{\"u}rgens},
  Booktitle                = icpc,
  Year                     = {2009},
  Pages                    = {188--197},

  Abstract                 = {Architecture defines the components of a system and their dependencies. The knowledge about how the architecture is intended to be implemented is essential to keep the system structure coherent and thereby comprehensible. In practice, this architectural knowledge is explicitly formulated only in the documentation (if at all), which usually gets outdated very soon. This leads to a growing amount of implicit knowledge during evolution that is especially volatile in projects with high developer fluctuation. In this paper we present a case study about the loss of architectural knowledge in three industrial projects to answer the following research questions: 1) to what degree is the architectural documentation kept in conformance with the code? 2) how well does the documentation reflect the intended architecture?, 3) how big is the architectural decay?, and 4) what are the causes for nonconformances? We answer these questions by investigating the architecture documentation, the source code, and by performing interviews with developers. The most important outcomes of our study are: the informal documentation and the source code are not kept in conformance with each other, none of them completely reflects the intended architecture, and even developers taken individually are not completely aware of the intended architecture. Quantitatively, between 70\% and 90\% of these nonconformances are caused by flaws in the documentation and between 10\% and 30\% represent architectural violations in the code.},
  Doi                      = {10.1109/ICPC.2009.5090042}
}

@Article{2002:simgam:feinstein,
  Title                    = {Constructs of Simulation Evaluation},
  Author                   = {Andrew Hale Feinstein and Hugh M. Cannon},
  Journal                  = simgam,
  Year                     = {2002},

  Month                    = dec,
  Number                   = {4},
  Pages                    = {425--440},
  Volume                   = {33},

  Abstract                 = {Although instructional research on simulation has been around for almost 40 years, validation research has failed to hold itself to a common, scientifically acceptable methodology for evaluating this type of learning environment. Several comprehensive reviews of simulation assessment literature have all concluded that this problem stems from poorly designed studies, a failure to adhere to a generally accepted research taxonomy, and no well-defined constructs with which to assess learning outcomes. This article seeks to address the problem by reviewing the various concepts employed in the literature of simulation evaluation and integrating them into a coherent framework from which the evaluative process may proceed in a more systematic manner.},
  Doi                      = {10.1177/1046878102238606}
}

@InProceedings{2012:umap:felfernig,
  Title                    = {Group decision support for requirements negotiation},
  Author                   = {Felfernig, Alexander and Zehentner, Christoph and Ninaus, Gerald and Grabner, Harald and Maalej, Walid and Pagano, Dennis and Weninger, Leopold and Reinfrank, Florian},
  Booktitle                = umapw,
  Year                     = {2012},
  Pages                    = {105--116},
  Series                   = lncs,
  Volume                   = {7138},

  Abstract                 = {Requirements engineering is one of the most critical phases in software development. Requirements verbalize decision alternatives that are negotiated by stakeholders. In this paper we present the results of an empirical analysis of the effects of applying group recommendation technologies to requirements negotiation. This analysis has been conducted within the scope of software development projects at our university where development teams were supported with group recommendation technologies when deciding which requirements should be implemented. A major result of the study is that group recommendation technologies can improve the perceived usability (in certain cases) and the perceived quality of decision support. Furthermore, it is not recommended to disclose preferences of individual group members at the beginning of a decision process---this could lead to an insufficient exchange of decision-relevant information.},
  Doi                      = {10.1007/978-3-642-28509-7_11}
}

@Article{2000:tse:fenton,
  Title                    = {Quantitative Analysis of Faults and Failures in a Complex Software System},
  Author                   = {Fenton, Norman E. and Ohlsson, Niclas},
  Journal                  = tse,
  Year                     = {2000},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {797--814},
  Volume                   = {26},

  Abstract                 = {The dearth of published empirical data on major industrial systems has been one of the reasons that software engineering has failed to establish a proper scientific basis. In this paper, we hope to provide a small contribution to the body of empirical knowledge. We describe a number of results from a quantitative study of faults and failures in two releases of a major commercial system. We tested a range of basic software engineering hypotheses relating to: The Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. However, in neither case is this explained by the size or complexity of the modules. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. We also discovered fairly stable numbers of faults discovered at corresponding testing phases. Our most surprising and important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults: Those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Not only is it misleading to use it as a surrogate quality measure, but, its previous extensive use in metrics studies is shown to be flawed. Our results provide data-points in building up an empirical picture of the software development process. However, even the strong results we have observed are not generally valid as software engineering laws because they fail to take account of basic explanatory data, notably testing effort and operational usage. After all, a module which has not been tested or used will reveal no faults, irrespective of its size, complexity, or any other factor.},
  Doi                      = {10.1109/32.879815},
  Issue                    = {8}
}

@InProceedings{2007:scsc:ferayorni,
  Title                    = {Domain driven simulation modeling for software design},
  Author                   = {Ferayorni, Andrew E. and Sarjoughian, Hessam S.},
  Booktitle                = scsc,
  Year                     = {2007},
  Pages                    = {297--304},

  Abstract                 = {System-theoretic modeling and simulation frameworks such as Object-Oriented Discrete-event System Specification (OO-DEVS) are commonly used for simulating complex systems, but they do not account for domain knowledge. In contrast, Model-Driven Design environments like Rhapsody support capturing domain-specific software design, but offer limited support for simulation. In this paper we describe the use of domain knowledge in empowering simulation environments to support domain-specific modeling. We show how software design pattern abstractions extend the domain-neutral simulation modeling. We applied Composite, Fa{\c{c}}ade, and Observer patterns to an astronomical observatory (AO) command and control system and developed domain-specific simulation models for the system using DEVSJAVA, a realization of OO-DEVS. This approach is exemplified with simulation models developed based on an actual AO system.}
}

@Article{1987:toplas:ferrante,
  Title                    = {The program dependence graph and its use in optimization},
  Author                   = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
  Journal                  = toplas,
  Year                     = {1987},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {319--349},
  Volume                   = {9},

  Abstract                 = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
  Doi                      = {10.1145/24039.24041}
}

@Article{2009:patreclet:ferrer,
  Title                    = {Median graph: A new exact algorithm using a distance based on the maximum common subgraph},
  Author                   = {Ferrer, M. and Valveny, E. and Serratosa, F.},
  Journal                  = patreclet,
  Year                     = {2009},

  Month                    = apr,
  Number                   = {5},
  Pages                    = {579--588},
  Volume                   = {30},

  Abstract                 = {Median graphs have been presented as a useful tool for capturing the essential information of a set of graphs. Nevertheless, computation of optimal solutions is a very hard problem. In this work we present a new and more efficient optimal algorithm for the median graph computation. With the use of a particular cost function that permits the definition of the graph edit distance in terms of the maximum common subgraph, and a prediction function in the backtracking algorithm, we reduce the size of the search space, avoiding the evaluation of a great amount of states and still obtaining the exact median. We present a set of experiments comparing our new algorithm against the previous existing exact algorithm using synthetic data. In addition, we present the first application of the exact median graph computation to real data and we compare the results against an approximate algorithm based on genetic search. These experimental results show that our algorithm outperforms the previous existing exact algorithm and in addition show the potential applicability of the exact solutions to real problems.},
  Doi                      = {10.1016/j.patrec.2008.12.014}
}

@Article{2009:patrecog:ferrer,
  Title                    = {Median graphs: A genetic approach based on new theoretical properties},
  Author                   = {Ferrer, M. and Valveny, E. and Serratosa, F.},
  Journal                  = patrecog,
  Year                     = {2009},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {2003--2012},
  Volume                   = {42},

  Abstract                 = {Given a set of graphs, the median graph has been theoretically presented as a useful concept to infer a representative of the set. However, the computation of the median graph is a highly complex task and its practical application has been very limited up to now. In this work we present two major contributions. On one side, and from a theoretical point of view, we show new theoretical properties of the median graph. On the other side, using these new properties, we present a new approximate algorithm based on the genetic search, that improves the computation of the median graph. Finally, we perform a set of experiments on real data, where none of the existing algorithms for the median graph computation could be applied up to now due to their computational complexity. With these results, we show how the concept of the median graph can be used in real applications and leaves the box of the only-theoretical concepts, demonstrating, from a practical point of view, that can be a useful tool to represent a set of graphs.},
  Doi                      = {10.1016/j.patcog.2009.01.034}
}

@InProceedings{2002:apsec:fidge,
  Title                    = {Contextual Matching of Software Library Components},
  Author                   = {Fidge, C. J.},
  Booktitle                = apsec,
  Year                     = {2002},
  Pages                    = {297--306},

  Abstract                 = {Many automated programming environments construct software by integrating predefined components from a software library. A fundamental challenge in this process is to match the programmer's specified requirements against the stated capabilities of the components. We explain how the chances of successfully achieving a match can be increased by taking the program context surrounding each requirement into consideration. Formal rules, based on program refinement theory, are defined for context-based matching. The rules allow properties that can be proven to hold at a particular point in the program to justify matching with components that operate correctly only in such a context.},
  Doi                      = {10.1109/APSEC.2002.1182999}
}

@InProceedings{2008:icse:figueiredo,
  Title                    = {Evolving software product lines with aspects: An empirical study on design stability},
  Author                   = {Figueiredo, Eduardo and Cacho, Nelio and Sant'Anna, Claudio and Monteiro, Mario and Kulesza, Uira and Garcia, Alessandro and Soares, S{\'e}rgio and Ferrari, Fabiano and Khan, Safoora and Castor Filho, Fernando and Dantas, Francisco},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {261--270},

  Abstract                 = {Software product lines (SPLs) enable modular, large-scale reuse through a software architecture addressing multiple core and varying features. To reap the benefits of SPLs, their designs need to be stable. Design stability encompasses the sustenance of the product line's modularity properties in the presence of changes to both the core and varying features. It is usually assumed that aspect-oriented programming promotes better modularity and changeability of product lines than conventional variability mechanisms, such as conditional compilation. However, there is no empirical evidence on its efficacy to prolong design stability of SPLs through realistic development scenarios. This paper reports a quantitative study that evolves two SPLs to assess various design stability facets of their aspect-oriented implementations. Our investigation focused upon a multi-perspective analysis of the evolving product lines in terms of modularity, change propagation, and feature dependency. We have identified a number of scenarios which positively or negatively affect the architecture stability of aspectual SPLs.},
  Doi                      = {10.1145/1368088.1368124}
}

@InProceedings{2011:aosd:figueiredo,
  Title                    = {On the impact of crosscutting concern projection on code measurement},
  Author                   = {Figueiredo, Eduardo and Garcia, Alessandro and Maia, Marcelo and Ferreira, Gabriel and Nunes, Camila and Whittle, Jon},
  Booktitle                = aosd,
  Year                     = {2011},
  Pages                    = {81--92},

  Abstract                 = {Many concern metrics have been defined to quantify properties of crosscutting concerns, such as scattering, tangling, and dedication. To quantify these properties, concern metrics directly rely on the projection (assignment) of concerns into source code. Although concern identification tools have emerged over the last years, they are still rarely used in practice to support concern projection and, therefore, it is a task often performed manually. This means that the results of concern metrics are likely to be influenced by how accurately programmers assign concerns to code elements. Even though concern assignment is an important and long-standing problem in software engineering, its impact on accurate measures of crosscutting concerns has never been studied and quantified. This paper presents a series of 5 controlled experiments to quantify and analyse the impact of concern projection on crosscutting concern measures. A set of 80 participants from 4 different institutions projected 10 concern instances into the source code of two software systems. We analyse the accuracy of concern projections independently made by developers, and their impact on a set of 12 concern metrics. Our results suggest that: (i) programmers are conservative when projecting crosscutting concerns, (ii) all concern metrics suffer with such conservative behaviour, and (iii) fine-grained tangling measures are more sensitive to different concern projections than coarse-grained scattering metrics.}
}

@InProceedings{2008:csmr:figueiredo,
  Title                    = {On the Maintainability of Aspect-Oriented Software: A Concern-Oriented Measurement Framework},
  Author                   = {Eduardo Figueiredo and Claudio Sant'Anna and Alessandro Garcia and Thiago T. Bartolomei and Walter Cazzola and Alessandro Marchetto},
  Booktitle                = csmr,
  Year                     = {2008},
  Pages                    = {183--192},

  Abstract                 = {Aspect-oriented design needs to be systematically assessed with respect to modularity flaws caused by the realization of driving system concerns, such as tangling, scattering, and excessive concern dependencies. As a result, innovative concern metrics have been defined to support quantitative analyses of concern's properties. However, the vast majority of these measures have not yet being theoretically validated and managed to get accepted in the academic or industrial settings. The core reason for this problem is the fact that they have not been built by using a clearly-defined terminology and criteria. This paper defines a concern-oriented framework that supports the instantiation and comparison of concern measures. The framework subsumes the definition of a core terminology and criteria in order to lay down a rigorous process to foster the definition of meaningful and well-founded concern measures. In order to evaluate the framework generality, we demonstrate the framework instantiation and extension to a number of concern measures suites previously used in empirical studies of aspect-oriented software maintenance.}
}

@InProceedings{2009:icpc:figueiredo,
  Title                    = {Crosscutting Patterns and Design Stability: An Exploratory Analysis},
  Author                   = {Eduardo Figueiredo and Bruno Silva and Claudio Sant'Anna and Alessandro Garcia and Jon Whittle and Daltro Nunes},
  Booktitle                = icpc,
  Year                     = {2009},
  Pages                    = {138--147},

  Abstract                 = {It is often claimed that inaccurate modularisation of crosscutting concerns hinders program comprehension and, as a consequence, leads to harmful software instabilities. However, recent studies have pointed out that crosscutting concerns are not always harmful to design stability. Hence, software maintainers would benefit from well documented patterns of crosscutting concerns and a better understanding about their actual impact on design stability. This paper presents a catalogue of crosscutting concern patterns recurrently observed in software systems. These patterns are described and classified based on an intuitive vocabulary that facilitates their recognition by software engineers. We analysed instances of the crosscutting patterns in object-oriented and aspect-oriented versions of three evolving programs. The outcomes of our exploratory evaluation indicated that: (i) a certain category of crosscutting patterns seems to be good indicator of harmful instabilities, and (ii) aspect-oriented solutions were unable to modularise concerns matching some crosscutting patterns.}
}

@InProceedings{2006:fse:filho,
  Title                    = {Exceptions and Aspects: The Devil Is in the Details},
  Author                   = {Fernando Castor Filho and Nelio Cacho and Raquel Maranh{\~a}o and Eduardo Figueiredo and Alessandro Garcia and Cec{\'\i}lia Mary F. Rubira},
  Booktitle                = fse,
  Year                     = {2006},
  Pages                    = {152--162},

  Abstract                 = {One of the fundamental motivations for employing exception handling in the development of robust applications is to lexically separate error handling code so that it can be independently reused and modified. It is usually assumed that the implementation of exception handling can be better modularized by the use of aspect-oriented programming (AOP). However, the trade-offs involved in using AOP with this goal are not yet well-understood. This paper presents an in-depth investigation of the adequacy of the AspectJ language for modularizing exception handling code. The study consisted in refactoring existing applications so that the code responsible for implementing heterogeneous error handling strategies was moved to separate aspects. We have performed quantitative assessments of four systems---three object-oriented and one aspect-oriented---based on four fundamental quality attributes, namely separation of concerns, coupling, cohesion, and conciseness. Our investigation also included a multi-perspective analysis of the refactored systems, including (i) the reusability of the aspectized error handling code, (ii) the beneficial and harmful aspectization scenarios, and (iii) the scalability of AOP to aspectize exception handling in the presence of other crosscutting concerns.},
  Doi                      = {10.1145/1181775.1181794}
}

@InProceedings{1997:cbr:finnie,
  Title                    = {Estimating Software Development Effort with Case-Based Reasoning},
  Author                   = {G. R. Finnie and G. E. Wittig and J.-M. Desharnais},
  Booktitle                = cbr,
  Year                     = {1997},
  Pages                    = {13--22},
  Series                   = lncs,
  Volume                   = {1266},

  Abstract                 = {Software project effort estimation is a difficult problem complicated by a variety of interrelated factors. Current regression-based models have not had much success in accurately estimating system size. This paper describes a case based reasoning approach to software estimation which performs somewhat better than regression models based on the same data and which has some similarity to human expert judgement approaches. An analysis is performed to determine whether different forms of averaging and adaptation improve the overall quality of the estimate.}
}

@Article{2000:ijase:fischer,
  Title                    = {Specification-Based Browsing of Software Component Libraries},
  Author                   = {Bernd Fischer},
  Journal                  = ijase,
  Year                     = {2000},

  Month                    = may,
  Number                   = {2},
  Pages                    = {179--200},
  Volume                   = {7},

  Abstract                 = {Specification-based retrieval provides exact content-oriented access to component libraries but requires too much deductive power. Specification-based browsing evades this bottleneck by moving any deduction into an off-line indexing phase. In this paper, we show how match relations are used to build an appropriate index and how formal concept analysis is used to build a suitable navigation structure. This structure has the single-focus property (i.e., any sensible subset of a library is represented by a single node) and supports attribute-based (via explicit component properties) and object-based (via implicit component similarities) navigation styles. It thus combines the exact semantics of formal methods with the interactive navigation possibilities of informal methods. Experiments show that current theorem provers can solve enough of the emerging proof problems to make browsing feasible. The navigation structure also indicates situations where additional abstractions are required to build a better index and thus helps to understand and to re-engineer component libraries.},
  Doi                      = {10.1023/A:1008766409590}
}

@Article{1987:software:fischer,
  Title                    = {Cognitive view of reuse and redesign},
  Author                   = {Gerhard Fischer},
  Journal                  = software,
  Year                     = {1987},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {60--72},
  Volume                   = {4},

  Abstract                 = {Reusable components are not enough. Program designers need tools that help them understand the components and how to use them. Fortunately, some support tools do exist.},
  Doi                      = {10.1109/MS.1987.231065}
}

@InProceedings{1991:icse:fischer,
  Title                    = {Cognitive tools for locating and comprehending software objects for reuse},
  Author                   = {Fischer, Gerhard and Henninger, Scott and Redmiles, David},
  Booktitle                = icse,
  Year                     = {1991},
  Pages                    = {318--328},

  Abstract                 = {The authors describe a conceptual framework to facilitate software reuse. It is shown that high functionality computer systems by themselves do not provide sufficient support for software reuse. Two systems that support this framework, CodeFinder and Explainer, are presented. CodeFinder addresses issues on information access for software reuse. Support for comprehending software objects is demonstrated with Explainer. A scenario describing how the two systems are used in a reuse situation is presented. The authors show how these systems fit into the bigger pictures of software development environments, address limitations of the systems, and discuss future directions.},
  Doi                      = {10.1109/ICSE.1991.130658}
}

@Article{1992:jai:fischer,
  Title                    = {Beyond intelligent interfaces: Exploring, analyzing, and creating success models of cooperative problem solving},
  Author                   = {Gerhard Fischer and Brent Reeves},
  Journal                  = jai,
  Year                     = {1992},

  Month                    = may,
  Number                   = {4},
  Pages                    = {311--332},
  Volume                   = {1},

  Abstract                 = {Cooperative problem-solving systems are computer-based systems that augment a person's ability to create, reflect, design, decide, and reason. Our work focuses on supporting cooperative problem solving in the context of high-functionality computer systems. We show how the conceptual framework behind a given system determines crucial aspects of the system's behavior. Several systems are described that attempted to address specific shortcomings of prevailing assumptions, resulting in a new conceptual framework. To further test this resulting framework, we conducted an empirical study of a success model of cooperative problem solving between people in a large hardware store. The conceptual framework is instantiated in a number of new system-building efforts, which are described and discussed.},
  Doi                      = {10.1007/BF00122020}
}

@InProceedings{2003:icsm:fischer,
  Title                    = {Populating a Release History Database from Version Control and Bug Tracking Systems},
  Author                   = {Michael Fischer and Martin Pinzger and Harald Gall},
  Booktitle                = icsm,
  Year                     = {2003},
  Pages                    = {23--32},

  Abstract                 = {Version control and bug tracking systems contain large amounts of historical information that can give deep insight into the evolution of a software project. Unfortunately, these systems provide only insufficient support for a detailed analysis of software evolution aspects. We address this problem and introduce an approach for populating a release history database that combines version data with bug tracking data and adds missing data not covered by version control systems such as merge points. Then simple queries can be applied to the structured data to obtain meaningful views showing the evolution of a software project. Such views enable more accurate reasoning of evolutionary aspects and facilitate the anticipation of software evolution. We demonstrate our approach on the large open source project Mozilla that offers great opportunities to compare results and validate our approach.},
  Doi                      = {10.1109/ICSM.2003.1235403}
}

@InProceedings{2002:issre:fisher,
  Title                    = {Test reuse in the spreadsheet paradigm},
  Author                   = {Fisher, II, Marc and Dalai Jin and Gregg Rothermel and Margaret Burnett},
  Booktitle                = issre,
  Year                     = {2002},
  Pages                    = {257--268},

  Abstract                 = {Spreadsheet languages are widely used by a variety of end users to perform many important tasks. Despite their perceived simplicity, spreadsheets often contain faults. Furthermore, users modify their spreadsheets frequently, which can render previously correct spreadsheets faulty. To address this problem, we previously introduced a visual approach by which users can systematically test their spreadsheets, see where new tests are required after changes, and request automated generation of potentially useful test inputs. To date, however, this approach has not taken advantage of previously developed test cases, which means that users of the approach cannot benefit, when re-testing following changes, from prior testing efforts. We have therefore been investigating ways to add support for test re-use into our spreadsheet testing methodology. In this paper we present a test re-use strategy for spreadsheets, and the algorithms that implement it, and describe their integration into our spreadsheet testing methodology. We report results of a case study examining the application of this strategy.},
  Doi                      = {10.1109/ISSRE.2002.1173265}
}

@Article{1922:jrss:fisher,
  Title                    = {On the interpretation of {$\chi^2$} from contingency tables, and the calculation of {$P$}},
  Author                   = {Fisher, R. A.},
  Journal                  = jrss,
  Year                     = {1922},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {87--94},
  Volume                   = {85},

  Abstract                 = {It is well known that the Pearsonian test of goodness of fit depends upon the calculation of the quantity $\chi^2$ so defined that if $m$ is the number of observations expected in any cell, and $m + x$ the number observed, then $$\chi^2 = S(\frac{x^2}{m}),$$ the summation being extended to all the cells. Pearson has shown (I) that \emph{when the deviations are distributed with the sole restriction that their sum shall be zero}, the distribution of $\chi^2$ is given by the Pearsonian curve of Type III, $$df \prop \chi^{n'-2}\,e^{-\frac{1}{2}\chi^2}\,d\chi,$$ where $n'$ is the number of cells. We are not here concerned to criticise the general adequacy of the $\chi^2$ test, which is certainly valid if the number of observations in each cell is large, but to emphasize the importance of the limitation italicized above. For the $\chi^2$ test has been applied by Pearson and others to contingency tables, in which the sum of the deviations in any row or column is necessarily zero. In these cases we shall show that Elderton's Tables of Goodness of Fit (2) meay still be applied, but that the value of $n'$ with which the table should be entered is not now equal to the number of cells, but to \emph{one more than the number of degrees of freedom in the distribution}. Thus for a contingency table of $r$ rows and $c$ columns we should take $n' = (c - 1) (r - 1) + 1$ instead of $n' = cr$. This modification often makes a very great difference to the probability (P) that a given value of $\chi^2$ should have been obtained by chance.},
  Doi                      = {10.2307/2340521}
}

@Article{1918:trse:fisher,
  Title                    = {{XV}.---The correlation between relatives on the supposition of {M}endelian inheritance},
  Author                   = {R. A. Fisher},
  Journal                  = trse,
  Year                     = {1918},

  Month                    = jan,
  Number                   = {2},
  Pages                    = {399--433},
  Volume                   = {52},

  Abstract                 = {Several attempts have already been made to interpret the well-established results of biometry in accordance with the Mendelian scheme of inheritance. It is here attempted to ascertain the biometrical properties of a population of a more general type than has hitherto been examined, inheritance in which follows this scheme. It is hoped that in this way it will be possible to make a more exact analysis of the causes of human variability. Te great body of available statistics show us that the deviations of a human measurement from its mean follow very closely the Normal Law of Errors, and, therefore, that the variability may be uniformly measured by the standard deviation corresponding to the squre root of the mean square error. When there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations $\sigma_1$ and $\sigma_2$, it is found that the distribution, when both causes act together, has a standard deviation $\sqrt{\sigma_1^2 + \sigma_2^2}$. It is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability. We shall term this quantity the Variance of the normal population to which it refers, and we may now ascribe to the constituent causes fractions or percentages of the total variance which they together produce. It is desirable on the one hand that the elementary ideas at the basis of the calculus of correlations should be clearly understood, and easily expressed in ordinary language, and on the other that loose phrases about the ``percentage of causation," which obscure the essential distinction between the individual and the population, should be carefully avoided.},
  Doi                      = {2440/15097}
}

@InProceedings{2002:pldi:flanagan,
  Title                    = {Extended static checking for {J}ava},
  Author                   = {Flanagan, Cormac and Leino, K. Rustan M. and Lillibridge, Mark and Nelson, Greg and Saxe, James B. and Stata, Raymie},
  Booktitle                = pldi,
  Year                     = {2002},
  Pages                    = {234--245},

  Doi                      = {10.1145/512529.512558}
}

@InProceedings{2005:scam:fluri,
  Title                    = {Fine-grained analysis of change couplings},
  Author                   = {Fluri, Beat and Gall, Harald C. and Pinzger, Martin},
  Booktitle                = scam,
  Year                     = {2005},
  Pages                    = {66--74},

  Abstract                 = {In software evolution analysis, many approaches analyze release history data available through versioning systems. The recent investigations of CVS data have shown that commonly committed files highlight their change couplings. However, CVS stores modifications on the basis of text but does not track structural changes, such as the insertion, removing, or modification of methods or classes. A detailed analysis whether change couplings are caused by source code couplings or by other textual modifications, such as updates in license terms, is not performed by current approaches. The focus of this paper is on adding structural change information to existing release history data. We present an approach that uses the structure compare services shipped with the Eclipse IDE to obtain the corresponding finegrained changes between two subsequent versions of any Java class. This information supports filtering those change couplings which result from structural changes. So we can distill the causes for change couplings along releases and filter out those that are structurally relevant. The first validation of our approach with a medium-sized open source software system showed that a reasonable amount of change couplings are not caused by source code changes.},
  Doi                      = {10.1109/SCAM.2005.14}
}

@Article{2007:tse:fluri,
  Title                    = {Change Distilling: Tree Differencing for Fine-Grained Source Code Change Extraction},
  Author                   = {Fluri, Beat and Wursch, Michael and Pinzger, Martin and Gall, Harald C.},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {725--743},
  Volume                   = {33},

  Abstract                 = {A key issue in software evolution analysis is the identification of particular changes that occur across several versions of a program. We present change distilling, a tree differencing algorithm for fine-grained source code change extraction. For that, we have improved the existing algorithm by Chawathe et al. for extracting changes in hierarchically structured data. Our algorithm extracts changes by finding both a match between the nodes of the compared two abstract syntax trees and a minimum edit script that can transform one tree into the other given the computed matching. As a result, we can identify fine-grained change types between program versions according to our taxonomy of source code changes. We evaluated our change distilling algorithm with a benchmark that we developed, which consists of 1,064 manually classified changes in 219 revisions of eight methods from three different open source projects. We achieved significant improvements in extracting types of source code changes: Our algorithm approximates the minimum edit script 45 percent better than the original change extraction approach by Chawathe et al. We are able to find all occurring changes and almost reach the minimum conforming edit script, that is, we reach a mean absolute percentage error of 34 percent, compared to the 79 percent reached by the original algorithm. The paper describes both our change distilling algorithm and the results of our evolution.},
  Doi                      = {10.1109/TSE.2007.70731}
}

@Article{2003:tse:foss,
  Title                    = {A Simulation Study of the Model Evaluation Criterion {MMRE}},
  Author                   = {Tron Foss and Erik Stensrud and Barbara Kitchenham and Ingunn Myrtveit},
  Journal                  = tse,
  Year                     = {2003},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {985--995},
  Volume                   = {29},

  Abstract                 = {The Mean Magnitude of Relative Error, MMRE, is probably the most widely used evaluation criterion for assessing the performance of competing software prediction models. One purpose of MMRE is to assist us to select the best model. In this paper, we have performed a simulation study demonstrating that MMRE does not always select the best model. Our findings cast some doubt on the conclusions of any study of competing software prediction models that used MMRE as a basis of model comparison. We therefore recommend not using MMRE to evaluate and compare prediction models. At present, we do not have any universal replacement for MMRE. Meanwhile, we therefore recommend using a combination of theoretical justification of the models that are proposed together with other metrics proposed in this paper.},
  Doi                      = {10.1109/TSE.2003.1245300}
}

@Book{1999:book:fowler,
  Title                    = {Refactoring: Improving the Design of Existing Code},
  Author                   = {Martin Fowler},
  Publisher                = {Addison-Wesley},
  Year                     = {1999}
}

@Article{1996:tse:frakes,
  Title                    = {Quality Improvement Using a Software Reuse Failure Modes Model},
  Author                   = {William B. Frakes and Christopher J. Fox},
  Journal                  = tse,
  Year                     = {1996},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {274--279},
  Volume                   = {22},

  Abstract                 = {The paper presents a failure modes model of parts-based software reuse, and shows how this model can be used to evaluate and improve software reuse processes. The model and the technique are illustrated using survey data about software reuse gathered from 113 people from 29 organizations.},
  Doi                      = {10.1109/32.491652}
}

@Article{1995:cacm:frakes,
  Title                    = {Sixteen questions about software reuse},
  Author                   = {William B. Frakes and Christopher J. Fox},
  Journal                  = cacm,
  Year                     = {1995},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {75--88},
  Volume                   = {38},

  Abstract                 = {Software reuse is the use of existing software knowledge or artifacts to build new software artifacts. Reuse is sometimes confused with porting. The two are distinguished as follows: Reuse is using an asset in different systems; porting is moving a system across environments or platforms. For example, in Figure 1 a component in System A is shown used again in System B; this is an example of reuse. System A, developed for Environment 1, is shown moved into Environment 2; this is an example of porting.},
  Doi                      = {10.1145/203241.203260}
}

@Article{2005:tse:frakes,
  Title                    = {Software reuse research: Status and future},
  Author                   = {William B. Frakes and Kyo Kang},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {529--536},
  Volume                   = {31},

  Abstract                 = {This paper briefly summarizes software reuse research, discusses major research contributions and unsolved problems, provides pointers to key publications, and introduces four papers selected from The Eighth International Conference on Software Reuse (ICSR8).},
  Doi                      = {10.1109/TSE.2005.85}
}

@Article{2001:jss:frakes,
  Title                    = {An industrial study of reuse, quality, and productivity},
  Author                   = {William B. Frakes and Giancarlo Succi},
  Journal                  = jss,
  Year                     = {2001},

  Month                    = {15 } # jun,
  Number                   = {2},
  Pages                    = {99--106},
  Volume                   = {57},

  Abstract                 = {The relationship between amount of reuse, quality, and productivity was studied using four sets of C and C++ modules collected from industrial organizations. The data domains are: text retrieval, user interface, distributed repository, medical records. Reuse in this paper is ad hoc, black box, compositional code reuse. The data generally show that more reuse results in higher quality, but are ambiguous regarding the relationship between amount of reuse and productivity.},
  Doi                      = {10.1016/S0164-1212(00)00121-7}
}

@Article{2012:tse:fraser,
  Title                    = {Mutation-driven generation of unit tests and oracles},
  Author                   = {Fraser, Gordon and Zeller, Andreas},
  Journal                  = tse,
  Year                     = {2012},

  Month                    = mar # {/} # apr,
  Number                   = {2},
  Pages                    = {278--292},
  Volume                   = {38},

  Doi                      = {10.1109/TSE.2011.93}
}

@Article{1997:jcss:freund,
  Title                    = {A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting},
  Author                   = {Freund, Yoav and Schapire, Robert E.},
  Journal                  = jcss,
  Year                     = {1997},

  Month                    = aug,
  Number                   = {1},
  Pages                    = {119--139},
  Volume                   = {55},

  Abstract                 = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone--Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in $\Re^n$. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
  Doi                      = {10.1006/jcss.1997.1504}
}

@InProceedings{1997:icse:froehlich,
  Title                    = {Hooking into Object-Oriented Application Frameworks},
  Author                   = {Gary Froehlich and H. James Hoover and Ling Liu and Paul Sorenson},
  Booktitle                = icse,
  Year                     = {1997},
  Pages                    = {491--501},

  Abstract                 = {An object-oriented application framework provides a generic design within a given domain and a reusable implementation of that design. Frameworks have great potential for reuse, but that potential can only be achieved if the framework can be understood and used effectively by application developers. The complexity of the design and implementation of a framework can make understanding the framework difficult, and so good documentation and guidance to the framework user is needed. Several methods exist for documenting the design of the framework, but less work has been done on documenting the purpose of the framework and how it is intended to be used. In this paper, we introduce the concept of hooks as a means of documenting and providing guidance on the intended use of a framework. Each hook provides a concise solution to a focused requirement or problem within the context of the framework. Solutions for more complex requirements can be provided by combining several hooks. A discussion and means of describing hooks is given, with a focus on the method of adaption used to fulfill the requirement and the level of support provided for the solution within the framework.},
  Doi                      = {10.1145/253228.253432}
}

@InProceedings{2010:icsm:fry,
  Title                    = {A human study of fault localization accuracy},
  Author                   = {Zachary P. Fry and Wesley Weimer},
  Booktitle                = icsm,
  Year                     = {2010},
  Pages                    = {1--10},

  Doi                      = {10.1109/ICSM.2010.5609691}
}

@InProceedings{2010:iui:fu,
  Title                    = {Facilitating exploratory search by model-based navigational cues},
  Author                   = {Wai-Tat Fu and Thomas G. Kannampallil and Ruogu Kang},
  Booktitle                = iui,
  Year                     = {2010},
  Pages                    = {199--208},

  Abstract                 = {We present an extension of a computational cognitive model of social tagging and exploratory search called the semantic imitation model. The model assumes a probabilistic representation of semantics for both internal and external knowledge, and utilizes social tags as navigational cues during exploratory search. We used the model to generate a measure of information scent that controls exploratory search behavior, and simulated the effects of multiple presentations of navigational cues on both simple information retrieval and exploratory search performance based on a previous model called SNIF-ACT. We found that search performance can be significantly improved by these model-based presentations of navigational cues for both experts and novices. The result suggested that exploratory search performance depends critically on the match between internal knowledge (domain expertise) and external knowledge structures (folksonomies). Results have significant implications on how social information systems should be designed to facilitate knowledge exchange among users with different background knowledge.}
}

@InProceedings{2007:wrt:fuhrer,
  Title                    = {Advanced Refactoring in the {E}clipse {JDT}: Past, Present, and Future},
  Author                   = {Robert M. Fuhrer and Markus Keller and Adam Kiezun},
  Booktitle                = wrt,
  Year                     = {2007},
  Pages                    = {30--31},

  Abstract                 = {In this position paper, we present the history, the present and our view of the future of refactoring support in Eclipse.},
  Url                      = {http://people.csail.mit.edu/akiezun/wrt07.pdf}
}

@Article{1987:cacm:furnas,
  Title                    = {The vocabulary problem in human-system communication},
  Author                   = {Furnas, G. W. and Landauer, T. K. and Gomez, L. M. and Dumais, S. T.},
  Journal                  = cacm,
  Year                     = {1987},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {964--971},
  Volume                   = {30},

  Abstract                 = {In almost all computer applications, users must enter correct words for the desired objects or actions. For success without extensive training, or in first-tries for new targets, the system must recognize terms that will be chosen spontaneously. We studied spontaneous word choice for objects in five application-related domains, and found the variability to be surprisingly large. In every case two people favored the same term with probability $<$0.20. Simulations show how this fundamental property of language limits the success of various design methodologies for vocabulary-driven interaction. For example, the popular approach in which access is via one designer's favorite single word will result in 80--90 percent failure rates in many common situations. An optimal strategy, unlimited aliasing, is derived and shown to be capable of several-fold improvements.},
  Doi                      = {10.1145/32206.32212}
}

@InProceedings{2006:esop:furr,
  Title                    = {Polymorphic Type Inference for the {JNI}},
  Author                   = {Michael Furr and Jeffrey S. Foster},
  Booktitle                = esop,
  Year                     = {2006},
  Pages                    = {309--324},

  Abstract                 = {We present a multi-lingual type inference system for checking type safety of programs that use the Java Native Interface (JNI). The JNI uses specially-formatted strings to represent class and field names as well as method signatures, and so our type system tracks the flow of string constants through the program. Our system embeds string variables in types, and as those variables are resolved to string constants during inference they are replaced with the structured types the constants represent. This restricted form of dependent types allows us to directly assign type signatures to each of the more than 200 functions in the JNI. Moreover, it allows us to infer types for user-defined functions that are parameterized by Java type strings, which we have found to be common practice. Our inference system allows such functions to be treated polymorphically by using instantiation constraints, solved with semiunification, at function calls. Finally, we have implemented our system and applied it to a small set of benchmarks. Although semi-unification is undecidable, we found our system to be scalable and effective in practice. We discovered 155 errors 36 cases of suspicious programming practices in our benchmarks.}
}

@Article{2006:jsmerp:girba,
  Title                    = {Modeling history to analyze software evolution},
  Author                   = {G{\^\i}rba, Tudor and Ducasse, St{\'e}phane},
  Journal                  = jsmerp,
  Year                     = {2006},

  Month                    = may # {/} # jun,
  Number                   = {3},
  Pages                    = {207--236},
  Volume                   = {18},

  Abstract                 = {The histories of software systems hold useful information when reasoning about the systems at hand or when reasoning about general laws of software evolution. Over the past 30 years, research has been increasingly spent on understanding software evolution. However, the approaches developed so far do not rely on an explicit meta-model and, thus, they make it difficult to reuse or compare their results. We argue that there is a need for an explicit meta-model for software evolution analysis. We present a survey of the evolution analyses and deduce a set of requirements that an evolution meta-model should have. We define Hismo, a meta-model in which history is modeled as an explicit entity. Hismo adds a time layer on top of structural information, and provides a common infrastructure for expressing and combining evolution analyses and structural analyses. We validate the usefulness of our meta-model by presenting how different analyses are expressed on it.},
  Doi                      = {10.1002/smr.v18:3},
  Key                      = {Girba and Ducasse}
}

@InProceedings{2005:iwpc:gorg,
  Title                    = {Detecting and Visualizing Refactorings from Software Archives},
  Author                   = {G{\"o}rg, Carsten and Wei{\ss}gerber, Peter},
  Booktitle                = iwpc,
  Year                     = {2005},
  Pages                    = {205--214},

  Abstract                 = {We perform knowledge discovery in software archives in order to detect refactorings on the level of classes and methods. Our REFVIS prototype finds these refactorings in CVS repositories and relates them to transactions and configurations. Additionally, REFVIS relates movements of methods to the class inheritance hierarchy of the analyzed project. Furthermore, we present our visualization technique that illustrates these refactorings. REFVIS provides both a class hierarchy layout and a package layout and uses color coding to distinguish different kinds of refactorings. Details on each can be displayed on demand using mouse-over tooltips. Finally, we demonstrate by case studies on two open source projects how REFVIS facilitates understanding of refactorings applied to a software project.}
}

@InProceedings{2005:msr:gorg,
  Title                    = {Error detection by refactoring reconstruction},
  Author                   = {G{\"o}rg, Carsten and Wei{\ss}gerber, Peter},
  Booktitle                = msrw,
  Year                     = {2005},
  Note                     = {5~pages},

  Abstract                 = {In many cases it is not sufficient to perform a refactoring only at one location of a software project. For example, refactorings may have to be performed consistently to several classes in the inheritance hierarchy, e.g. subclasses or implementing classes, to preserve equal behavior. In this paper we show how to detect incomplete refactorings---which can cause long standing bugs because some of them do not cause compiler errors---by analyzing software archives. To this end we reconstruct the class inheritance hierarchies, as well as refactorings on the level of methods. Then, we relate these refactorings to the corresponding hierarchy in order to find missing refactorings and thus, errors and inconsistencies that have been introduced in a software project at some point of the history. Finally. we demonstrate our approach by case studies on two open source projects.}
}

@InProceedings{2009:issta:gorg,
  Title                    = {Identifying semantic differences in {AspectJ} programs},
  Author                   = {G{\"o}rg, Martin Th. and Zhao, Jianjun},
  Booktitle                = issta,
  Year                     = {2009},
  Pages                    = {25--36},

  Abstract                 = {Program differencing is a common means of software debugging. Although many differencing algorithms have been proposed for procedural and object-oriented languages like C and Java, there is no differencing algorithm for aspect-oriented languages so far. In this paper we propose an approach for difference analysis of aspect-oriented programs. The proposed algorithm contains a novel way of matching two versions of a module of which the signature has been modified. For this, we also work out a set of well defined signatures for the new elements in the AspectJ language. In accordance with these signatures, and with those existent for elements of the Java language, we investigate a set of signature patterns to be used with the module matching algorithm. Furthermore, we demonstrate successful application of a node-by-node comparison algorithm originally developed for object-oriented programs. Using a tool which implements our algorithms, we set up and evaluate a set of test cases. The results demonstrate the effectiveness of our approach for a large subset of the AspectJ language.},
  Doi                      = {10.1145/1572272.1572276}
}

@InProceedings{2008:icse:gabel,
  Title                    = {Scalable detection of semantic clones},
  Author                   = {Gabel, Mark and Jiang, Lingxiao and Su, Zhendong},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {321--330},

  Abstract                 = {Several techniques have been developed for identifying similar code fragments in programs. These similar fragments, referred to as code clones, can be used to identify redundant code, locate bugs, or gain insight into program design. Existing scalable approaches to clone detection are limited to finding program fragments that are similar only in their contiguous syntax. Other, semantics-based approaches are more resilient to differences in syntax, such as reordered statements, related statements interleaved with other unrelated statements, or the use of semantically equivalent control structures. However, none of these techniques have scaled to real world code bases. These approaches capture semantic information from Program Dependence Graphs (PDGs), program representations that encode data and control dependencies between statements and predicates. Our definition of a code clone is also based on this representation: we consider program fragments with isomorphic PDGs to be clones. In this paper, we present the first scalable clone detection algorithm based on this definition of semantic clones. Our insight is the reduction of the difficult graph similarity problem to a simpler tree similarity problem by mapping carefully selected PDG subgraphs to their related structured syntax. We efficiently solve the tree similarity problem to create a scalable analysis. We have implemented this algorithm in a practical tool and performed evaluations on several million-line open source projects, including the Linux kernel. Compared with previous approaches, our tool locates significantly more clones, which are often more semantically interesting than simple copied and pasted code fragments.},
  Doi                      = {10.1145/1368088.1368132}
}

@InProceedings{2010:fse:gabel,
  Title                    = {A study of the uniqueness of source code},
  Author                   = {Gabel, Mark and Su, Zhendong},
  Booktitle                = fse,
  Year                     = {2010},
  Pages                    = {147--156},

  Abstract                 = {This paper presents the results of the first study of the uniqueness of source code. We define the uniqueness of a unit of source code with respect to the entire body of written software, which we approximate with a corpus of 420 million lines of source code. Our high-level methodology consists of examining a collection of 6,000 software projects and measuring the degree to which each project can be `assembled' solely from portions of this corpus, thus providing a precise measure of `uniqueness' that we call syntactic redundancy. We parameterized our study over a variety of variables, the most important of which being the level of granularity at which we view source code. Our suite of experiments together consumed approximately four months of CPU time, providing quantitative answers to the following questions: at what levels of granularity is software unique, and at a given level of granularity, how unique is software? While we believe these questions to be of intrinsic interest, we discuss possible applications to genetic programming and developer productivity tools.},
  Doi                      = {10.1145/1882291.1882315}
}

@InProceedings{1992:icse:gaffney,
  Title                    = {A general economics model of software reuse},
  Author                   = {Gaffney, Jr., J. E. and R. D. Cruickshank},
  Booktitle                = icse,
  Year                     = {1992},
  Pages                    = {327--337},

  Abstract                 = {A general model of software reuse economics is presented. The model provides a framework for making estimates and decisions about the economic desirability of reusing software. It covers costs and productivity, return on investment, and incremental funding schemes.},
  Doi                      = {10.1145/143062.143150}
}

@InProceedings{1997:icsm:gall,
  Title                    = {Software evolution observations based on product release history},
  Author                   = {Gall, Harald and Jazayeri, Mehdi and Kl{\"o}sch, Ren{\'e} R. and Trausmuth, Georg},
  Booktitle                = icsm,
  Year                     = {1997},
  Pages                    = {160--166},

  Abstract                 = {Large software systems evolve slowly but constantly. In this paper we examine the structure of several releases of a telecommunication switching system (TSS) based on information stored in a database of product releases. We tracked the historical evolution of the TSS structure and related the adaptations made (e.g. addition of new features, etc.) to the structure of the system. Such a systematic examination can uncover potential shortcomings in the structure of the system and identify modules or subsystems that should be subject to restructuring or reengineering. Further, we have identified additional information that would be useful for such investigations but is currently lacking in the database.}
}

@InProceedings{2009:suite:gallardo-valencia,
  Title                    = {Internet-Scale Code Search},
  Author                   = {Gallardo-Valencia, Rosalva E. and Sim, Susan Elliott},
  Booktitle                = suite,
  Year                     = {2009},
  Pages                    = {49--52},

  Abstract                 = {Internet-Scale Code Search is the problem of finding source on the Internet. Developers are typically searching for code to reuse as-is on a project or as a reference example. This phenomenon has emerged due to the increasing availability and quality of open source and resources on the web. Solutions to this problem will involve more than the simple application of information retrieval techniques or a scaling-up of tools for code search. Instead, new, purpose-built solutions are needed that draw on results from these areas, as well as program comprehension and software reuse.},
  Doi                      = {10.1109/SUITE.2009.5070022}
}

@Book{1994:book:gamma,
  Title                    = {Design Patterns: Elements of Reusable Object-Oriented Software},
  Author                   = {Erich Gamma and Richard Helm and Ralph Johnson and John Vlissides},
  Publisher                = {Addison-Wesley},
  Year                     = {1994}
}

@Article{1996:ijase:gangopadhyay,
  Title                    = {Design by Framework Completion},
  Author                   = {Dipayan Gangopadhyay and Subrata Mitra},
  Journal                  = ijase,
  Year                     = {1996},

  Month                    = aug,
  Number                   = {3/4},
  Pages                    = {219--237},
  Volume                   = {3},

  Abstract                 = {An object-oriented framework in essence defines an architecture for a family of applications or subsystems in a given domain. Every application in the family obeys these architectural restrictions. Such frameworks are typically delivered as collections of inter-dependent abstract classes, together with their concrete subclasses. The abstract classes and their interdependencies implicitly realize the architecture. Developing a new application reusing classes of a framework requires a thorough understanding of the framework architecture. We introduce an approach called ``Design by Framework Completion", in which an exemplar (an executable visual model for a minimal instantiation of the architecture) is used for documenting frameworks. We propose exploration of exemplars as a means for learning the architecture, following which new applications can be built by replacing selected pieces of the exemplar. For the piece to be replaced, the inheritance lattice around its class provides the space of alternatives, one of these classes may be suitably adapted (say, by sub-classing) to create the new replacement. ``Design by Framework Completion" proposes a paradigm shift when designing in presence of reusable components: It enables a much simpler ``top-down" approach for creating applications, as opposed to the prevalent ``search for components and assemble them bottom-up" strategy. We believe that this paradigm shift is essential because components can only be fitted together if they all obey the same architectural rules that govern the framework.},
  Doi                      = {10.1007/BF00132567}
}

@InProceedings{1995:iwcase:gangopadhyay,
  Title                    = {Understanding Frameworks by Exploration of Exemplars},
  Author                   = {Dipayan Gangopadhyay and Subrata Mitra},
  Booktitle                = iwcase,
  Year                     = {1995},
  Pages                    = {90--99},

  Abstract                 = {A framework is designed to cover a family of applications or subsystems in a given domain and is typically delivered as a collection of interdependent abstract classes, together with their concrete subclasses. The abstract classes and their interdependencies describe the architecture of the framework. Developing a new application reusing classes of a framework requires a thorough understanding of the framework architecture. We introduce the notion of an exemplar for documenting framework, and propose exploration of exemplars as a means for architecture understanding. An exemplar is a executable visual model consisting of instances of concrete classes together with explicit representation of their collaborations. For each abstract class in the framework, at least one of its concrete subclasses must be instantiated in the exemplar. Model level exploration of exemplars is unique among the prevalent approaches to framework based development; existing approaches still emphasize different class browsing and retrieval technologies, active cookbooks, or code tracing.}
}

@InProceedings{2010:sess:gao,
  Title                    = {Empirical analysis of software coupling networks in object-oriented software systems},
  Author                   = {Yang Gao and Guoai Xu and Yixian Yang and Xinxin Niu and Shize Guo},
  Booktitle                = sess,
  Year                     = {2010},
  Pages                    = {178--181},

  Abstract                 = {The increasing scale and complexity of software systems have led to a large amount of work being performed in the area of software architecture of object-oriented systems. For the purpose of reasonable description of software structure, we make use of complex network theory to characterize and describe their macroscopic properties. We propose a directed software coupling network, and then empirical analysis of four open source software is implemented. We summarize the general statistical features of software coupling network, and find that software coupling network is of small-world and scale-free property, the exponents of in-degree and out-degree distributions are different, and the betweenness distribution of SCN is also power law. Furthermore, we analyzed the causes of behaviors software coupling network possessed.}
}

@InProceedings{2006:shark:garcia,
  Title                    = {Driving and managing architectural decisions with aspects},
  Author                   = {Garcia, Alessandro and Batista, Thais and Rashid, Awais and Sant'Anna, Claudio},
  Booktitle                = shark,
  Year                     = {2006},
  Pages                    = {6:1--6:8},

  Doi                      = {10.1145/1163514.1178646}
}

@InProceedings{2005:aosd:garcia,
  Title                    = {Modularizing Design Patterns with Aspects: A Quantitative Study},
  Author                   = {Alessandro Garcia and Cl{\'a}udio Sant'Anna and Eduardo Figueiredo and Uir{\'a} Kulesza and Carlos Lucena and von Staa, Arndt},
  Booktitle                = aosd,
  Year                     = {2005},
  Pages                    = {3--14},

  Abstract                 = {Design patterns offer flexible solutions to common problems in software development. Recent studies have shown that several design patterns involve crosscutting concerns. Unfortunately, object-oriented (OO) abstractions are often not able to modularize those crosscutting concerns, which in turn decrease the system reusability and maintainability. Hence, it is important verifying whether aspect-oriented approaches support improved modularization of crosscutting concerns relative to design patterns. Ideally, quantitative studies should be performed to compare OO and aspect-oriented implementations of classical patterns with respect to important software engineering attributes, such as coupling and cohesion. This paper presents a quantitative study that compares aspect-based and OO solutions for the 23 Gang-of-Four patterns. We have used stringent software engineering attributes as the assessment criteria. We have found that most aspect-oriented solutions improve separation of patternrelated concerns, although only 4 aspect-oriented implementations have exhibited significant reuse.},
  Doi                      = {10.1145/1052898.1052899}
}

@Article{2009:software:garlan,
  Title                    = {Architectural mismatch: Why reuse is still so hard},
  Author                   = {David Garlan and Robert Allen and John Ockerbloom},
  Journal                  = software,
  Year                     = {2009},

  Month                    = jul # {/} # aug,
  Number                   = {4},
  Pages                    = {66--69},
  Volume                   = {26},

  Abstract                 = {In this article, David Garlan, Robert Allen, and John Ockerbloom reflect on the state of architectural mismatch, a term they coined in their 1995 IEEE Software article, ``Architectural Mismatch: Why Reuse Is So Hard." Although the nature of software systems has changed dramatically since the earlier article was published, the challenge of architectural mismatch remains an important concern for the software engineering field.},
  Doi                      = {10.1109/MS.2009.86}
}

@Article{1995:software:garlan,
  Title                    = {Architectural mismatch: Why reuse is so hard},
  Author                   = {David Garlan and Robert Allen and John Ockerbloom},
  Journal                  = software,
  Year                     = {1995},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {17--26},
  Volume                   = {12},

  Abstract                 = {Architectural mismatch stems from mismatched assumptions a reusable part makes about the system structure it is to be part of. These assumptions often conflict with the assumptions of other parts and are almost always implicit, making them extremely difficult to analyze before building the system. To illustrate how the perspective of architectural mismatch can clarify our understanding of component integration problems, we describe our experience of building a family of software design environments from existing parts. On the basis of our experience, we show how an analysis of architectural mismatch exposes some fundamental, thorny problems for software composition and suggests some possible research avenues needed to solve them.},
  Doi                      = {10.1109/52.469757}
}

@InProceedings{2009:wicsa_ecsa:garlan,
  Title                    = {Evolution styles: Foundations and tool support for software architecture evolution},
  Author                   = {David Garlan and Jeffrey M. Barnes and Bradley R. Schmerl and Orieta Celiku},
  Booktitle                = wicsa_ecsa,
  Year                     = {2009},
  Pages                    = {131--140},

  Abstract                 = {As new market opportunities, technologies, platforms, and frameworks become available, systems require large-scale and systematic architectural restructuring to accommodate them. Today's architects have few tools and techniques to help them plan this architecture evolution. In particular, they have little assistance in planning alternative evolution paths, trading off various aspects of the different paths, or knowing best practices for particular domains. In this paper we describe an approach for assisting architects in developing and reasoning about architectural evolution paths. The key insight of our approach is that, architecturally, many system evolutions follow certain common patterns---or evolution styles. We define what we mean by an evolution style, and show how it can be used to provide automated assistance for expressing architectural evolution, and for reasoning about both the correctness and quality of evolution paths.},
  Doi                      = {10.1109/WICSA.2009.5290799}
}

@InProceedings{2009:icse:garlan,
  Title                    = {{\AE}vol: A tool for defining and planning architecture evolution},
  Author                   = {David Garlan and Bradley R. Schmerl},
  Booktitle                = icse,
  Year                     = {2009},
  Pages                    = {591--594},

  Abstract                 = {Architecture evolution is a key feature of most software systems. There are few tools that help architects plan and execute these evolutionary paths. We demonstrate a tool to enable architects to describe evolution paths, associate properties with elements of the paths, and perform tradeoff analysis over these paths.}
}

@Article{1990:sej:garnett,
  Title                    = {Software reclamation},
  Author                   = {E. S. Garnett and J. A. Mariani},
  Journal                  = sej,
  Year                     = {1990},

  Month                    = may,
  Number                   = {3},
  Pages                    = {185--191},
  Volume                   = {5},

  Abstract                 = {One of the major barriers to the introduction of reuse technology into the software development process is the absence of large repositories of reusable components from which manufacturers can build new generations of systems. Owing to the tremendous investment that has been made in developing systems, companies use an evolutionary approach to software development whereby the old version becomes the basis of the next generation. Owing to such costs, companies are understandably reluctant to develop completely new versions from scratch. If software reusability is to emerge as a discipline, then some mechanism whereby components are reclaimed from existing systems and transformed according to reuse criteria must be found. The authors discuss an approach designed to reclaim software components from existing systems and transform them into objects, which are inherently more reusable.},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=54407}
}

@InProceedings{2012:issta:geldenhus,
  Title                    = {Probabilistic symbolic execution},
  Author                   = {Geldenhuys, Jaco and Dwyer, Matthew B. and Visser, Willem},
  Booktitle                = issta,
  Year                     = {2012},
  Pages                    = {166--176},

  Abstract                 = {The continued development of efficient automated decision procedures has spurred the resurgence of research on symbolic execution over the past decade. Researchers have applied symbolic execution to a wide range of software analysis problems including: checking programs against contract specifications, inferring bounds on worst-case execution performance, and generating path-adequate test suites for widely used library code. In this paper, we explore the adaptation of symbolic execution to perform a more quantitative type of reasoning---the calculation of estimates of the probability of executing portions of a program. We present an extension of the widely used Symbolic PathFinder symbolic execution system that calculates path probabilities. We exploit state-of-the-art computational algebra techniques to count the number of solutions to path conditions, yielding exact results for path probabilities. To mitigate the cost of using these techniques, we present two optimizations, PC slicing and count memoization, that significantly reduce the cost of probabilistic symbolic execution. Finally, we present the results of an empirical evaluation applying our technique to challenging library container implementations and illustrate the benefits that adding probabilities to program analyses may offer.}
}

@InProceedings{2005:metrics:geppert,
  Title                    = {Refactoring for changeability: A way to go?},
  Author                   = {Geppert, Birgit and Mockus, Audris and R{\"o}{\ss}ler, Frank},
  Booktitle                = metrics,
  Year                     = {2005},
  Pages                    = {13:1--13:10},

  Abstract                 = {Legacy systems are difficult and expensive to maintain due to size, complexity, and age of their code base. Business needs require continuously adding new features and maintaining older releases. This and the ever present worry about feature breakage are often the reason why the sweeping changes for reversing design degradation are considered too costly, risky and difficult to implement. We study a refactoring carried out on a part of a large legacy business communication product where protocol logic in the registration domain was restructured. We pose a number of hypotheses about the strategies and effects of the refactoring effort on aspects of changeability and measure the outcomes. The results of this case study show a significant decrease in customer reported defects and in effort needed to make changes.},
  Doi                      = {10.1109/METRICS.2005.40}
}

@InProceedings{2003:oose:german,
  Title                    = {Automating the measurement of open source projects},
  Author                   = {Daniel German and Audris Mockus},
  Booktitle                = oose,
  Year                     = {2003},
  Pages                    = {63--67},

  Abstract                 = {The proliferation of open source projects raises a number of vital economic, social, and software engineering questions that are subject of intense research. Based on experience analyzing numerous open source and commercial projects we propose a set of tools to support extraction and validation of software project data. Such tools would streamline empirical investigation of open source projects and make it possible to test existing and new theories about the nature of open source projects. Our software includes tools to extract and summarize information from mailing lists, CVS logs, ChangeLog files, and defect tracing databases. More importantly, it cross-links records from various data sources and identifies all contributors for a software change. We illustrate some of the capabilities by analyzing data from Ximian Evolution project.}
}

@InProceedings{2004:icsm:german,
  Title                    = {An Empirical Study of Fine-Grained Software Modifications},
  Author                   = {German, Daniel M.},
  Booktitle                = icsm,
  Year                     = {2004},
  Pages                    = {316--325},

  Abstract                 = {Software is typically improved and modified in small increments. These changes are usually stored in a configuration management or version control system and can be retrieved. In this paper we retrieved each individual modification made to a mature software project and proceeded to analyze them. We studied the characteristics of these Modification Requests (MRs), the interrelationships of the files that compose them, and their authors. We propose several metrics to quantify MRs, and use these metrics to create visualization graphs that can be used to understand the interrelationships.},
  Doi                      = {10.1109/ICSM.2004.1357817}
}

@Article{2004:jsmerp:german,
  Title                    = {Using software trails to reconstruct the evolution of software},
  Author                   = {German, Daniel M.},
  Journal                  = jsmerp,
  Year                     = {2004},

  Month                    = nov # {/} # dec,
  Note                     = {Special Issue: Analyzing the Evolution of Large-Scale Software},
  Number                   = {6},
  Pages                    = {367--384},
  Volume                   = {16},

  Abstract                 = {This paper describes a method to recover the evolution of a software system using its software trails: information left behind by the contributors to the development process of the product, such as mailing lists, Web sites, version control logs, software releases, documentation, and the source code. This paper demonstrates the use of this method by recovering the evolution of Ximian Evolution, a mail client for Unix. By extracting useful facts stored in these software trails and correlating them, it was possible to provide a detailed view of the history of this project. This view provides interesting insight into how an open source software project evolves and some of the practices used by its software developers.},
  Doi                      = {10.1002/smr.301}
}

@InProceedings{2009:msr:german,
  Title                    = {Code siblings: Technical and legal implications of copying code between applications},
  Author                   = {German, Daniel M. and Di Penta, Massimiliano and Gu{\'e}h{\'e}neuc, Yann-Ga{\"e}l and Antoniol, Giuliano},
  Booktitle                = msrwc,
  Year                     = {2009},
  Pages                    = {81--90},

  Abstract                 = {Source code cloning does not happen within a single system only. It can also occur between one system and another. We use the term code sibling to refer to a code clone that evolves in a different system than the code from which it originates. Code siblings can only occur when the source code copyright owner allows it and when the conditions imposed by such license are not incompatible with the license of the destination system. In some situations copying of source code fragments are allowed---legally---in one direction, but not in the other. In this paper, we use clone detection, license mining and classification, and change history techniques to understand how code siblings---under different licenses---flow in one direction or the other between Linux and two BSD Unixes, FreeBSD and OpenBSD. Our results show that, in most cases, this migration appears to happen according to the terms of the license of the original code being copied, favoring always copying from less restrictive licenses towards more restrictive ones. We also discovered that sometimes code is inserted to the kernels from an outside source.}
}

@InProceedings{2010:ase:german,
  Title                    = {A sentence-matching method for automatic license identification of source code files},
  Author                   = {German, Daniel M. and Manabe, Yuki and Inoue, Katsuro},
  Booktitle                = ase,
  Year                     = {2010},
  Pages                    = {437--446},

  Abstract                 = {The reuse of free and open source software (FOSS) components is becoming more prevalent. One of the major challenges in finding the right component is finding one that has a license that is e for its intended use. The license of a FOSS component is determined by the licenses of its source code files. In this paper, we describe the challenges of identifying the license under which source code is made available, and propose a sentence-based matching algorithm to automatically do it. We demonstrate the feasibility of our approach by implementing a tool named Ninka. We performed an evaluation that shows that Ninka outperforms other methods of license identification in precision and speed. We also performed an empirical study on 0.8 million source code files of Debian that highlight interesting facts about the manner in which licenses are used by FOSS.},
  Doi                      = {10.1145/1858996.1859088}
}

@InProceedings{2005:ecoop:gibbs,
  Title                    = {Sustainable System Infrastructure and Big Bang Evolution: Can Aspects Keep Pace?},
  Author                   = {Celina Gibbs and Chunjan Robin Liu and Yvonne Coady},
  Booktitle                = ecoop,
  Year                     = {2005},
  Pages                    = {241--261},
  Series                   = lncs,
  Volume                   = {3586},

  Abstract                 = {Realistically, many rapidly evolving systems eventually require extensive restructuring in order to effectively support further evolution. Not surprisingly, these overhauls reverberate throughout the system. Though several studies have shown the benefits of aspect-oriented programming (AOP) from the point of view of the modularization and evolution of crosscutting concerns, the question remains as to how well aspects fare when the code that is crosscut undergoes extensive restructuring. That is, when evolution is a big bang, can aspects keep pace? The case study presented here considers several categories of aspects---design invariants, dynamic analysis tools, and domain specific design patterns---and shows the concrete ways in which aspects had positive, negative and neutral impact during the restructuring of the memory management subsystem of a virtual machine. Compared with best efforts in a hierarchical decomposition coupled with a preprocessor, aspects fared better than the original implementation in two out of four aspects, and no worse in the remaining two aspects.}
}

@InProceedings{2010:rsse:giger,
  Title                    = {Predicting the Fix Time of Bugs},
  Author                   = {Giger, Emanuel and Pinzger, Martin and Gall, Harald},
  Booktitle                = rsse,
  Year                     = {2010},
  Pages                    = {52--56},

  Doi                      = {10.1145/1808920.1808933}
}

@Book{2000:book:gladwell,
  Title                    = {The Tipping Point: How Little Things Can Make a Big Difference},
  Author                   = {Malcolm Gladwell},
  Publisher                = {Little Brown},
  Year                     = {2000}
}

@Book{1967:book:glaser,
  Title                    = {The Discovery of Grounded Theory: Strategies for Qualitative Research},
  Author                   = {Glaser, Barney G. and Strauss, Anselm L.},
  Publisher                = {Transaction},
  Year                     = {1967}
}

@InProceedings{2009:sosp:glerum,
  Title                    = {Debugging in the (Very) Large: Ten Years of Implementation and Experience},
  Author                   = {Glerum, Kirk and Kinshumann, Kinshuman and Greenberg, Steve and Aul, Gabriel and Orgovan, Vince and Nichols, Greg and Grant, David and Loihle, Gretchen and Hunt, Galen},
  Booktitle                = sosp,
  Year                     = {2009},
  Pages                    = {103--116},

  Doi                      = {10.1145/1629575.1629586}
}

@InProceedings{2002:iwpse:godfrey,
  Title                    = {Tracking structural evolution using origin analysis},
  Author                   = {Godfrey, Michael and Tu, Qiang},
  Booktitle                = iwpse,
  Year                     = {2002},
  Pages                    = {117--119},

  Abstract                 = {Many long term studies of software evolution have made the simplifying assumption that the system's architecture and low-level structure is relatively stable. In our past work, we have found that this is often untrue; therefore, we have sought to investigate ways to detect and model structural change in software systems through a technique we call origin analysis [6] and supported a tool called Beagle [7]. In this position paper, we present a summary of our recent and ongoing work in this area, and we argue that more attention needs to be paid to techniques for understanding architectural and structural evolution of software systems.}
}

@InProceedings{2001:iwpse:godfrey,
  Title                    = {Growth, evolution, and structural change in open source software},
  Author                   = {Godfrey, Michael and Tu, Qiang},
  Booktitle                = iwpse,
  Year                     = {2001},
  Pages                    = {103--106},

  Abstract                 = {Our recent work has addressed how and why software systems evolve over time, with a particular emphasis on software architecture and open source software systems [2, 3, 6]. In this position paper, we present a short summary of two recent projects.First, we have performed a case study on the evolution of the Linux kernel [3], as well as some other open source software (OSS) systems. We have found that several OSS systems appear not to obey some of ``Lehman's laws" of software evolution [5, 7], and that Linux in particular is continuing to grow at a geometric rate. Currently, we are working on a detailed study of the evolution of one of the subsystems of the Linux kernel: the SCSI drivers subsystem. We have found that cloning, which is usually considered to be an indicator of lazy development and poor process, is quite common and is even considered to be a useful practice.Second, we are developing a tool called Beagle to aid software maintainers in understanding how large systems have changed over time. Beagle integrates data from various static analysis and metrics tools and provides a query engine as well as navigable visualizations. Of particular note, Beagle aims to provide help in modelling long term evolution of systems that have undergone architectural and structural change.},
  Doi                      = {10.1145/602461.602482}
}

@InProceedings{2008:fosm:godfrey,
  Title                    = {The past, present, and future of software evolution},
  Author                   = {Godfrey, M. W and German, D. M},
  Booktitle                = fosm,
  Year                     = {2008},
  Pages                    = {129--138},

  Abstract                 = {Change is an essential characteristic of software development, as software systems must respond to evolving requirements, platforms, and other environmental pressures. In this paper, we discuss the concept of software evolution from several perspectives. We examine how it relates to and differs from software maintenance. We discuss insights about software evolution arising from Lehman's laws of software evolution and the staged lifecycle model of Bennett and Rajlich. We compare software evolution to other kinds of evolution, from science and social sciences, and we examine the forces that shape change. Finally, we discuss the changing nature of software in general as it relates to evolution, and we propose open challenges and future directions for software evolution research.},
  Doi                      = {10.1109/FOSM.2008.4659256}
}

@InProceedings{2000:icsm:godfrey,
  Title                    = {Evolution in Open Source Software: A Case Study},
  Author                   = {Michael W. Godfrey and Qiang Tu},
  Booktitle                = icsm,
  Year                     = {2000},
  Pages                    = {131--142},

  Abstract                 = {Most studies of software evolution have been performed on systems developed within a single company using traditional management techniques. With the widespread availability of several large software systems that have been developed using an ``open source'' development approach, we now have a chance to examine these systems in detail, and see if their evolutionary narratives are significantly different from commercially developed systems. This paper summarizes our preliminary investigations into the evolution of the best known open source system: the Linux operating system kernel. Because Linux is large (over two million lines of code in the most recent version) and because its development model is not as tightly planned and managed as most industrial software processes, we had expected to find that Linux was growing more slowly as it got bigger and more complex. Instead, we have found that Linux has been growing at a super-linear rate for several years. In this paper, we explore the evolution of the Linux kernel both at the system level and within the major subsystems, and we discuss why we think Linux continues to exhibit such strong growth.}
}

@Article{2005:tse:godfrey,
  Title                    = {Using origin analysis to detect merging and splitting of source code entities},
  Author                   = {Godfrey, Michael W. and Zou, Lijie},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {166--181},
  Volume                   = {31},

  Abstract                 = {Merging and splitting source code entities is a common activity during the lifespan of a software system; as developers rethink the essential structure of a system or plan for a new evolutionary direction, so must they be able to reorganize the design artifacts at various abstraction levels as seems appropriate. However, while the raw effects of such changes may be plainly evident in the new artifacts, the original context of the design changes is often lost. That is, it may be obvious which characters of which files have changed, but it may not be obvious where or why moving, renaming, merging, and/or splitting of design elements has occurred. In this paper, we discuss how we have extended origin analysis (Q. Tu et al., 2002), (M.W. Godfrey et al., 2002) to aid in the detection of merging and splitting of files and functions in procedural code; in particular, we show how reasoning about how call relationships have changed can aid a developer in locating where merges and splits have occurred, thereby helping to recover some information about the context of the design change. We also describe a case study of these techniques (as implemented in the Beagle tool) using the PostgreSQL database system as the subject.},
  Doi                      = {10.1109/TSE.2005.28}
}

@Article{2002:pnas:goh,
  Title                    = {Classification of scale-free networks},
  Author                   = {Kwang-Il Goh and Eulsik Oh and Hawoong Jeong and Byungnam Kahng and Doochul Kim},
  Journal                  = pnas,
  Year                     = {2002},

  Month                    = {1 } # oct,
  Number                   = {20},
  Pages                    = {12583--12588},
  Volume                   = {99},

  Abstract                 = {While the emergence of a power-law degree distribution in complex networks is intriguing, the degree exponent is not universal. Here we show that the betweenness centrality displays a power-law distribution with an exponent $\eta$, which is robust, and use it to classify the scale-free networks. We have observed two universality classes with $\eta \approx 2.2(1)$ and 2.0, respectively. Real-world networks for the former are the protein-interaction networks, the metabolic networks for eukaryotes and bacteria, and the coauthorship network, and those for the latter one are the Internet, the World Wide Web, and the metabolic networks for Archaea. Distinct features of the mass-distance relation, generic topology of geodesics, and resilience under attack of the two classes are identified. Various model networks also belong to either of the two classes, while their degree exponents are tunable.},
  Doi                      = {10.1073/pnas.202301299}
}

@InProceedings{2013:icse:gokhale,
  Title                    = {Inferring likely mappings between {API}s},
  Author                   = {Amruta Gokhale and Vinod Ganapathy and Yogesh Padmanaban},
  Booktitle                = icse,
  Year                     = {2013},
  Pages                    = {82--91}
}

@InProceedings{2003:icsm:gold,
  Title                    = {A framework for understanding conceptual changes in evolving source code},
  Author                   = {Gold, Nicolas and Mohan, Andrew},
  Booktitle                = icsm,
  Year                     = {2003},
  Pages                    = {431--439},

  Abstract                 = {As systems evolve, they become harder to understand because the implementation of concepts (e.g. business rules) becomes less coherent. To preserve source code comprehensibility, we need to be able to predict how this property will change. This would allow the construction of a tool to suggest what information should be added or clarified (e.g. in comments) to maintain the code's comprehensibility. We propose a framework to characterize types of concept change during evolution. It is derived from an empirical investigation of concept changes in evolving commercial COBOL II files. The framework describes transformations in the geometry and interpretation of regions of source code. We conclude by relating our observations to the types of maintenance performed and suggest how this work could be developed to provide methods for preserving code quality based on comprehensibility.},
  Doi                      = {10.1109/ICSM.2003.1235453}
}

@InProceedings{2007:wsc:goldsman,
  Title                    = {Introduction to simulation},
  Author                   = {Goldsman, David},
  Booktitle                = wsc,
  Year                     = {2007},
  Pages                    = {26--37},

  Abstract                 = {This is an introductory tutorial on the statistical aspects of computer simulation, and is intended to serve as a springboard to many of the other introductory tutorials that appear elsewhere in the Proceedings. We present a number of motivational examples, followed by material on random number and random variate generation, input analysis of the random variables that drive a simulation, and output analysis of the random observations that a simulation produces.}
}

@InProceedings{2007:cbr:gomes,
  Title                    = {Helping Software Engineers Reusing {UML} Class Diagrams},
  Author                   = {P. Gomes and P. Gandola and J. Cordeiro},
  Booktitle                = cbr,
  Year                     = {2007},
  Pages                    = {449--462},
  Series                   = lncs,
  Volume                   = {4626},

  Abstract                 = {Software development is a knowledge-intensive task, with an increasing demand for higher productivity. During the design phase, the use of visual modelling languages like UML (Unified Modeling Language) are wide spread across the software industry. In this paper we present a CBR tool that helps the software engineers to reuse UML diagrams. We describe our system, REBUILDER UML, and present experimental work showing that our system decreases the number of errors made by software engineers during the design of UML diagrams.},
  Doi                      = {10.1007/978-3-540-74141-1_31}
}

@InProceedings{2003:cbr:gomes,
  Title                    = {Solution Verification in Software Design: A {CBR} Approach},
  Author                   = {P. Gomes and F. C. Pereira and P. Carreiro and P. Paiva and N. Seco and J. L. Ferreira and C. Bento},
  Booktitle                = cbr,
  Year                     = {2003},
  Pages                    = {171--185},
  Series                   = lncs,
  Volume                   = {2689},

  Abstract                 = {Software design is becoming a demanding task, not only because the complexity of software systems is increasing, but also due to the pressure that development teams suffer from clients. CASE tools capable of performing more work and of having more intelligent abilities are needed, so that, they can provide more help to the designer. In this paper we describe a CASE tool capable of assisting the designer in a more intelligent way, be it by suggesting new solutions or by learning user preferences. We detail how the solutions are generated and focus on the verification process, which enables the new designs to have less errors. This verification process takes a CBR approach, which has the advantage of being personalized. We describe experimental results that show the effect of the verification process in the generated solutions.},
  Doi                      = {10.1007/3-540-45006-8_16}
}

@Book{2005:book:gosling,
  Title                    = {The Java Language Specification},
  Author                   = {James Gosling and Bill Joy and Guy Steele and Gilad Bracha},
  Publisher                = {Addison-Wesley},
  Year                     = {2005},
  Edition                  = {3rd}
}

@Book{2012:book:gosling,
  Title                    = {The Java Language Specification},
  Author                   = {James Gosling and Bill Joy and Guy Steele and Gilad Bracha and Alex Buckley},
  Publisher                = {Addison-Wesley},
  Year                     = {2012},
  Edition                  = {{Java SE 7}},

  Url                      = {http://docs.oracle.com/javase/specs/jls/se7/html/index.html}
}

@Article{1991:tse:gouda,
  Title                    = {Adaptive programming},
  Author                   = {Mohamed G. Gouda and Ted Herman},
  Journal                  = tse,
  Year                     = {1991},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {911--921},
  Volume                   = {17},

  Abstract                 = {An adaptive program is one that changes its behavior base on the current state of its environment. This notion of adaptivity is formalized, and a logic for reasoning about adaptive programs is presented. The logic includes several composition operators that can be used to define an adaptive program in terms of given constituent programs; programs resulting from these compositions retain the adaptive properties of their constituent programs. The authors begin by discussing adaptive sequential programs, then extend the discussion to adaptive distributed programs. The relationship between adaptivity and self-stabilization is discussed. A case study for constructing an adaptive distributed program where a token is circulated in a ring of processes is presented.},
  Doi                      = {10.1109/32.92911}
}

@InProceedings{1992:usenixcpp:grass,
  Title                    = {Cdiff: A Syntax Directed Diff for {C++} Programs},
  Author                   = {Judith E. Grass},
  Booktitle                = usenixcpp,
  Year                     = {1992},
  Note                     = {No apparent electronic version is available.},
  Pages                    = {181--193},

  Abstract                 = {TBD}
}

@InProceedings{1990:usenixcpp:grass,
  Title                    = {The {C++} Information Abstractor},
  Author                   = {Judith E. Grass and
 Yih-Farn Chen},
  Booktitle                = usenixcpp,
  Year                     = {1990},
  Pages                    = {265--278}
}

@InProceedings{2007:ecoop:greenwood,
  Title                    = {On the Impact of Aspectual Decompositions on Design Stability: An Empirical Study},
  Author                   = {Phil Greenwood and Thiago Bartolomei and Eduardo Figueiredo and Marcos Dosea and Alessandro Garcia and Nelio Cacho and Cl{\'a}udio Sant'Anna and Sergio Soares and Paulo Borba and Uir{\'a} Kulesza and Awais Rashid},
  Booktitle                = ecoop,
  Year                     = {2007},
  Pages                    = {176--200},
  Series                   = lncs,
  Volume                   = {4609},

  Abstract                 = {Although one of the main promises of aspect-oriented (AO) programming techniques is to promote better software changeability than objectoriented (OO) techniques, there is no empirical evidence on their efficacy to prolong design stability in realistic development scenarios. For instance, no investigation has been performed on the effectiveness of AO decompositions to sustain overall system modularity and minimize manifestation of ripple-effects in the presence of heterogeneous changes. This paper reports a quantitative case study that evolves a real-life application to assess various facets of design stability of OO and AO implementations. Our evaluation focused upon a number of system changes that are typically performed during software maintenance tasks. They ranged from successive re-factorings to more broadly-scoped software increments relative to both crosscutting and non-crosscutting concerns. The study included an analysis of the application in terms of modularity, change propagation, concern interaction, identification of ripple-effects and adherence to well-known design principles.}
}

@Misc{1999:misc:gregory,
  Title                    = {Architectural Evolution: A Case Study of {A}pache},

  Author                   = {Richard Gregory and Ladan Tahvildari},
  HowPublished             = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.5886\&rep=rep1\&type=pdf},
  Month                    = apr,
  Note                     = {Department of Electrical and Computer Engineering, University of Waterloo},
  Year                     = {1999},

  Abstract                 = {This paper presents a high level view of the architecture as it existed throughout the life of a growing software system, namely the Apache Web Server. Its goal is to show the impact that evolution has had on the architecture. We present a brief introduction to the subject of Software Architecture and provide an overview of Apache and the environment in which it operates. We also describe our methodology in analyzing the Apache web server, across three different versions. Having presented our methods, we examine in detail the architecture as it existed in each version. Following a discussion of our findings, we conclude that, in general, the architecture of Apache has matured gracefully.},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.5886&rep=rep1&type=pdf}
}

@InProceedings{2000:seke:gresse_von_wangenheim,
  Title                    = {Goal-oriented and similarity-based retrieval of software engineering experienceware},
  Author                   = {Gresse von Wangenheim, Christiane and Klaus-Dieter Althoff and Ricardo M. Barcia},
  Booktitle                = seke,
  Year                     = {2000},
  Pages                    = {118--141},
  Series                   = lncs,
  Volume                   = {1756},

  Abstract                 = {For the successful reuse of software engineering know-how in practice, useful and appropriate experienceware has to be retrieved from a corporate memory. As support is required for different processes, purposes, and environments, with usefulness of retrieved experiences depends mainly on the particular reuse situation. Thus, a flexible retrieval method and similarity measure is required, which can continuously be tailored to specific situations based on feedback from its application in practice. This paper proposes a case-based approach for the retrieval of software engineering experienceware taking into account those specific characteristics of the software engineering domain, such as the lack of explicit domain models in practice, diversity of environments and software processes to be experiences. The approach is illustrated through its application in the REMEX system, a prototypical Experience Base application for the experiencebased support of the planning of software measurement programs.},
  Doi                      = {10.1007/BFb0101417}
}

@InProceedings{2006:fatesrv:grieskamp,
  Title                    = {Multi-paradigmatic Model-Based Testing},
  Author                   = {Wolfgang Grieskamp},
  Booktitle                = fatesrv,
  Year                     = {2006},
  Pages                    = {1--19},
  Series                   = lncs,
  Volume                   = {4262},

  Abstract                 = {For half a decade model-based testing has been applied at Microsoft in the internal development process. Though a success story compared to other formal quality assurance approaches like verification, a break-through of the technology on a broader scale is not in sight. What are the obstacles? Some lessons can be learned from the past and will be discussed. An approach to MBT is described which is based on multi-paradigmatic modeling, which gives users the freedom to choose among programmatic and diagrammatic notations, as well as state-based and scenario-based (interaction-based) styles, reflecting the different concerns in the process. The diverse model styles can be combined by model composition in order to achieve an integrated and collaborative model-based testing process. The approach is realized in the successor of Microsoft Research's MBT tool Spec Explorer, and has a formal foundation in the framework of action machines.}
}

@InProceedings{2006:pldi:grimm,
  Title                    = {Better extensibility through modular syntax},
  Author                   = {R. Grimm},
  Booktitle                = pldi,
  Year                     = {2006},
  Pages                    = {38--51},

  Abstract                 = {We explore how to make the benefits of modularity available for syntactic specifications and present Rats!, a parser generator for Java that supports easily extensible syntax. Our parser generator builds on recent research on parsing expression grammars (PEGs), which, by being closed under composition, prioritizing choices, supporting unlimited lookahead, and integrating lexing and parsing, offer an attractive alternative to context-free grammars. PEGs are implemented by so-called packrat parsers, which are recursive descent parsers that memoize all intermediate results (hence their name). Memoization ensures linear-time performance in the presence of unlimited lookahead, but also results in an essentially lazy, functional parsing technique. In this paper, we explore how to leverage PEGs and packrat parsers as the foundation for extensible syntax. In particular, we show how make packrat parsing more widely applicable by implementing this lazy, functional technique in a strict, imperative language, while also generating better performing parsers through aggressive optimizations. Next, we develop a module system for organizing, modifying, and composing large-scale syntactic specifications. Finally, we describe a new technique for managing (global) parsing state in functional parsers. Our experimental evaluation demonstrates that the resulting parser generator succeeds at providing extensible syntax. In particular, Rats! enables other grammar writers to realize real-world language extensions in little time and code, and it generates parsers that consistently out-perform parsers created by two GLR parser generators.}
}

@Article{1998:tse:griswold,
  Title                    = {Tool Support for Planning the Restructuring of Data Abstractions in Large Systems},
  Author                   = {Griswold, William G. and Chen, Morison I. and Bowdidge, Robert W. and Cabaniss, Jenny L. and Nguyen, Van B. and Morgenthaler, J. David},
  Journal                  = tse,
  Year                     = {1998},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {534--558},
  Volume                   = {24},

  Abstract                 = {Restructuring software to improve its design can lower software maintenance costs. One problem encountered during restructuring is formulating the new design. A meaning-preserving program restructuring tool with a star diagram manipulable visualization can help a programmer redesign a program based on abstract data types. However, the transformational support required for meaning-preserving restructuring is costly to provide. Also, programmers encounter comprehension and recall difficulties in complex restructuring tasks. Consequently, transformations were replaced with visual and organizational aids that help a programmer to plan and carry out a complex restructuring. For example, a star diagram manipulation called trimming was added, which mimics the way that basic restructuring transformations affect the star diagram display, allowing a programmer to plan a restructuring without depending upon restructuring transformations. With the ability to annotate trimmed star diagram components, plans can be recorded and later recalled. Programmer-controlled elision was added to help remove clutter from star diagram views. We implemented a star diagram planning tool for C programs, measured its elision capabilities, and performed a programmer study. We found that elision is effective in controlling star diagram size, and the study revealed that each programming team successfully planned its restructuring in rather different, unanticipated ways. These experiments resulted in important improvements in the tool's software design and user interface.},
  Doi                      = {10.1109/32.708568}
}

@Article{1993:tosem:griswold,
  Title                    = {Automated assistance for program restructuring},
  Author                   = {William G. Griswold and David Notkin},
  Journal                  = tosem,
  Year                     = {1993},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {228--269},
  Volume                   = {2},

  Abstract                 = {Maintenance tends to degrade the structure of software, ultimately making maintenance more costly. At times, then, it is worthwhile to manipulate the structure of a system to make changes easier. However, manual restructuring is an error-prone and expensive activity. By separating structural manipulations from other maintenance activities, the semantics of a system can be held constant by a tool, assuring that no errors are introduced by restructuring. To allow the maintenance team to focus on the aspects of restructuring and maintenance requiring human judgment, a transformation-based tool can be provided---based on a model that exploits preserving data flow dependence and control flow dependence---to automate the repetitive, error-prone, and computationally demanding aspects of restructuring. A set of automatable transformations is introduced; their impact on structure is described, and their usefulness is demonstrated in examples. A model to aid building meaning-preserving restructuring transformations is described, and its realization in a functioning prototype tool for restructuring Scheme programs is discussed.},
  Doi                      = {10.1145/152388.152389}
}

@Article{2010:jvlc:gross,
  Title                    = {Non-programmers identifying functionality in unfamiliar code: Strategies and barriers},
  Author                   = {Gross, Paul and Kelleher, Caitlin},
  Journal                  = jvlc,
  Year                     = {2010},

  Month                    = dec,
  Number                   = {5},
  Pages                    = {263--276},
  Volume                   = {21},

  Abstract                 = {Source code on the web is a widely available and potentially rich learning resource for non-programmers. However, unfamiliar code can be daunting to end-users without programming experience. This paper describes the results of an exploratory study in which we asked non-programmers to find and modify the code responsible for specific functionality within unfamiliar programs. We present two interacting models of how non-programmers approach this problem: the Task Process Model and the Landmark-Mapping model. Using these models, we describe code search strategies non-programmers employed and the barriers they encountered. Finally, we propose guidelines for future programming environments that support non-programmers in finding functionality in unfamiliar programs.},
  Doi                      = {10.1016/j.jvlc.2010.08.002}
}

@InProceedings{2005:snpd_sawn:grunske,
  Title                    = {Formalizing architectural refactorings as graph transformation systems},
  Author                   = {Grunske, Lars},
  Booktitle                = snpd_sawn,
  Year                     = {2005},
  Pages                    = {324--329},

  Abstract                 = {Architectural refactorings are an appropriate technique for the development and improvement of architectural specifications. However, these refactorings are often applied manually. This paper presents a mapping of an architectural specification language to a hypergraph-based data structure. Thus, architectural refactorings can be formalized as hypergraph transformation rules and can be applied automatically.}
}

@InProceedings{2004:wcre:gueheneuc,
  Title                    = {Fingerprinting Design Patterns},
  Author                   = {Gu{\'e}h{\'e}neuc, Yann-Ga{\"e}l and Houari Sahraoui and Zaidi, Farouk},
  Booktitle                = wcre,
  Year                     = {2004},
  Pages                    = {172--181},

  Abstract                 = {Design patterns describe good solutions to common and recurring problems in program design. The solutions are design motifs which software engineers imitate and introduce in the architecture of their program. It is important to identify the design motifs used in a program architecture to understand solved design problems and to make informed changes to the program. The identification of micro-architectures similar to design motifs is difficult because of the large search space, i.e., the many possible combinations of classes. We propose an experimental study of classes playing roles in design motifs using metrics and a machine learning algorithm to fingerprint design motifs roles. Finger-prints are sets of metric values characterising classes playing a given role. We devise fingerprints experimentally using a repository of micro-architectures similar to design motifs. We show that fingerprints help in reducing the search space of micro-architectures similar to design motifs efficiently using the Composite design motif and the JHotDraw framework.}
}

@InProceedings{2010:icse:guo,
  Title                    = {Characterizing and predicting which bugs get fixed: An empirical study of {M}icrosoft {W}indows},
  Author                   = {Guo, Philip J. and Zimmermann, Thomas and Nagappan, Nachiappan and Murphy, Brendan},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {495--504},
  Volume                   = {1},

  Abstract                 = {We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68\% and recall of 64\% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.},
  Doi                      = {10.1145/1806799.1806871}
}

@InProceedings{2011:icsm:guo,
  Title                    = {Tracking technical debt: An exploratory case study},
  Author                   = {Guo, Yuepu and Seaman, Carolyn and Gomes, Rebeka and Cavalcanti, Antonio and Tonin, Graziela and Da Silva, Fabio Q. B. and Santos, Andre L. M. and Siebra, Clauirton},
  Booktitle                = icsm,
  Year                     = {2011},
  Pages                    = {528--531},

  Abstract                 = {The technical debt metaphor is increasingly being used to describe the effect of delaying certain software maintenance tasks on software projects. Practitioners understand intuitively how technical debt can turn into a serious problem if it is left unattended. However, it remains unknown how serious the problem is and whether explicit measurement and management of technical debt is useful. In this paper, we explore the effect of technical debt by tracking a single delayed maintenance task in a real software project throughout its lifecycle and simulate how explicit technical debt management might have changed project outcomes. The results from this study demonstrate how and to what extent technical debt affects software projects. The study also sheds light on the research methodologies that can be used to investigate the technical debt management problem.},
  Doi                      = {10.1109/ICSM.2011.6080824}
}

@Article{2009:ese:gupta,
  Title                    = {A case study comparing defect profiles of a reused framework and of applications reusing it},
  Author                   = {Gupta, Anita and Li, Jingyue and Conradi, Reidar and R{\o}nneberg, Harald and Landre, Einar},
  Journal                  = ese,
  Year                     = {2009},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {227--255},
  Volume                   = {14},

  Doi                      = {10.1007/s10664-008-9081-9}
}

@Article{2010:jips:gupta,
  Title                    = {A Dynamic Approach to Estimate Change Impact using Type of Change Propagation},
  Author                   = {Gupta, Chetna and Singh, Yogesh and Chauhan, Durg Singh},
  Journal                  = jips,
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {597--608},
  Volume                   = {6},

  Doi                      = {10.3745/JIPS.2010.6.4.597}
}

@Article{2002:jss:van_gurp,
  Title                    = {Design erosion: Problems and causes},
  Author                   = {van Gurp, Jilles and Jan Bosch},
  Journal                  = jss,
  Year                     = {2002},

  Month                    = {15 } # mar,
  Number                   = {2},
  Pages                    = {105--119},
  Volume                   = {61},

  Abstract                 = {Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal.},
  Doi                      = {10.1016/S0164-1212(01)00152-2}
}

@Article{2001:spe:van_gurp,
  Title                    = {Design, implementation and evolution of object oriented frameworks: Concepts and guidelines},
  Author                   = {van Gurp, J. and J. Bosch},
  Journal                  = spe,
  Year                     = {2001},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {277--300},
  Volume                   = {31},

  Abstract                 = {Object-oriented frameworks provide software developers with the means to build an infrastructure for their applications. Unfortunately, frameworks do not always deliver on their promises of reusability and flexibility. To address this, we have developed a conceptual model for frameworks and a set of guidelines to build object oriented frameworks that adhere to this model. Our guidelines focus on improving the flexibility, reusability and usability (i.e. making it easy to use a framework) of frameworks.},
  Doi                      = {10.1002/spe.366}
}

@Article{2005:tse:gyimothy,
  Title                    = {Empirical Validation of Object-Oriented Metrics on Open Source Software for Fault Prediction},
  Author                   = {Gyim{\'o}thy, Tibor and Ferenc, Rudolf and Siket, Istv{\'a}n},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {897--910},
  Volume                   = {31},

  Abstract                 = {Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database---called Bugzilla---using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.},
  Doi                      = {10.1109/TSE.2005.112}
}

@InProceedings{2010:icse:gysin,
  Title                    = {Improved social trustability of code search results},
  Author                   = {Gysin, Florian S.},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {513--514},

  Abstract                 = {Search is a fundamental activity in software development. However, to search source code efficiently, it is not sufficient to implement a traditional full text search over a base of source code, human factors have to be taken into account as well. We looked into ways of increasing the search results code trustability by providing and analysing a range of meta data alongside the actual search results.}
}

@InProceedings{2009:ecoop:host,
  Title                    = {Debugging method names},
  Author                   = {Einar W. H{\o}st and Bjarte M. {\O}stvold},
  Booktitle                = ecoop,
  Year                     = {2009},
  Pages                    = {294--317},
  Publisher                = {Springer},
  Series                   = lncs,
  Volume                   = {5653},

  Abstract                 = {Meaningful method names are crucial for the readability and maintainability of software. Existing naming conventions focus on syntactic details, leaving programmers with little or no support in assuring meaningful names. In this paper, we show that naming conventions can go much further: we can mechanically check whether or not a method name and implementation are likely to be good matches for each other. The vast amount of software written in Java defines an implicit convention for pairing names and implementations. We exploit this to extract rules for method names, which are used to identify ``naming bugs'' in well-known Java applications. We also present an approach for automatic suggestion of more suitable names in the presence of mismatch between name and implementation.},
  Doi                      = {10.1007/978-3-642-03013-0_14}
}

@Article{1986:tse:habermann,
  Title                    = {Gandalf: Software Development Environments},
  Author                   = {A. Nico Habermann and
 David Notkin},
  Journal                  = tse,
  Year                     = {1986},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {1117--1127},
  Volume                   = {12},

  Doi                      = {10.1109/TSE.1986.6313007}
}

@Article{2008:ms:haefliger,
  Title                    = {Code Reuse in Open Source Software},
  Author                   = {Haefliger, Stefan and von Krogh, Georg and Spaeth, Sebastian},
  Journal                  = mansci,
  Year                     = {2008},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {180--193},
  Volume                   = {54},

  Abstract                 = {Code reuse is a form of knowledge reuse in software development that is fundamental to innovation in many fields. However, to date there has been no systematic investigation of code reuse in open source software projects. This study uses quantitative and qualitative data gathered from a sample of six open source software projects to explore two sets of research questions derived from the literature on software reuse in firms and open source software development. We find that code reuse is extensive across the sample and that open source software developers, much like developers in firms, apply tools that lower their search costs for knowledge and code, assess the quality of software components, and have incentives to reuse code. Open source software developers reuse code because they want to integrate functionality quickly, because they want to write preferred code, because they operate under limited resources in terms of time and skills, and because they can mitigate development costs through code reuse.},
  Doi                      = {10.1287/mnsc.1070.0748}
}

@InProceedings{2010:icse:haiduc,
  Title                    = {Supporting program comprehension with source code summarization},
  Author                   = {Haiduc, Sonia and Aponte, Jairo and Marcus, Andrian},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {223--226},
  Volume                   = {2},

  Abstract                 = {One of the main challenges faced by today's developers is keeping up with the staggering amount of source code that needs to be read and understood. In order to help developers with this problem and reduce the costs associated with it, one solution is to use simple textual descriptions of source code entities that developers can grasp easily, while capturing the code semantics precisely. We propose an approach to automatically determine such descriptions, based on automated text summarization technology.}
}

@InProceedings{2008:icpc:haiduc,
  Title                    = {On the Use of Domain Terms in Source Code},
  Author                   = {Haiduc, Sonia and Marcus, Andrian},
  Booktitle                = icpc,
  Year                     = {2008},
  Pages                    = {113--122},

  Abstract                 = {Information about the problem domain of the software and the solution it implements is often embedded by developers in comments and identifiers. When using software developed by others or when are new to a project, programmers know little about how domain information is reflected in the source code. Programmers often learn about the domain from external sources such as books, articles, etc. Hence, it is important to use in comments and identifiers terms that are commonly known in the domain literature, as it is likely that programmers will use such terms when searching the source code. The paper presents a case study that investigated how domain terms are used in comments and identifiers. The study focused on three research questions: (1) to what degree are domain terms found in the source code of software from a particular problem domain?; (2) which is the preponderant source of domain terms: identifiers or comments?; and (3) to what degree are domain terms shared between several systems from the same problem domain? Within the studied software, we found that in average: 42\% of the domain terms were used in the source code; 23\% of the domain terms used in the source code are present in comments only, whereas only 11\% in the identifiers alone, and there is a 63\% agreement in the use of domain terms between any two software systems.}
}

@Article{2013:scp:halder,
  Title                    = {Abstract program slicing on dependence condition graphs},
  Author                   = {Raju Halder and Agostino Cortesi},
  Journal                  = scp,
  Year                     = {2013},

  Month                    = {1 } # sep,
  Number                   = {9},
  Pages                    = {1240--1263},
  Volume                   = {78},

  Abstract                 = {Context: Mastroeni and Zanardini introduced the notion of semantics-based data dependences, both at concrete and abstract domains, that helps in converting the traditional syntactic Program Dependence Graphs (PDGs) into more refined semanticsbased (abstract) PDGs by disregarding some false dependences from them. As a result, the slicing techniques based on these semantics-based (abstract) PDGs result in more precise slices. Aim: The aim of this paper is to further refine the slicing algorithms when focusing on a given property. Method: The improvement is obtained by (i) applying the notions of semantic relevancy of statements and semantic data dependences, and (ii) combining them with conditional dependences. Result: We provide an abstract slicing algorithm based on semantics-based abstract Dependence Condition Graphs (DCGs) that enable us to identify the conditions for dependences between program points.},
  Doi                      = {10.1016/j.scico.2012.05.007}
}

@InProceedings{2005:ase:halfond,
  Title                    = {{AMNESIA}: Analysis and Monitoring for NEutralizing {SQL}-Injection Attacks},
  Author                   = {Halfond, William G. J. and Orso, Alessandro},
  Booktitle                = ase,
  Year                     = {2005},
  Pages                    = {174--183},

  Doi                      = {10.1145/1101908.1101935}
}

@InProceedings{2006:fse:halfond,
  Title                    = {Using positive tainting and syntax-aware evaluation to counter {SQL} injection attacks},
  Author                   = {William G. J. Halfond and Alessandro Orso and Panagiotis Manolios},
  Booktitle                = fse,
  Year                     = {2006},
  Pages                    = {175--185},

  Abstract                 = {SQL injection attacks pose a serious threat to the security of Web applications because they can give attackers unrestricted access to databases that contain sensitive information. In this paper, we propose a new, highly automated approach for protecting existing Web applications against SQL injection. Our approach has both conceptual and practical advantages over most existing techniques. From the conceptual standpoint, the approach is based on the novel idea of positive tainting and the concept of syntax-aware evaluation. From the practical standpoint, our technique is at the same time precise and efficient and has minimal deployment requirements. The paper also describes wasp, a tool that implements our technique, and a set of studies performed to evaluate our approach. In the studies, we used our tool to protect several Web applications and then subjected them to a large and varied set of attacks and legitimate accesses. The evaluation was a complete success: wasp successfully and efficiently stopped all of the attacks without generating any false positives.}
}

@Article{2009:sigkddex:hall,
  Title                    = {The {WEKA} Data Mining Software: An Update},
  Author                   = {Mark Hall and Eibe Frank and Geoffrey Holmes and Bernhard Pfahringer and Peter Reutemann and Ian H. Witten},
  Journal                  = sigkddex,
  Year                     = {2009},

  Month                    = jun,
  Number                   = {1},
  Pages                    = {10--18},
  Volume                   = {11},

  Doi                      = {10.1145/1656274.1656278}
}

@Article{1980:csur:hall,
  Title                    = {Approximate String Matching},
  Author                   = {Hall, Patrick A. V. and Dowling, Geoff R.},
  Journal                  = csur,
  Year                     = {1980},

  Month                    = dec,
  Number                   = {4},
  Pages                    = {381--402},
  Volume                   = {12},

  Abstract                 = {Approximate matching of strings is reviewed with the aim of surveying techniques suitable for finding an item in a database when there may be a spelling mistake or other error in the keyword. The methods found are classified as either equivalence or similarity problems. Equivalence problems are seen to be readily solved using canonical forms. For similarity problems difference measures are surveyed, with a full description of the well-established dynamic programming method relating this to the approach using probabilities and likelihoods. Searches for approximate matches in large sets using a difference function are seen to be an open problem still, though several promising ideas have been suggested. Approximate matching (error correction) during parsing is briefly reviewed.},
  Doi                      = {10.1145/356827.356830}
}

@Article{2012:tse:hall,
  Title                    = {A systematic literature review on fault prediction performance in software engineering},
  Author                   = {Tracy Hall and Sarah Beecham and David Bowes and David Gray and Steve Counsell},
  Journal                  = tse,
  Year                     = {2012},

  Month                    = nov # {/} # dec,
  Number                   = {6},
  Pages                    = {1276--1304},
  Volume                   = {38},

  Abstract                 = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
  Doi                      = {10.1109/TSE.2011.103}
}

@Article{2002:wos:halloran,
  Title                    = {High Quality and Open Source Software Practices},
  Author                   = {T. J. Halloran and William L. Scherlis},
  Journal                  = wos,
  Year                     = {2002},

  Abstract                 = {Surveys suggest that, according to various metrics, the quality and dependability of today{\textquoteright}s open source software is roughly on par with commercial and government developed software. What are the prospects for advancing to much higher levels of quality in open source software? More specifically, what attributes must be possessed by quality-related interventions for them to be feasibly adoptable in open source practice? In order to identify some of these attributes, we conducted a preliminary survey of the quality practices of a number of successful open source projects. We focus, in particular, on attributes related to adoptability by the open source practitioner community.},
  Url                      = {http://flosshub.org/content/high-quality-and-open-source-software-practices}
}

@InProceedings{2006:cscw:halverson,
  Title                    = {Designing task visualizations to support the coordination of work in software development},
  Author                   = {Christine A. Halverson and Jason B. Ellis and Catalina Danis and Wendy A. Kellogg},
  Booktitle                = cscw,
  Year                     = {2006},
  Pages                    = {39--48},

  Abstract                 = {Software development tools primarily focus on supporting the technical work. Yet no matter the tools employed, the process followed, or the size of the team, important aspects of development are non-technical, and largely unsupported. For example, increasing distribution of development teams highlights the issues of coordination and cooperation. This paper focuses on one area: managing change requests. Interviews with industry and open-source programmers were used to create designs for the visual inspection of change requests. This paper presents fieldwork findings and two designs. We conclude by reflecting on the issues that task visualizations that support social inferences address in software development.}
}

@Article{2012:patreclet:hancock,
  Title                    = {Pattern analysis with graphs: Parallel work at {B}ern and {Y}ork},
  Author                   = {Hancock, Edwin R. and Wilson, Richard C.},
  Journal                  = patreclet,
  Year                     = {2012},

  Month                    = may,
  Number                   = {7},
  Pages                    = {833--841},
  Volume                   = {33},

  Abstract                 = {This paper provides a review of recent work on the analysis of graphs, focussing in depth and comparing work by the Bern and York groups. The paper also offers directions for future investigation.},
  Doi                      = {10.1016/j.patrec.2011.08.012}
}

@InProceedings{2005:aosd:hannemann,
  Title                    = {Role-based refactoring of crosscutting concerns},
  Author                   = {Hannemann, Jan and Murphy, Gail C. and Kiczales, Gregor},
  Booktitle                = aosd,
  Year                     = {2005},
  Pages                    = {135--146},

  Abstract                 = {Improving the structure of code can help developers work with a software system more efficiently and more consistently. To aid developers in re-structuring the implementation of crosscutting concerns using aspect-oriented programming, we introduce a role-based refactoring approach and tool. Crosscutting concerns (CCCs) are described in terms of abstract roles, and instructions for refactoring crosscutting concerns are written in terms of those roles. To apply a refactoring, a developer maps a subset of the roles to concrete program elements; a tool can then help complete the mapping of roles to the existing program. Refactoring instructors are then applied to manipulate and modularize the concrete elements corresponding to the crosscutting concern. Evaluation of the prototype tool on a graphical editing framework suggests that the approach helps planning and executing complex CCC refactorings.}
}

@InProceedings{1993:sac:harandi,
  Title                    = {The role of analogy in software reuse},
  Author                   = {Mehdi T. Harandi},
  Booktitle                = sac,
  Year                     = {1993},
  Pages                    = {40--47},

  Abstract                 = {Analogical reasoning can enhance software reusability by serving as a mechanism for retrieval of nwsable artifacts, and by assisting the developer in the adaptation and modification of a candidate mmsable component. In this paper we examine issues of analogical reasoning as they apply to software reuse and present two examples of the use of analogy in software reuse.},
  Doi                      = {10.1145/162754.162795}
}

@InProceedings{2006:aosd:harbulot,
  Title                    = {A join point for loops in {AspectJ}},
  Author                   = {Harbulot, Bruno and Gurd, John R.},
  Booktitle                = aosd,
  Year                     = {2006},
  Pages                    = {63--74},

  Doi                      = {10.1145/1119655.1119666}
}

@InProceedings{2013:wcre:harman,
  Title                    = {Genetic Programming for Reverse Engineering},
  Author                   = {Mark Harman and William B. Langdon and Westley Weimer},
  Booktitle                = wcre,
  Year                     = {2013},
  Pages                    = {1--10},

  Doi                      = {10.1109/WCRE.2013.6671274}
}

@InProceedings{1998:metrics:harrison,
  Title                    = {Coupling metrics for object-oriented design},
  Author                   = {R. Harrison and S. Counsell and R. Nithi},
  Booktitle                = metrics,
  Year                     = {1998},
  Pages                    = {150--157},

  Abstract                 = {We describe and evaluate some recently innovated coupling metrics for object-oriented (OO) design. The Coupling Between Objects (CBO) metric of Chidamber and Kemerer (1991) is evaluated empirically using five OO systems, and compared with an alternative OO design metric called SAS, which measures the number of associations between a class and its peers. The NAS metric is directly collectible from design documents such as the Object Model of OMT. Results from all systems studied indicate a strong relationship between CBO and NAS, suggesting that they are not orthogonal. We hypothesised that coupling would be related to understandability, the number of errors and error density. So relationships were found for any of the systems between class understandability and coupling. Only limited evidence was found to support our hypothesis linking increased coupling to increased error density. The work described in this paper is part of the `Metrics for OO Programming Systems' (MOOPS) project, which aims to evaluate existing OO metrics, and to innovate and evaluate new OO analysis and design metrics, aimed specifically at the early stages of development.}
}

@InProceedings{1997:step:harrison,
  Title                    = {An overview of object-oriented design metrics},
  Author                   = {R. Harrison and S. Counsell and R. Nithi},
  Booktitle                = step,
  Year                     = {1997},
  Pages                    = {230--235},

  Abstract                 = {In this paper, we examine the current state in the field of object-oriented design metrics. We describe three sets of currently available metrics suites, namely, those of Chidamber and Kemerer, Lorenz and Kidd and Abreu. We consider the important features of each set, and assess the appropriateness and usefulness of each in evaluating the design of object-oriented systems. As a result, we identify problems common to all three sets of metrics, allowing us to suggest possible improvements in this area.}
}

@InProceedings{1998:icse:harrold,
  Title                    = {Reuse-driven interprocedural slicing},
  Author                   = {Harrold, Mary Jean and Ci, Ning},
  Booktitle                = icse,
  Year                     = {1998},
  Pages                    = {74--83},

  Abstract                 = {To manage the evolution of software systems effectively, software developers must understand software systems, identify and evaluate alternative modification strategies, implement appropriate modifications, and validate the correctness of the modifications. One analysis technique that assists in many of these activities is program slicing. To facilitate the application of slicing to large software systems, we adapted a control flow-based interprocedural slicing algorithm so that it accounts for interprocedural control dependencies not recognized by other slicing algorithms, and reuses slicing information for improved efficiency. Our initial studies suggest that additional slice accuracy and slicing efficiency may be achieved with our algorithm.},
  Doi                      = {10.1109/ICSE.1998.671104}
}

@InProceedings{2010:chi:hartmann:a,
  Title                    = {d.note: Revising user interfaces through change tracking, annotations, and alternatives},
  Author                   = {Hartmann, Bj{\"o}rn and Follmer, Sean and Ricciardi, Antonio and Cardenas, Timothy and Klemmer, Scott R.},
  Booktitle                = chi,
  Year                     = {2010},
  Pages                    = {493--502},

  Abstract                 = {Interaction designers typically revise user interface prototypes by adding unstructured notes to storyboards and screen printouts. How might computational tools increase the efficacy of UI revision? This paper introduces d.note, a revision tool for user interfaces expressed as control flow diagrams. d.note introduces a command set for modifying and annotating both appearance and behavior of user interfaces; it also defines execution semantics so proposed changes can be tested immediately. The paper reports two studies that compare production and interpretation of revisions in d.note to freeform sketching on static images (the status quo). The revision production study showed that testing of ideas during the revision process led to more concrete revisions, but that the tool also affected the type and number of suggested changes. The revision interpretation study showed that d.note revisions required fewer clarifications, and that additional techniques for expressing revision intent could be beneficial.},
  Doi                      = {10.1145/1753326.1753400}
}

@InProceedings{2010:chi:hartmann:b,
  Title                    = {What would other programmers do: Suggesting solutions to error messages},
  Author                   = {Hartmann, Bj\"{o}rn and MacDougall, Daniel and Brandt, Joel and Klemmer, Scott R.},
  Booktitle                = chi,
  Year                     = {2010},
  Pages                    = {1019--1028},

  Abstract                 = {Interpreting compiler errors and exception messages is challenging for novice programmers. Presenting examples of how other programmers have corrected similar errors may help novices understand and correct such errors. This paper introduces HelpMeOut, a social recommender system that aids the debugging of error messages by suggesting solutions that peers have applied in the past. HelpMeOut comprises IDE instrumentation to collect examples of code changes that fix errors; a central database that stores fix reports from many users; and a suggestion interface that, given an error, queries the database for a list of relevant fixes and presents these to the programmer. We report on implementations of this architecture for two programming languages. An evaluation with novice programmers found that the technique can suggest useful fixes for 47\% of errors after 39 person-hours of programming in an instrumented environment.},
  Doi                      = {10.1145/1753326.1753478}
}

@Article{2010:eceasst:hashmi,
  Title                    = {Methodologies and Tools for {OSS}: Current State of the Practice},
  Author                   = {Zulqarnain Hashmi and
 Siraj Ahmed Shaikh and
 Naveed Ikram},
  Journal                  = eceasst,
  Year                     = {2010},
  Pages                    = {5/1--5/11},
  Volume                   = {33},

  Abstract                 = {Over the years, the Open Source Software (OSS) development has matured and strengthened, building on some established methodologies and tools. An understanding of the current state of the practice, however, is still lacking. This paper presents the results of a survey of the OSS developer community with a view to gain insight of peer review, testing and release management practices, along with the current tool sets used for testing, debugging and, build and release management. Such an insight is important to appreciate the obstacles to overcome to introduce certification and more rigour into the development process. It is hoped that the results of this survey will initiate a useful discussion and allow the community to identify further process improvement opportunities for producing better quality software.},
  Url                      = {http://journal.ub.tu-berlin.de/eceasst/article/view/453}
}

@PhdThesis{1995:phd:hasker,
  Title                    = {The Replay of Program Derivations},
  Author                   = {Robert W. Hasker},
  School                   = {University of Illinois at Urbana--Champaign},
  Year                     = {1995}
}

@InProceedings{2012:csmr:hassaine,
  Title                    = {{ADvISE}: Architectural Decay in Software Evolution},
  Author                   = {Hassaine, S. and Gu{\'e}h{\'e}neuc, Y.-G. and Hamel, S. and Antoniol, G.},
  Booktitle                = csmr,
  Year                     = {2012},
  Pages                    = {267--276},

  Abstract                 = {Software systems evolve continuously, requiring continuous maintenance and development. Consequently, their architecture tends to degrade with time as it becomes less relevant to new, emerging requirements. Therefore, stability or resilience is a primary criterion for evaluating an architecture. In this paper, we propose a quantitative approach to study the evolution of the architecture of object oriented systems over time. In particular, we represent an architecture as a set of triplets (S, R, T), where S and T represent two classes and R is a relationship linking them. We use these triplets as basic unit to measure the stability of an architecture. We show the applicability and usefulness of our approach by studying the evolution of three open source systems: JFree Chart and Xerces-J and Rhino.},
  Doi                      = {10.1109/CSMR.2012.34}
}

@InProceedings{2010:rcis:hassan,
  Title                    = {A knowledge-based system for change impact analysis on software architecture},
  Author                   = {Hassan, Mohamed Oussama and Deruelle, Laurent and Basson, Henri},
  Booktitle                = rcis,
  Year                     = {2010},
  Pages                    = {545--556},

  Abstract                 = {This paper deals with the change impact analysis for software architectures evolution. Some approaches have been proposed for integrating the evolution issue at the architectural level. However, none of these studies the impact assessment between the software architecture and its related source code. To deal with that, we propose a model, called ASCM (Architectural Software Components Model), on which we define our change propagation process. Our model describes the common elements defined in an architecture description, independently of architecture description languages (ADLs). The change propagation process is based on a knowledge-based system, in which the model instances are stored. When a modification is applied on these, propagation rules are fired to simulate the impact on software architecture and on its source code. This is done using a platform developed on the top of Eclipse Environment.},
  Doi                      = {10.1109/RCIS.2010.5507308}
}

@Article{2009:tse:hatton,
  Title                    = {Power-Law Distributions of Component Size in General Software Systems},
  Author                   = {Hatton, Les},
  Journal                  = tse,
  Year                     = {2009},

  Month                    = jul # {/} # aug,
  Number                   = {4},
  Pages                    = {566--572},
  Volume                   = {35},

  Abstract                 = {This paper begins by modeling general software systems using concepts from statistical mechanics which provide a framework for linking microscopic and macroscopic features of any complex system. This analysis provides a way of linking two features of particular interest in software systems: first the microscopic distribution of defects within components and second the macroscopic distribution of component sizes in a typical system. The former has been studied extensively, but the latter much less so. This paper shows that subject to an external constraint that the total number of defects is fixed in an equilibrium system, commonly used defect models for individual components directly imply that the distribution of component sizes in such a system will obey a power-law Pareto distribution. The paper continues by analyzing a large number of mature systems of different total sizes, different implementation languages, and very different application areas, and demonstrates that the component sizes do indeed appear to obey the predicted power-law distribution. Some possible implications of this are explored.},
  Doi                      = {10.1109/TSE.2008.105}
}

@InProceedings{2008:ase:hattori,
  Title                    = {On the nature of commits},
  Author                   = {Hattori, Lile P. and Lanza, Michele},
  Booktitle                = ase,
  Year                     = {2008},
  Pages                    = {63--71},

  Abstract                 = {Information contained in versioning system commits has been frequently used to support software evolution research. Concomitantly, some researchers have tried to relate commits to certain activities, e.g., large commits are more likely to be originated from code management activities, while small ones are related to development activities. However, these characterizations are vague, because there is no consistent definition of what is a small or a large commit. In this paper, we study the nature of commits in two dimensions. First, we define the size of commits in terms of number of files, and then we classify commits based on the content of their comments. To perform this study, we use the history log of nine large open source projects.}
}

@Article{1978:cacm:heckel,
  Title                    = {A technique for isolating differences between files},
  Author                   = {Paul Heckel},
  Journal                  = cacm,
  Year                     = {1978},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {264--268},
  Volume                   = {21},

  Abstract                 = {A simple algorithm is described for isolating the differences between two files. One application is the comparing of two versions of a source program or other file in order to display all differences. The algorithm isolates differences in a way that corresponds closely to our intuitive notion of difference, is easy to implement, and is computationally efficient, with time linear in the file length. For most applications the algorithm isolates differences similar to those isolated by the longest common subsequence. Another application of this algorithm merges files containing independently generated changes into a single file. The algorithm can also be used to generate efficient encodings of a file in the form of the differences between itself and a given ``datum'' file, permitting reconstruction of the original file from the diference and datum files.},
  Doi                      = {10.1145/359460.359467}
}

@InProceedings{1990:ecoop_oopsla:helm,
  Title                    = {Contracts: Specifying Behavioral Compositions in Object-Oriented Systems},
  Author                   = {Richard Helm and Ian M. Holland and Dipayan Gangopadhyay},
  Booktitle                = ecoop_oopsla,
  Year                     = {1990},
  Pages                    = {169--180},

  Abstract                 = {Behavioral compositions, groups of interdependent objects cooperating to accomplish tasks, are an important feature of object-oriented systems. This paper introduces Contracts, a new technique for specifying behavioral compositions and the obligations on participating objects. Refinement and composition of contracts allows for the creation of large grain abstractions based on behavior, orthogonal to those provided by existing class constructs. Using contracts thus provides a basis and vocabulary for Interaction-Oriented design which greatly facilitates the early identification, abstraction and reuse of patterns of behavior in programs. Contracts differ from previous work in that they capture explicitly and abstractly the behavioral dependencies amongst cooperating objects. By explicitly stating these dependencies, contract also provide an effective aid for program understanding and reuse.},
  Doi                      = {10.1145/97945.97967}
}

@Book{1996:book:henderson-sellers,
  Title                    = {Object-Oriented Metrics: Measures of Complexity},
  Author                   = {Brian Henderson-Sellers},
  Publisher                = {Prentice-Hall},
  Year                     = {1996}
}

@InProceedings{2005:icse:henkel,
  Title                    = {{CatchUp!}: Capturing and replaying refactorings to support {API} evolution},
  Author                   = {Johannes Henkel and Amer Diwan},
  Booktitle                = icse,
  Year                     = {2005},
  Pages                    = {274--283},

  Abstract                 = {Library developers who have to evolve a library to accommodate changing requirements often face a dilemma: Either they implement a clean, efficient solution but risk breaking client code, or they maintain compatibility with client code, but pay with increased design complexity and thus higher maintenance costs over time.We address this dilemma by presenting a lightweight approach for evolving application programming interfaces (APIs), which does not depend on version control or configuration management systems. Instead, we capture API refactoring actions as a developer evolves an API. Users of the API can then replay the refactorings to bring their client software components up to date.We present catchup!, an implementation of our approach that captures and replays refactoring actions within an integrated development environment semi-automatically. Our experiments suggest that our approach could be valuable in practice.},
  Doi                      = {10.1145/1062455.1062512}
}

@Article{1997:tosem:henninger,
  Title                    = {An evolutionary approach to constructing effective software reuse repositories},
  Author                   = {Henninger, Scott},
  Journal                  = tosem,
  Year                     = {1997},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {111--140},
  Volume                   = {6},

  Abstract                 = {Repositories for software reuse are faced with two interrelated problems: (1) acquiring the knowledge to initially construct the repository and (2) modifying the repository to meet the evolving and dynamic needs of software development organizations. Current software repository methods rely heavily on classification, which exacerbates acquistition and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively, This article outlines an approach that avoids these problems by choosing a retrieval method that utilizes minimal repository structure to effectively support the process of finding software conponents. The approach is demonstrated through a pair of proof-of-concept prototypes: PEEL, a tool to semiautomatically identify reusable components, and CodeFinder, a retrieval system that compensates for the lack of explicit knowledge structures through a spreading activation retrieval process. CodeFinder also allows component representations to be modified while users are searching for information. This mechanism adapts to the changing nature of the information in the repository and incrementally improves the repository while people use it. The combination of these techniques holds potential for designing software repositories that minimize up-front costs, effectively support the search process, and evolve with an organization's changing needs.},
  Doi                      = {10.1145/248233.248242}
}

@Article{1994:software:henninger,
  Title                    = {Using Iterative Refinement to Find Reusable Software},
  Author                   = {Henninger, Scott},
  Journal                  = software,
  Year                     = {1994},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {48--59},
  Volume                   = {11},

  Abstract                 = {Component libraries are the dominant paradigm for software reuse, but they suffer from a lack of tools that support the problem-solving process of locating relevant components. Most retrieval tools assume that retrieval is a simple matter of matching well-formed queries to a repository. But forming queries can be difficult. A designer's understanding of the problem evolves while searching for a component, and large repositories often use an esoteric vocabulary. CodeFinder is a retrieval system that combines retrieval by reformulation (which supports incremental query construction) and spreading activation (which retrieves items related to the query) to help users find information. I designed it to investigate the hypothesis that this design makes for a more effective retrieval system. My study confirmed that it was more helpful to users seeking relevant information with ill-defined tasks and vocabulary mismatches than other query systems. The study supports the hypothesis that combining techniques effectively satisfies the kind of information needs typically encountered in software design.},
  Doi                      = {10.1109/52.311059}
}

@InProceedings{1991:sigir:henninger,
  Title                    = {Retrieving Software Objects in an Example-Based Programming Environment},
  Author                   = {Scott Henninger},
  Booktitle                = sigir,
  Year                     = {1991},
  Pages                    = {251--260},

  Abstract                 = {Example-based programming is a form of software reuse in which existing code examples are modified to meet current task needs. Example-based programming systems that have enough exampIes to be useful present the problem of finding relevant examples. A prototype system named CodeFinder, which explores issues of retrieving software objects relevant to the design task, is presented. CodeFinder supports human-computer dialogue by providing the means to incrementally construct a query and by providing associative cues that are compatible with human memory retrieval principles.},
  Doi                      = {10.1145/122860.122886}
}

@InProceedings{2003:esec_fse:herbsleb,
  Title                    = {Formulation and preliminary test of an empirical theory of coordination in software engineering},
  Author                   = {Herbsleb, James D. and Mockus, Audris},
  Booktitle                = esec_fse,
  Year                     = {2003},
  Pages                    = {138--137},

  Abstract                 = {Motivated by evidence that coordination and dependencies among engineering decisions in a software project are key to better understanding and better methods of software creation, we set out to create empirically testable theory to characterize and make predictions about coordination of engineering decisions. We demonstrate that our theory is capable of expressing some of the main ideas about coordination in software engineering, such as Conway's law and the effects of information hiding in modular design. We then used software project data to create measures and test two hypotheses derived from our theory. Our results provide preliminary support for our formulations.},
  Doi                      = {10.1145/940071.940091}
}

@InProceedings{2013:icse:herzig,
  Title                    = {It's not a bug, it's a feature: How misclassification impacts bug prediction},
  Author                   = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
  Booktitle                = icse,
  Year                     = {2013},
  Pages                    = {392--401},

  Abstract                 = {In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8\% of all bug reports to be misclassified---that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39\% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.}
}

@InProceedings{2013:msr:herzig,
  Title                    = {The impact of tangled code changes},
  Author                   = {Herzig, Kim and Zeller, Andreas},
  Booktitle                = msr,
  Year                     = {2013},
  Pages                    = {121--130},

  Abstract                 = {When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source JAVA projects, we found up to 15\% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6\% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.}
}

@PhdThesis{2013:phd:herzig,
  Title                    = {Mining and Untangling Change Genealogies},
  Author                   = {Kim Sebastian Herzig},
  School                   = {Universit{\"a}t des Saarlandes},
  Year                     = {2013},

  Address                  = {Saarbr{\"u}cken, Germany},

  Abstract                 = {Developers change source code to add new functionality, fix bugs, or refactor their code. Many of these changes have immediate impact on quality or stability. However, some impact of changes may become evident only in the long term. This thesis makes use of change genealogy dependency graphs modeling dependencies between code changes capturing how earlier changes enable and cause later ones. Using change genealogies, it is possible to: (a) apply formal methods like model checking on version archives to reveal temporal process patterns. Such patterns encode key features of the software process and can be validated automatically: In an evaluation of four open source histories, our prototype would recommend pending activities with a precision of 60--72\%. (b) classify the purpose of code changes. Analyzing the change dependencies on change genealogies shows that change genealogy network metrics can be used to automatically separate bug fixing from feature implementing code changes. (c) build competitive defect prediction models. Defect prediction models based on change genealogy network metrics show competitive prediction accuracy when compared to state-of-the-art defect prediction models. As many other approaches mining version archives, change genealogies and their applications rely on two basic assumptions: code changes are considered to be atomic and bug reports are considered to refer to corrective maintenance tasks. In a manual examination of more than 7,000 issue reports and code changes from bug databases and version control systems of open-source projects, we found 34\% of all issue reports to be misclassified and that up to 15\% of all applied issue fixes consist of multiple combined code changes serving multiple developer maintenance tasks. This introduces bias in bug prediction models confusing bugs and features. To partially solve these issues we present an approach to untangle such combined changes with a mean success rate of 58--90\% after the fact.},
  Url                      = {http://www.st.cs.uni-saarland.de/~kim/wp-content/uploads/2013/05/PhD_Thesis_Kim_Herzig_06_05_2013.pdf}
}

@InProceedings{2011:ase:hill,
  Title                    = {Improving source code search with natural language phrasal representations of method signatures},
  Author                   = {Emily Hill and Lori Pollock and K. Vijay-Shanker},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {524--527},

  Abstract                 = {As software continues to grow, locating code for maintenance tasks becomes increasingly difficult. Software search tools help developers find source code relevant to their maintenance tasks. One major challenge to successful search tools is locating relevant code when the user's query contains words with multiple meanings or words that occur frequently throughout the program. Traditional search techniques, which treat each word individually, are unable to distinguish relevant and irrelevant methods under these conditions. In this paper, we present a novel search technique that uses information such as the position of the query word and its semantic role to calculate relevance. Our evaluation shows that this approach is more consistently effective than three other state of the art search techniques.}
}

@InProceedings{2009:icse:hill,
  Title                    = {Automatically capturing source code context of {NL}-queries for software maintenance and reuse},
  Author                   = {Hill, Emily and Pollock, Lori and Vijay-Shanker, K.},
  Booktitle                = icse,
  Year                     = {2009},
  Pages                    = {232--242},

  Abstract                 = {As software systems continue to grow and evolve, locating code for maintenance and reuse tasks becomes increasingly difficult. Existing static code search techniques using natural language queries provide little support to help developers determine whether search results are relevant, and few recommend alternative words to help developers reformulate poor queries. In this paper, we present a novel approach that automatically extracts natural language phrases from source code identifiers and categorizes the phrases and search results in a hierarchy. Our contextual search approach allows developers to explore the word usage in a piece of software, helping them to quickly identify relevant program elements for investigation or to quickly recognize alternative words for query reformulation. An empirical evaluation of 22 developers reveals that our contextual search approach significantly outperforms the most closely related technique in terms of effort and effectiveness.},
  Doi                      = {10.1109/ICSE.2009.5070524}
}

@InProceedings{2007:ase:hill,
  Title                    = {Exploring the Neighborhood with {D}ora to Expedite Software Maintenance},
  Author                   = {Emily Hill and Lori Pollock and K. Vijay-Shanker},
  Booktitle                = ase,
  Year                     = {2007},
  Pages                    = {14--23},

  Abstract                 = {Completing software maintenance and evolution tasks for today's large, complex software systems can be difficult, often requiring considerable time to understand the system well enough to make correct changes. Despite evidence that successful programmers use program structure as well as identifier names to explore software, most existing program exploration techniques use either structural or lexical identifier information. By using only one type of information, automated tools ignore valuable clues about a developer's intentions---clues critical to the human program comprehension process. In this paper, we present and evaluate a technique that exploits both program structure and lexical information to help programmers more effectively explore programs. Our approach uses structural information to focus automated program exploration and lexical information to prune irrelevant structure edges from consideration. For the important program exploration step of expanding from a seed, our experimental results demonstrate that an integrated lexical-and structural-based approach is significantly more effective than a state-of-the-art structural program exploration technique.},
  Doi                      = {10.1145/1321631.1321637}
}

@InProceedings{2004:ase:hill,
  Title                    = {Automatic Method Completion},
  Author                   = {Rosco Hill and Joe Rideout},
  Booktitle                = ase,
  Year                     = {2004},
  Pages                    = {228--235},

  Abstract                 = {Modern software development environments include tools to help programmers write code efficiently and accurately. For example many integrated development environments include variable name completion, method name completion and recently refactoring tools have been added to some environments. This paper extends the idea of automatic completion to include completion of the body of a method by employing machine learning algorithms on the near duplicate code segments that frequently exist in large software projects.},
  Doi                      = {10.1109/ASE.2004.19}
}

@InProceedings{2007:vissoft:hindle,
  Title                    = {{YARN}: Animating Software Evolution},
  Author                   = {Hindle, Abram and Zhen Ming Jiang and Koleilat, Walid and Godfrey, Michael W. and Holt, Richard C.},
  Booktitle                = vissoft,
  Year                     = {2007},
  Pages                    = {129--136},

  Abstract                 = {A problem that faces the study of software evolution is how to explore the aggregated and cumulative effects of changes that occur within a software system over time. In this paper we describe an approach to modeling, extracting, and animating the architectural evolution of a software system. We have built a prototype tool called YARN (yet another reverse-engineering narrative) that implements our approach; YARN mines the source code changes of the target system, and generates YARN ``balls" (animations) that a viewer can unravel (watch). The animation is based on a static layout of the modules connected by animated edges that model the changing dependencies. The edges can be weighted by the number of dependencies or the importance of the change. We demonstrate our approach by visualizing the evolution of PostgreSQL DBMS.},
  Doi                      = {10.1109/VISSOF.2007.4290711}
}

@InProceedings{2009:qosa:hinsman,
  Title                    = {Achieving Agility through Architecture Visibility},
  Author                   = {Hinsman, Carl and Sangal, Neeraj and Stafford, Judith},
  Booktitle                = qosa,
  Year                     = {2009},
  Pages                    = {116--129},

  Abstract                 = {L.L.Bean is a large retail organization whose development processes must be agile in order to allow rapid enhancement and maintenance of its technology infrastructure. Over the past decade L.L.Bean's software code-base had become brittle and difficult to evolve. An effort was launched to identify and develop new approaches to software development that would enable ongoing agility to support the ever-increasing demands of a successful business. This paper recounts L.L.Bean's effort in restructuring its code-base and adoption of process improvements that support an architecture-based agile approach to development, governance, and maintenance. Unlike traditional refactoring, this effort was guided by an architectural blueprint that was created in a Dependency Structure Matrix where the refactoring was first prototyped before being applied to the actual code base.},
  Doi                      = {10.1007/978-3-642-02351-4_8}
}

@TechReport{2007:tr:hinsman,
  Title                    = {Large Scale Refactoring through Architecture Visibility},
  Author                   = {Carl Hinsman and Neeraj Sangal and Judith Stafford},
  Institution              = {Tufts University},
  Year                     = {2007},

  Address                  = {Medford, Massachusetts, USA},
  Month                    = sep,
  Number                   = {TR-2007-8},

  Abstract                 = {L.L.Bean is a large retail organization whose development processes must be agile in order to allow rapid enhancement and maintenance of its technology infrastructure. Over the past seven years L.L.Bean's Java software code-base had become more difficult to evolve. A corporate-wide effort was launched to identify and develop new approaches to software development that would enable ongoing agility to support the ever increasing demands of a successful business. This paper recounts L.L.Bean's effort in restructuring its current Java code-base and adoption of process improvements that support an architecture-based agile approach to development and maintenance.},
  Url                      = {http://http://www.cs.tufts.edu/tr/techreps/TR-2007-8}
}

@Article{1996:tse:hitz,
  Title                    = {{C}hidamber and {K}emerer's metrics suite: A measurement theory perspective},
  Author                   = {Martin Hitz and Behzad Montazeri},
  Journal                  = tse,
  Year                     = {1996},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {267--271},
  Volume                   = {22},

  Abstract                 = {The metrics suite for object-oriented design put forward by Chidamber and Kemerer [8] is partly evaluated by applying principles of measurement theory. Using the object coupling measure (CBO) as an example, it is shown that failing to establish a sound empirical relation system can lead to deficiencies of software metrics. Similarly, for the object-oriented cohesion measure (LCOM) it is pointed out that the issue of empirical testing the representation condition must not be ignored, even if other validation principles are carefully obeyed. As a by-product, an alternative formulation for LCOM is proposed.},
  Doi                      = {10.1109/32.491650}
}

@Article{1999:ijamt:hlupic,
  Title                    = {Evaluation Framework for Simulation Software},
  Author                   = {V. Hlupic and Z. Irani and R. J. Paul},
  Journal                  = ijamt,
  Year                     = {1999},

  Month                    = may,
  Number                   = {5},
  Pages                    = {366--382},
  Volume                   = {15},

  Abstract                 = {An increase in the use of simulation as a modelling and analysis tool has resulted in a growing number of simulation software products on the market. Because of this, the importance of an adequate approach to simulation software evaluation and selection is apparent. This paper presents a comprehensive framework which can be used for evaluation of simulation software packages. The evaluation criteria are grouped according to their nature, and they can be of practical use to anyone involved in simulation software evaluation and selection.},
  Doi                      = {10.1007/s001700050079}
}

@Article{2005:ist:hochstein,
  Title                    = {Combating architectural degeneration: A survey},
  Author                   = {Lorin Hochstein and Mikael Lindvall},
  Journal                  = ist,
  Year                     = {2005},

  Month                    = jul,
  Number                   = {10},
  Pages                    = {643--656},
  Volume                   = {47},

  Abstract                 = {As software systems evolve over time, they invariably undergo changes that can lead to a degeneration of the architecture. Left unchecked, degeneration may reach a level where a complete redesign is necessary, a task that requires significant effort. In this paper, we present a survey of technologies developed by researchers that can be used to combat degeneration, that is, technologies that can be employed in identifying, treating and researching degeneration. We also discuss the various causes of degeneration and how it can be prevented. },
  Doi                      = {10.1016/j.infsof.2004.11.005}
}

@InProceedings{2001:esec_fse:van_der_hoek,
  Title                    = {Taming architectural evolution},
  Author                   = {van der Hoek, Andr{\'e} and Marija Mikic-Rakic and Roshanak Roshandel and Nenad Medvidovic},
  Booktitle                = esec_fse,
  Year                     = {2001},
  Pages                    = {1--10},

  Abstract                 = {In the world of software development everything evolves. So, then, do software architectures. Unlike source code, for which the use of a configuration management (CM) system is the predominant approach to capturing and managing evolution, approaches to capturing and managing architectural evolution span a wide range of disconnected alternatives. This paper contributes a novel architecture evolution environment, called Mae, which brings together a number of these alternatives. The environment facilitates an incremental design process in which all changes to all architectural elements are integrally captured and related. Key to the environment is a rich system model that combines architectural concepts with those from the field of CM. Not only does this system model form the basis for Mae, but in precisely capturing architectural evolution it also facilitates automated support for several innovative capabilities that rely on the integrated nature of the system model. This paper introduces three of those: the provision of design guidance at the architectural level, the use of specialized software connectors to ensure run-time reliability during component upgrades, and the creation of component-level patches to be applied to deployed system configurations.}
}

@InProceedings{2008:icse:hoffman,
  Title                    = {Towards reusable components with aspects: An empirical study on modularity and obliviousness},
  Author                   = {Kevin J. Hoffman and Patrick Eugster},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {91--100},

  Abstract                 = {The potential of aspect-oriented programming to represent cross-cutting concerns as reusable components has yet to be fully realized. Indeed, authors have detailed significant challenges in creating reusable aspect component libraries. Proposed solutions include restricting the power of aspects upfront, inferring concern interaction, and shaping base code to conform to abstract design rules. Another proposed strategy is to reduce obliviousness in return for increased modularity by extending AspectJ with explicit join points (EJPs). This paper presents the results of an empirical case study that aides in the understanding of the tradeoffs between obliviousness and modularity. We present a refactoring of the exception handling concern for three real-life Java applications to use EJPs instead of oblivious aspects. The empirical differences between this version and an equivalent oblivious version are analyzed. Finally, we present guiding principles on how to strike a favorable balance between obliviousness and modularity.}
}

@InProceedings{1992:ecoop:holland,
  Title                    = {Specifying reusable components using contracts},
  Author                   = {Holland, Ian M.},
  Booktitle                = ecoop,
  Year                     = {1992},
  Pages                    = {287--308},
  Series                   = lncs,
  Volume                   = {615},

  Abstract                 = {Contracts were introduced by Helm et al. as a high level construct for explicitly specifying interactions among groups of objects. This paper describes further developments and application of the Contract construct. We show how Contracts can be used to represent classic algorithms as large grained reusable object oriented abstractions, how these algorithms can be customized through Contract refinement and how they are reused through Contract conformance. The example algorithm used throughout is the classic graph depth first traversal algorithm. This algorithm is represented as a Contract which is then refined to specify algorithms which number connected regions of graphs and which check graphs for cycles. Changes to the Contract language are introduced and we discuss some new problems resulting from the simultaneous reuse of related contracts.},
  Doi                      = {10.1007/BFb0053043}
}

@Book{1992:book:holland,
  Title                    = {Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence},
  Author                   = {J. H. Holland},
  Publisher                = {The MIT Press},
  Year                     = {1992}
}

@Article{2002:physreve:holme,
  Title                    = {Growing scale-free networks with tunable clustering},
  Author                   = {Petter Holme and Beom Jun Kim},
  Journal                  = physreve,
  Year                     = {2002},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {026107:1--026107:4},
  Volume                   = {65},

  Abstract                 = {We extend the standard scale-free network model to include a ``triad formation step." We analyze the geometric properties of networks generated by this algorithm both analytically and by numerical calculations, and find that our model possesses the same characteristics as the standard scale-free networks such as the power-law degree distribution and the small average geodesic length, but with the high clustering at the same time. In our model, the clustering coefficient is also shown to be tunable simply by changing a control parameter---the average number of triad formation trials per time step.},
  Doi                      = {10.1103/PhysRevE.65.026107}
}

@InProceedings{1996:wcre:holt,
  Title                    = {{GASE}: Visualizing software evolution-in-the-large},
  Author                   = {Holt, Ric and Pak, Jason Y.},
  Booktitle                = wcre,
  Year                     = {1996},
  Pages                    = {163--167},

  Abstract                 = {Large and long lived software systems, sometimes called legacy systems, must evolve if they are to remain useful. Too often, it is difficult to control or to understand this evolution. This paper presents an approach to visualizing software structural change. A visualization tool, called GASE (Graphical Analyzer for Software Evolution), has been used to elucidate the architectural changes in a sequence of eleven revisions of an eighty thousand line industrial software system.}
}

@InProceedings{2005:scam:hong,
  Title                    = {Abstract slicing: A new approach to program slicing based on abstract interpretation and model checking},
  Author                   = {Hong, Hyoung Seok and Lee, Insup and Sokolsky, Oleg},
  Booktitle                = scam,
  Year                     = {2005},
  Pages                    = {25--34},

  Abstract                 = {This paper proposes a new approach to program slicing based on abstract interpretation and model checking. First, the notion of abstract slicing is introduced. Abstract slicing extends static slicing with predicates and constraints by using as the program model an abstract state graph, which is obtained by applying predicate abstraction to a program, rather than a flow graph. This leads to a program slice that is more precise and smaller than its static counterpart. Second, a method for performing abstract slicing is developed. It is shown that abstract slicing can be reduced to a least fixpoint computation over formulas in the branching time temporal logic CTL. This enables one to use symbolic model checkers for CTL as an efficient computation engine for abstract slicing. A prototype implementation and experimental results are reported demonstrating the feasibility of the approach.}
}

@InProceedings{2007:ase:hooimeijer,
  Title                    = {Modeling bug report quality},
  Author                   = {Hooimeijer, Pieter and Weimer, Westley},
  Booktitle                = ase,
  Year                     = {2007},
  Pages                    = {34--43},

  Abstract                 = {Software developers spend a significant portion of their resources handling user-submitted bug reports. For software that is widely deployed, the number of bug reports typically outstrips the resources available to triage them. As a result, some reports may be dealt with too slowly or not at all. We present a descriptive model of bug report quality based on a statistical analysis of surface features of over 27,000 publicly available bug reports for the Mozilla Firefox project. The model predicts whether a bug report is triaged within a given amount of time. Our analysis of this model has implications for bug reporting systems and suggests features that should be emphasized when composing bug reports. We evaluate our model empirically based on its hypothetical performance as an automatic filter of incoming bug reports. Our results show that our model performs significantly better than chance in terms of precision and recall. In addition, we show that our modelcan reduce the overall cost of software maintenance in a setting where the average cost of addressing a bug report is more than 2\% of the cost of ignoring an important bug report.},
  Doi                      = {10.1145/1321631.1321639}
}

@Article{2011:jss:hoorn,
  Title                    = {The lonesome architect},
  Author                   = {Hoorn, Johan F. and Farenhorst, Rik and Lago, Patricia and van Vliet, Hans},
  Journal                  = jss,
  Year                     = {2011},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {1424--1435},
  Volume                   = {84},

  Abstract                 = {Although the benefits are well-known and undisputed, sharing architectural knowledge is not something architects automatically do. In an attempt to better understand what architects really do and what kind of support they need for sharing knowledge, we have conducted large-scale survey research. The results of our study indicate that architects can be characterized as rather lonesome decision makers who mainly consume, but neglect documenting and actively sharing architectural knowledge. Acknowledging this nature of architects suggests ways to develop more effective support for architectural knowledge sharing.},
  Doi                      = {10.1016/j.jss.2010.11.909}
}

@Misc{2012:misc:hornik,
  Title                    = {The {R} {FAQ}},

  Author                   = {Kurt Hornik},
  HowPublished             = {http://CRAN.R-project.org/doc/FAQ/R-FAQ.html},
  Year                     = {2012},

  Url                      = {http://CRAN.R-project.org/doc/FAQ/R-FAQ.html}
}

@InProceedings{1990:pldi:horwitz,
  Title                    = {Identifying the semantic and textual differences between two versions of a program},
  Author                   = {Horwitz, Susan},
  Booktitle                = pldi,
  Year                     = {1990},
  Pages                    = {234--245},

  Abstract                 = {Text-based file comparators (e.g., the Unix utility diff), are very general tools that can be applied to arbitrary files. However, using such tools to compare programs can be unsatisfactory because their only notion of change is based on program text rather than program behavior. This paper describes a technique for comparing two versions of a program, determining which program components represents changes, and classifying each changed component as representing either a semantic or a textual change.},
  Doi                      = {10.1145/93542.93574}
}

@InProceedings{2008:icsm:hou,
  Title                    = {Investigating the effects of framework design knowledge in example-based framework learning},
  Author                   = {Daqing Hou},
  Booktitle                = icsm,
  Year                     = {2008},
  Pages                    = {37--46},

  Abstract                 = {Studying example applications is a common approach to learning software frameworks. However, to be truly effective in adapting example solutions with high confidence and accuracy, a developer needs to learn enough about the framework designs. The empirical study described in this paper investigates the effectiveness of a new approach to framework learning, where example-based learning is augmented with instruction on framework designs. Learning framework designs up-front from an instructor helps developers acquire the necessary design knowledge and avoid the time-consuming task of recovering such knowledge from code and other artifacts. The particular question of interest in this study is how characteristics of the framework designs influence project outcome. 11 student projects are analyzed using both qualitative and quantitative methods to characterize the overall reuse practice and to detect salient patterns that address the question. The contribution of this paper is a set of well-supported hypotheses that can be tested in future studies as well as their implications.},
  Doi                      = {10.1109/ICSM.2008.4658052}
}

@InProceedings{2007:etx:hou,
  Title                    = {Studying the evolution of the {E}clipse {J}ava editor},
  Author                   = {Hou, Daqing},
  Booktitle                = etx,
  Year                     = {2007},
  Pages                    = {65--69},

  Abstract                 = {In evolutionary software development, knowing how design evolves with features can be valuable in guiding future projects. It helps answer questions like ``How much upfront design should and can be done?" and ``How and why are designs changed?" To shed light on these questions, we report on a study of the evolution history of the Eclipse Java editor. We find that the MVC-based design was cleanly laid out in the beginning of the project and carefully followed and maintained, which has contributed positively to the continuous growth of the editor features. Although design changes at the individual feature level happened for reasons like extensibility and reusability, they appear to be local and manageable. The AST facility is a key service that enables more than one half of the Java editor features.},
  Doi                      = {10.1145/1328279.1328293}
}

@InProceedings{2001:cascon:hou,
  Title                    = {Towards Specifying Constraints for Object-Oriented Frameworks},
  Author                   = {Daqing Hou and H. James Hoover},
  Booktitle                = cascon,
  Year                     = {2001},
  Pages                    = {5:1--5:14},

  Abstract                 = {Object-oriented frameworks are often hard to learn and use [1, 3]. As a result, software cost rises and quality suffers. Thus the capability to automatically detect errors occurring at the boundary between frameworks and applications is considered crucial to mitigate the problem. This paper introduces the notion of framework constraints and a specification language, FCL (Framework Constraints Language), to formally specify them. Framework constraints are rules that frameworks impose on the code of framework-based applications. The semantics of FCL is primarily based on first order predicate logic and set theory though the syntax is designed to resemble that of programming languages as much as possible. We take examples from the MFC (Microsoft Foundation Classes) framework [19] demonstrating both the nature of framework constraints and the semantics of FCL. Essentially, framework constraints can be regarded as framework-specific typing rules conveyed by the specification language FCL, and thus can be enforced by techniques analogous to those of conventional type checking.}
}

@InProceedings{2011:wcre:hou,
  Title                    = {Exploring the Intent behind {API} Evolution: {A} Case Study},
  Author                   = {Daqing Hou and Xiaojia Yao},
  Booktitle                = wcre,
  Year                     = {2011},
  Pages                    = {131--140},

  Abstract                 = {Reuse has significantly improved software productivity and quality. An application interacts with a reused system through its Application Programming Interfaces (API). To make the life of the application developer easier, it is desirable for the API to be both sufficiently powerful and stable. Unfortunately, in reality APIs inevitably change, to be more powerful or to remove design flaws. This may create additional work for the application developer to adapt to the changed API. Thus, to counter the negative impacts of API evolution, we need to study how and why APIs are evolved. To that end, we performed a detailed analysis of the evolution of a production API. In particular, we categorized the changes to the API according to its domain semantics and design intent. We discussed the implications of our findings for both API designers and application developers.},
  Doi                      = {10.1109/WCRE.2011.24}
}

@InProceedings{2004:msr:howison,
  Title                    = {The perils and pitfalls of mining {SourceForge}},
  Author                   = {James Howison and Kevin Crowston},
  Booktitle                = msrw,
  Year                     = {2004},
  Pages                    = {7--11}
}

@Article{2006:jvlc:hunag,
  Title                    = {Clustering graphs for visualization via node similarities},
  Author                   = {Huang, Xiaodi and Lai, Wei},
  Journal                  = jvlc,
  Year                     = {2006},

  Month                    = jun,
  Number                   = {3},
  Pages                    = {225--253},
  Volume                   = {17},

  Abstract                 = {Graph visualization is commonly used to visually model relations in many areas. Examples include Web sites, CASE tools, and knowledge representation. When the amount of information in these graphs becomes too large, users, however, cannot perceive all elements at the same time. A clustered graph can greatly reduce visual complexity by temporarily replacing a set of nodes in clusters with abstract nodes. This paper proposes a new approach to clustering graphs. The approach constructs the node similarity matrix of a graph that is derived from a novel metric of node similarity. The linkage pattern of the graph is thus encoded into the similarity matrix, and then one obtains the hierarchical abstraction of densely linked subgraphs by applying the k-means algorithm to the matrix. A heuristic method is developed to overcome the inherent drawbacks of the k-means algorithm. For clustered graphs we present a multilevel multi-window approach to hierarchically drawing them in different abstract level views with the purpose of improving their readability. The proposed approaches demonstrate good results in our experiments. As application examples, visualization of part of Java class diagrams and Web graphs are provided. We also conducted usability experiments on our algorithm and approach. The results have shown that the hierarchically clustered graph used in our system can improve user performance for certain types of tasks.},
  Doi                      = {10.1016/j.jvlc.2005.10.003}
}

@PhdThesis{1976:phd:huet,
  Title                    = {R{\'e}solution d'{\'e}quations dans des langages d'ordre 1,2,$\ldots$,$\omega$},
  Author                   = {G{\'e}rard Huet},
  School                   = {Universit{\'e} Paris VII},
  Year                     = {1976},
  Month                    = sep,
  Type                     = {Th{\`e}se de Doctorat {\`e}s Sciences Math{\'e}matiques}
}

@InCollection{1999:book:morgan:hughes,
  Title                    = {The {I}sing Model, Computer Simulation, and Universal Physics},
  Author                   = {R. I. G. Hughes},
  Booktitle                = {Models as Mediators: Perspectives on Natural and Social Science},
  Publisher                = {Cambridge University Press},
  Year                     = {1999},
  Chapter                  = {5},
  Editor                   = {Mary S. Morgan and Margaret Morrison},
  Number                   = {52},
  Series                   = {Ideas in Context},

  Doi                      = {10.1017/CBO9780511660108.006}
}

@InProceedings{2010:suite:hummel,
  Title                    = {Facilitating the comparison of software retrieval systems through a reference reuse collection},
  Author                   = {Hummel, Oliver},
  Booktitle                = suite,
  Year                     = {2010},
  Pages                    = {17--20},

  Abstract                 = {Although the idea of component-based software reuse has been around for more than four decades the technology for retrieving reusable software artefacts has grown out of its infancy only recently. After about 30 years of basic research in which scientists struggled to get their hands on meaningful numbers of reusable artifacts to evaluate their prototypes, the ``open source revolution" has made software reuse a serious practical possibility. Millions of reusable files have become freely available and more sophisticated retrieval tools have emerged providing better ways of searching among them. However, while the development of such systems has made considerable progress, their evaluation is still largely driven by proprietary approaches which are all too often neither comprehensive nor comparable to one another. Hence, in this position paper, we propose the compilation of a reference collection of reusable artifacts in order to facilitate the future evaluation and comparison of software retrieval tools.},
  Doi                      = {10.1145/1809175.1809180}
}

@PhdThesis{2008:phd:hummel,
  Title                    = {Semantic Component Retrieval in Software Engineering},
  Author                   = {Oliver Hummel},
  School                   = {Universit{\"a}t Mannheim},
  Year                     = {2008},

  Address                  = {Mannheim, Germany},

  Abstract                 = {In the early days of programming the concept of subroutines, and through this software reuse, was invented to spare limited hardware resources. Since then software systems have become increasingly complex and developing them would not have been possible without reusable software elements such as standard libraries and frameworks. Furthermore, other approaches commonly subsumed under the umbrella of software reuse such as product lines and design patterns have become very successful in recent years. However, there are still no software component markets available that would make buying software components as simple as buying parts in a do-it-yourself hardware store and millions of software fragments are still lying un(re)used in configuration management repositories all over the world. The literature primarily blames this on the immense effort required so far to set up and maintain searchable component repositories and the weak mechanisms available for retrieving components from them, resulting in a severe usability problem. In order to address these issues within this thesis, we developed a proactive component reuse recommendation system, naturally integrated into test-first development approaches, which is able to propose semantically appropriate, reusable components according to the specification a developer is just working on. We have implemented an appropriate system as a plugin for the well-known Eclipse IDE and demonstrated its usefulness by carrying out a case study from a popular agile development book. Furthermore, we present a precision analysis for our approach and examples of how components can be retrieved based on a simplified semantics description in terms of standard test cases.},
  Url                      = {https://ub-madoc.bib.uni-mannheim.de/1883/1/Dissertation\_OliverHummel.pdf}
}

@InProceedings{2009:icsr:hummel,
  Title                    = {The {M}anaged {A}dapter {P}attern: Facilitating Glue Code Generation for Component Reuse},
  Author                   = {Hummel, Oliver and Atkinson, Colin},
  Booktitle                = icsr,
  Year                     = {2009},
  Pages                    = {211--224},
  Series                   = lncs,
  Volume                   = {5791},

  Abstract                 = {The adapter or wrapper pattern is one of the most widely used patterns in software engineering since the problem of reconciling unsuitable component interfaces is so ubiquitous. However, the classic adapter pattern as described by the Gang of Four has some limitations which rule out its use in certain situations. Of the two forms of the pattern, only the object adapter form is usable with common programming languages not supporting multiple inheritance (such as Java or C\#), and this is not able to adapt interfaces of classes whose own type is used in one or more of their operations. This makes it impossible for a tool to automatically generate ``glue code" for such components and forces developers to come up with some non-trivial (and typically invasive) workarounds to enable clients to use them. In this paper we present an enhanced form of the adapter pattern which solves this problem by extending the way in which an adapter stores and manages adaptees. We therefore call it the Managed Adapter Pattern. After describing the pattern in the usual Gang of Four-oriented way, we describe its application in the system that initially motivated its development---a test-driven component search engine which is able to retrieve reusable assets based on their semantics. A key challenge in the implementation of this engine was developing a flexible glue code generator that was able to automate the creation of adapters for all the kinds of components delivered by the underlying component repository.},
  Doi                      = {10.1007/978-3-642-04211-9\_21}
}

@InProceedings{2007:xp:hummel,
  Title                    = {Supporting Agile Reuse through Extreme Harvesting},
  Author                   = {Oliver Hummel and Colin Atkinson},
  Booktitle                = xp,
  Year                     = {2007},
  Pages                    = {28--37},
  Series                   = lncs,
  Volume                   = {4536},

  Abstract                 = {Agile development and software reuse are both recognized as effective ways of improving time to market and quality in software engineering. However, they have traditionally been viewed as mutually exclusive technologies which are difficult if not impossible to use together. In this paper we show that, far from being incompatible, agile development and software reuse can be made to work together and, in fact, complement each other. The key is to tightly integrate reuse into the test-driven development cycles of agile methods and to use test cases---the agile measure of semantic acceptability---to influence the component search process. In this paper we discuss the issues involved in doing this in association with Extreme Programming, the most widely known agile development method, and Extreme Harvesting, a prototype technique for the test-driven harvesting of components from the Web. When combined in the appropriate way we believe they provide a good foundation for the fledgling concept of agile reuse.},
  Doi                      = {10.1007/978-3-540-73101-6\_5}
}

@InProceedings{2004:iri:hummel,
  Title                    = {Extreme Harvesting: Test Driven Discovery and Reuse of Software Components},
  Author                   = {Oliver Hummel and Colin Atkinson},
  Booktitle                = iri,
  Year                     = {2004},
  Pages                    = {66--72},

  Abstract                 = {The reuse of software components is the key to improving productivity and quality levels in software engineering. However, although the technologies for plugging together components have evolved dramatically over the last few years (e.g. EJB, .NET, Web Services) the technologies for actually finding them in the first place are still relatively immature. In this paper we present a simple but effective approach for harvesting software components from the Internet. The initial discovery of components is achieved using a standard Web search engine such as Google, and the evaluation of ``fitness for purpose" is performed by automated testing. Since test-driven evaluation of software is the hallmark of Extreme Programming, and the approach naturally complements the extreme approach to software engineering, we refer to it as "Extreme Harvesting". The paper first explains the principles behind Extreme Harvesting and then describes a prototype implementation.},
  Doi                      = {10.1109/IRI.2004.1431438}
}

@InProceedings{2010:rsse:hummel,
  Title                    = {Proposing software design recommendations based on component interface intersecting},
  Author                   = {Hummel, Oliver and Janjic, Werner and Atkinson, Colin},
  Booktitle                = rsse,
  Year                     = {2010},
  Pages                    = {64--68},

  Abstract                 = {The open source movement had a tremendous impact on software engineering in recent years. It not only established serious competition for many commercial software vendors, it also led to the availability of millions of source code artifacts on the Internet. For the time being there exists a fledgling community working on software search solutions and associated recommendation engines. However, the potential for reusing knowledge contained in internet-scale software repositories is far from being exhausted. While existing systems are limited to retrieving existing artifacts during the coding phase, in this position paper we propose a novel idea for determining the ``intersection" of multiple similar artifacts that allows creating design recommendations for a developer even earlier in the development lifecycle.}
}

@Article{2008:software:hummel,
  Title                    = {{C}ode {C}onjurer: Pulling Reusable Software out of Thin Air},
  Author                   = {Oliver Hummel and Werner Janjic and Colin Atkinson},
  Journal                  = software,
  Year                     = {2008},

  Month                    = sep # {/} # oct,
  Number                   = {5},
  Pages                    = {45--52},
  Volume                   = {25},

  Abstract                 = {For many years, the IT industry has sought to accelerate the software development process by assembling new applications from existing software assets. However, true component-based reuse of the form Douglas Mcllroy envisaged in the 1960s is still the exception rather than the rule, and most of the systematic software reuse practiced today uses heavyweight approaches such as product-line engineering or domain-specific frameworks. By component, we mean any cohesive and compact unit of software functionality with a well-defined interface---from simple programming language classes to more complex artifacts such as Web services and Enterprise JavaBeans.},
  Doi                      = {10.1109/MS.2008.110}
}

@InProceedings{2007:seke:hummel,
  Title                    = {Evaluating the Efficiency of Retrieval Methods for Component Repositories},
  Author                   = {Oliver Hummel and Werner Janjic and Colin Atkinson},
  Booktitle                = seke,
  Year                     = {2007},
  Pages                    = {404--409},

  Abstract                 = {Component-based software reuse has long been seen as a means of improving the efficiency of software development projects and the resulting quality of software systems. However, in practice it has proven difficult to set up and maintain viable software repositories and provide effective mechanisms for retrieving components and services from them. Although the literature contains a comprehensive collection of retrieval methods, to date there have been few evaluations of their relative efficiency. Moreover, those that are available only study small repositories of about a few hundred components. Since today's internet-based repositories are many orders of magnitude larger they require much higher search precision to deliver usable results. In this paper we present an evaluation of well known component retrieval techniques in the context of modern component repositories available on the World Wide Web.},
  Url                      = {http://www.ksi.edu/seke/Proceedings/seke/SEKE2007_Proceedings.pdf}
}

@InBook{1999:book:hunt,
  Title                    = {The Pragmatic Programmer: From Journeyman to Master},
  Author                   = {Andrew Hunt and David Thomas},
  Publisher                = {Addison-Wesley},
  Year                     = {1999},
  Note                     = {Section 2.7: The Evils of Duplication}
}

@InProceedings{2002:icsm:hunt,
  Title                    = {Extensible language-aware merging},
  Author                   = {Hunt, J. J and Tichy, W. F},
  Booktitle                = icsm,
  Year                     = {2002},
  Pages                    = {511--520},

  Abstract                 = {Parallel development has become standard practice in software development and maintenance. Though almost every revision control and configuration management system provides some form of merging for combining changes made in parallel, these mechanisms often yield unsatisfactory results. The authors present a new merging algorithm, that uses a fast differencing algorithm and renaming analysis to provide better merge results. The system is language aware, but not language dependent and does not require a special editor so it can be easily integrated in current development environments.}
}

@TechReport{1976:tr:hunt,
  Title                    = {An Algorithm for Differential File Comparison},
  Author                   = {J. W. Hunt and M. D. McIlroy},
  Institution              = {Bell Telephone Laboratories},
  Year                     = {1976},
  Number                   = {41},
  Type                     = {Computer Science Technical Report},

  Abstract                 = {The program diff reports differences between two files, expressed as a minimal list of line changes to bring either file into agreement with the other. Diff has been engineered to make efficient use of time and space on typical inputs that arise in vetting version-to-version changes in computer-maintained or computer-generated documents. Time and space usage are observed to vary about as the sum of the file lengths on real data, although they are known to vary as the product of the file lengths in the worst case. The central algorithm of diff solves the `longest common subsequence problem' to find the lines that do not change between files. Practical efficiency is gained by attending only to certain critical `candidate' matches between the files, the breaking of which would shorten the longest subsequence common to some pair of initial segments of the two files. Various techniques of hashing, presorting into equivalence classes, merging by binary search, and dynamic storage allocation are used to obtain good performance.},
  Url                      = {http://www.cs.dartmouth.edu/~doug/diff.ps}
}

@InProceedings{2007:acom:huynh,
  Title                    = {An Evolutionary Approach to Software Modularity Analysis},
  Author                   = {Huynh, Sunny and Cai, Yuanfang},
  Booktitle                = acom,
  Year                     = {2007},
  Pages                    = {6:1--6:6},

  Abstract                 = {Modularity determines software quality in terms of evolveability, changeability, maintainability, etc. and a module could be a vertical slicing through source code directory structure or class boundary. Given a modularized design, we need to determine whether its implementation realizes the designed modularity. Manually comparing source code modular structure with abstracted design modular structure is tedious and error-prone. In this paper, we present an automated approach to check the conformance of source code modularity to the designed modularity. Our approach uses design structure matrices (DSMs) as a uniform representation; it uses existing tools to automatically derive DSMs from the source code and design, and uses a genetic algorithm to automatically cluster DSMs and check the conformance. We applied our approach to a small canonical software system as a proof of concept experiment. The results supported our hypothesis that it is possible to check the conformance between source code structure and design structure automatically, and this approach has the potential to be scaled for use in large software systems.}
}

@TechReport{2006:tr:hyland-wood,
  Title                    = {Scale-Free Nature of {J}ava Software Package, Class and Method Collaboration Graphs},
  Author                   = {David Hyland-Wood and David Carrington and Simon Kaplan},
  Institution              = {University of Maryland, College Park},
  Year                     = {2006},
  Number                   = {TR-MS1286},

  Abstract                 = {Software collaboration graphs for two Open Source software projects written in the Java programming language, the Kowari Metastore and JRDF, were analyzed for fifteen-month periods of development. Collaboration graphs were produced at the package, class and method levels. The collaboration graphs were found to form networks which exhibited approximately scale-free properties at all three levels and during each period analyzed. This finding tends to support claims made by others that software class collaboration graphs approximate scale-free networks and provides new insights that they seem to retain the scale-free properties across different levels of granularity and over the course of their development life cycle. Significant differences between the magnitudes of power law exponents identified and those found in C and C++ applications by previous researchers are noted and discussed. Specifically, only the method-level collaboration graphs were found to have power law exponents in the most common range for scale-free networks ($2 < \gamma < 3$).},
  Url                      = {http://www.itee.uq.edu.au/~dwood/papers/Scale-FreeNatureOfJavaSoftwarePackageClassMethodCollaborationGraphs.pdf}
}

@InProceedings{2008:aswec:ichii,
  Title                    = {An exploration of power-law in use-relation of {J}ava software systems},
  Author                   = {Makoto Ichii and Makoto Matsushita and Katsuro Inoue},
  Booktitle                = aswec,
  Year                     = {2008},
  Pages                    = {422--431},

  Abstract                 = {A software component graph, where a node represents a component and an edge represents a use-relation between components, is widely used for analysis methods of software engineering. It is said that a graph is characterized by its degree distribution. In this paper, we investigate software component graphs composed of Java classes, to seek whether the degree distribution follows so-called the power-law, which is a fundamental characteristic of various kinds of graphs in different fields. We found that the in-degree distribution follows the power-law and the out-degree distribution does not follow the power-law. In a software component graph with about 180 thousand components, just a few of the components have more than ten thousand in-degrees while most of the components have only one or zero in-degree.}
}

@InProceedings{2009:pse:ihara,
  Title                    = {An analysis method for improving a bug modification process in open source software development},
  Author                   = {Ihara, Akinori and Ohira, Masao and Matsumoto, Ken-ichi},
  Booktitle                = pse,
  Year                     = {2009},
  Pages                    = {135--144},

  Abstract                 = {As open source software products have evolved over time to satisfy a variety of demands from increasing users, they have become large and complex in general. Open source developers often face with challenges in fixing a considerable amount of bugs which are reported into a bug tracking system on a daily basis. As a result, the mean time to resolve bugs has been protracted in these days. In order to reduce the mean time to resolve bugs, managers/leaders of open source projects need to identify and understand the bottleneck of a bug modification process in their own projects. In this paper, we propose an analysis method which represents a bug modification process using a bug tracking system as a state transition diagram and then calculates the amount of time required to transit between states. We have conducted a case study using Firefox and Apache project data to confirm the usefulness of the analysis method. From the results of the case study, we have found that the method helped to reveal that both of the projects took a lot of time to verify results of bug modifications by developers.}
}

@InProceedings{2007:ase:inkumsah,
  Title                    = {Evacon: A framework for integrating evolutionary and concolic testing for object-oriented programs},
  Author                   = {Inkumsah, Kobi and Xie, Tao},
  Booktitle                = ase,
  Year                     = {2007},
  Pages                    = {425--428},

  Doi                      = {10.1145/1321631.1321700}
}

@InProceedings{2012:icse:inoue,
  Title                    = {Where does this code come from and where does it go? Integrated code history tracker for open source systems},
  Author                   = {Inoue, Katsuro and Sasaki, Yusuke and Xia, Pei and Manabe, Yuki},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {331--341},

  Abstract                 = {When we reuse a code fragment in an open source system, it is very important to know the history of the code, such as the code origin and evolution. In this paper, we propose an integrated approach to code history tracking for open source repositories. This approach takes a query code fragment as its input, and returns the code fragments containing the code clones with the query code. It utilizes publicly available code search engines as external resources. Based on this model, we have designed and implemented a prototype system named Ichi Tracker. Using Ichi Tracker, we have conducted three case studies. These case studies show the ancestors and descendents of the code, and we can recognize their evolution history.}
}

@Article{2005:tse:inoue,
  Title                    = {Ranking Significance of Software Components Based on Use Relations},
  Author                   = {Katsuro Inoue and Reishi Yokomori and Tetsuo Yamamoto and Makoto Matsushita and Shinji Kusumoto},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {213--225},
  Volume                   = {31},

  Abstract                 = {Collections of already developed programs are important resources for efficient development of reliable software systems. In this paper, we propose a novel graph-representation model of a software component library (repository), called component rank model. This is based on analyzing actual usage relations of the components and propagating the significance through the usage relations. Using the component rank model, we have developed a Java class retrieval system named SPARS-J and applied SPARS-J to various collections of Java files. The result shows that SPARS-J gives a higher rank to components that are used more frequently. As a result, software engineers looking for a component have a better chance of finding it quickly. SPARS-J has been used by two companies, and has produced promising results.},
  Doi                      = {10.1109/TSE.2005.38}
}

@Article{2000:tosem:inverardi,
  Title                    = {Static checking of system behaviors using derived component assumptions},
  Author                   = {Inverardi, Paola and Wolf, Alexander L. and Yankelevich, Daniel},
  Journal                  = tosem,
  Year                     = {2000},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {239--272},
  Volume                   = {9},

  Abstract                 = {A critical challenge faced by the developer of a software system is to understand whether the system's components correctly integrate. While type theory has provided substantial help in detecting and preventing errors in mismatched static properties, much work remains in the area of dynamics. In particular, components make assumptions about their behavioral interaction with other components, but currently we have only limited ways in which to state those assumptions and to analyze those assumptions for correctness. We have formulated a method that begins to address this problem. The method operates at the architectural level so that behavioral integration errors, such as deadlock, can be revealed early and at a high level. For each component, a specification is given of its interaction behavior. Form this specification, assumptions that the component makes about the corresponding interaction behavior of the external context are automatically derived. We have defined an algorithm that performs compatibility checks between finite representations of a component's context assumptions and the actual interaction behaviors of the components with which it is intended to interact. A configuration of a system is possible if and only if a successful way of matching actual behaviors with assumptions can be found. The state-space complexity of this algorithm is significantly less than that of comparable approaches, and in the worst case, the time complexity is comparable to the worst case of standard rachability analysis.},
  Doi                      = {10.1145/352591.352593}
}

@InProceedings{2006:csmr:ivkovic,
  Title                    = {A Framework for Software Architecture Refactoring using Model Transformations and Semantic Annotations},
  Author                   = {Ivkovic, Igor and Kontogiannis, Kostas},
  Booktitle                = csmr,
  Year                     = {2006},
  Pages                    = {135--144},

  Abstract                 = {Software-intensive systems evolve continuously under the pressure of new and changing requirements, generally leading to an increase in overall system complexity. In this respect, to improve quality and decrease complexity, software artifacts need to be restructured and refactored throughout their lifecycle. Since software architecture artifacts represent the highest level of implementation abstraction, and constitute the first step in mapping requirements to design, architecture refactorings can be considered as the first step in the quest of maintaining system quality during evolution. In this paper, we introduce an approach for refactoring software architecture artifacts using model transformations and quality improvement semantic annotations. First, the conceptual architecture view is represented as a UML 2.0 profile with corresponding stereotypes. Second, instantiated architecture models are annotated using elements of the refactoring context, including soft-goals, metrics, and constraints. Finally, the actions that are most suitable for the given refactoring context are applied after being selected from a set of possible refactorings. The approach is applied to a simple example, demonstrating refactoring transformations for improved maintainability, performance, and security.},
  Doi                      = {10.1109/CSMR.2006.3}
}

@Article{2002:tois:jarvelin,
  Title                    = {Cumulated gain-based evaluation of {IR} techniques},
  Author                   = {J\"{a}rvelin, Kalervo and Kek\"{a}l\"{a}inen, Jaana},
  Journal                  = tois,
  Year                     = {2002},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {422--446},
  Volume                   = {20},

  Abstract                 = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
  Doi                      = {10.1145/582415.582418}
}

@InProceedings{2007:etx:jablonski,
  Title                    = {{CReN}: A tool for tracking copy-and-paste code clones and renaming identifiers consistently in the {IDE}},
  Author                   = {Patricia Jablonski and Daqing Hou},
  Booktitle                = etx,
  Year                     = {2007},
  Pages                    = {16--20},

  Abstract                 = {Programmers often copy and paste code so that they can reuse the existing code to complete a similar task. Many times, modifications to the newly pasted code include renaming all instances of an identifier, such as a variable name, consistently throughout the fragment. When these modifications are done manually, undetected inconsistencies and errors can result in the code, for example, a single instance can be missed and mistakenly not renamed. To help programmers avoid making this type of copy-paste error, we created a tool, named CReN, to provide tracking and identifier renaming support within copy-and-paste clones in an integrated development environment (IDE). CReN tracks the code clones involved when copying and pasting occurs in the IDE and infers a set of rules based on the relationships between the identifiers in these code fragments. These rules capture the programmer's intentions, for example, that a particular group of identifiers should be renamed consistently together. Programmers can also provide feedback to improve the accuracy of the inferred rules by specifying that a particular instance of an identifier is to be renamed separately. We introduce our CReN tool, which is implemented as an Eclipse plug-in in Java.},
  Doi                      = {10.1145/1328279.1328283}
}

@InProceedings{1994:icsm:jackson,
  Title                    = {Semantic {D}iff: A tool for summarizing the effects of modifications},
  Author                   = {Jackson, Daniel and Ladd, David A},
  Booktitle                = icsm,
  Year                     = {1994},
  Pages                    = {243--252},

  Abstract                 = {Describes a tool that takes two versions of a procedure and generates a report summarizing the semantic differences between them. Unlike existing tools based on comparison of program dependence graphs, our tool expresses its results in terms of the observable input-output behaviour of the procedure, rather than its syntactic structure. And because the analysis is truly semantic, it requires no prior matching of syntactic components, and generates fewer spurious differences, so that meaning-preserving transformations (such as renaming local variables) are correctly determined to have no visible effect. A preliminary experiment on modifications applied to the code of a large real-time system suggests that the approach is practical},
  Doi                      = {10.1109/ICSM.1994.336770}
}

@Book{2011:book:jacobson,
  Title                    = {{API}s: A Strategy Guide},
  Author                   = {Jacobson, Daniel and Brail, Greg and Woods, Dan},
  Publisher                = {O'Reilly Media, Inc.},
  Year                     = {2011},

  ISBN                     = {1449308929, 9781449308926}
}

@InProceedings{2010:suite:janjic,
  Title                    = {More archetypal usage scenarios for software search engines},
  Author                   = {Janjic, Werner and Hummel, Oliver and Atkinson, Colin},
  Booktitle                = suite,
  Year                     = {2010},
  Pages                    = {21--24},

  Abstract                 = {The increasing availability of software in all kinds of repositories has renewed interest in software retrieval and software reuse. Not only has there been significant progress in developing various types of tools for searching for reusable artifacts, but also the integration of these tools into development environments has matured considerably. Yet, relatively little is known on why and how developers use these features and whether there are applications of the technology that go beyond classic reuse. Since we believe it is important for our fledgling community to understand how developers can benefit from software search systems, we present an initial collection of archetypal usage scenarios for them. These are derived from a survey of existing literature along with novel ideas from ongoing experiments with a state of the art software search engine.},
  Doi                      = {10.1145/1809175.1809181}
}

@InProceedings{2013:msr:janjic,
  Title                    = {An Unabridged Source Code Dataset for Research in Software Reuse},
  Author                   = {Janjic, Werner and Hummel, Oliver and Schumacher, Marcus and Atkinson, Colin},
  Booktitle                = msrwc,
  Year                     = {2013},
  Pages                    = {339--342},

  Doi                      = {10.1109/MSR.2013.6624047}
}

@Book{2010:book:jannach,
  Title                    = {Recommender Systems: An Introduction},
  Author                   = {Dietmar Jannach and Markus Zanker and Alexander Felfernig and Gerhard Friedrich},
  Publisher                = {Cambridge University Press},
  Year                     = {2010},
  Month                    = sep,

  Abstract                 = {``Which digital camera should I buy? What is the best holiday for me and my family? Which is the best investment for supporting the education of my children? Which movie should I rent? Which web sites will I find interesting? Which book should I buy for my next vacation? Which degree and university are the best for my future?" It is easy to expand this list with many examples in which people have to make decisions about how they want to spend their money or, on a broader level, about their future. Traditionally, people have used a variety of strategies to solve such decision-making problems: conversations with friends, obtaining information from a trusted third party, hiring an expert team, consulting the Internet, using various methods from decision theory (if one tries to be rational), making a gut decision, or simply following the crowd. However, almost everyone has experienced a situation in which the advice of a friendly sales rep was not really useful, in which the gut decision to follow the investments of our rich neighbor was not really in our interest, or in which spending endless hours on the Internet led to confusion rather than to quick and good decisions. To sum up, good advice is difficult to receive, is in most cases time-consuming or costly, and even then is often of questionable quality. Wouldn't it be great to have an affordable personal advisor who helps us make good decisions efficiently? The construction of systems that support users in their (online) decision making is the main goal of the field of recommender systems. In particular, the goal of recommender systems is to provide easily accessible, high-quality recommendations for a large user community. This focus on volume and easy accessibility makes the technology very powerful. Although recommender systems aim at the individual decisions of users, these systems have a significant impact in a larger sense because of their mass application---as, for instance, Amazon.com's recommendation engines. Because of the far reach of the Internet market, this issue must not be underestimated, as the control of recommender systems allows markets themselves to be controlled in a broader sense. Consider, for example, a department store in which all the sales clerks follow orders to push only certain products. One can argue that recommender systems are for the masses who cannot afford or are not willing to pay for high-quality advice provided by experts. This is partially true in some domains, such as financial services or medicine; however, the goal of making good decisions includes the aim of outperforming domain experts. Although this is clearly not possible and also not necessary in all domains, there are many cases in which the wisdom of the crowds can be exploited to improve decisions. Thus, given the huge information bases available on the Internet, can we develop systems that provide better recommendations than humans? The challenge of providing affordable, personal, and high-quality recommendations is central to the field and generates many interesting follow-up goals on both a technical and a psychological level. Although, on the technical level, we are concerned with finding methods that exploit the available information and knowledge as effectively and efficiently as possible, psychological factors must be considered when designing the end-user interaction processes. The design of these communication processes greatly influences the trust in the subsequent recommendations and ultimately in the decisions themselves. Users rarely act as rational agents who know exactly what they want. Even the way a recommender agent asks for a customer's preferences or which decision options are presented will affect a customer's choice. Therefore, recommender systems cannot be reduced to simple decision theoretical concepts. More than fifteen years have passed since the software systems that are now called ``recommender systems" were first developed. Since then, researchers have continuously developed new approaches for implementing recommender systems, and today most of us are used to being supported by recommendation services such as the one found on Amazon.com. Historically, recommender systems have gained much attention by applying methods from artificial intelligence to information filtering - that is, to recommend web sites or to filter and rank news items. In fact, recommendation methods such as case-based or rule-based techniques have their roots in the expert systems of the 1980s. However, the application areas of recommender systems go far beyond pure information filtering methods, and nowadays recommendation technology is providing solutions in domains as diverse as financial products, real estate, electronic consumer products, movies, books, music, news, and web sites, just to name a few. This book provides an introduction to the broad field of recommender systems technology, as well as an overview of recent improvements. It is aimed at both graduate students or new PhDs who are starting their own research in the field and practitioners and IT experts who are looking for a starting point for the design and implementation of real-world recommender applications. Additional advanced material can be found, for instance, in Recommender Systems Handbook (Ricci et al. 2010), which contains a comprehensive collection of contributions from leading researchers in the field. This book is organized into two parts. In the first part, we start by summarizing the basic approaches to implementing recommender systems and discuss their individual advantages and shortcomings. In addition to describing how such systems are built, we focus on methods for evaluating the accuracy of recommenders and examining their effect on the behavior of online customers. The second part of the book focuses on recent developments and covers issues such as trust in recommender systems and emerging applications based on Web 2.0 and Semantic Web technologies. Teaching material to accompany the topics presented in this book is provided at the site http://www.recommenderbook.net/. We would like to thank everyone who contributed to this book, in particular, Heather Bergman and Lauren Cowles from Cambridge University Press, who supported us throughout the editorial process. Particular thanks also go to Arthur Pitman, Kostyantyn Shchekotykhin, Carla Delgado-Battenfeld, and Fatih Gedikli for their great help in proofreading the manuscript, as well as to several scholar colleagues for their effort in reviewing and giving helpful feedback.}
}

@InProceedings{2004:ase:jansen,
  Title                    = {Evaluation of tool support for architectural evolution},
  Author                   = {Anton Jansen and Jan Bosch},
  Booktitle                = ase,
  Year                     = {2004},
  Pages                    = {375--378},

  Abstract                 = {Evolution of software architectures is, different from architectural design, an area that only few tools have covered. We claim this is due to the lack of support for an important concept of architectural evolution: the notion of architectural design decisions. The absence of this concept in architectural evolution leads to several problems. In order to address these problems, we present a set of requirements that tools should support for architectural evolution. We evaluate existing software architecture tools against these architectural requirements. The results are analyzed and an outline for future research directions for architectural evolution tool support is presented.},
  Doi                      = {10.1109/ASE.2004.1342768}
}

@Article{2008:software:jansen,
  Title                    = {Pragmatic and Opportunistic Reuse in Innovative Start-up Companies},
  Author                   = {Jansen, Slinger and Brinkkemper, Sjaak and Hunink, Ivo and Demir, Cetin},
  Journal                  = software,
  Year                     = {2008},

  Month                    = nov # {/} # dec,
  Number                   = {6},
  Pages                    = {42--49},
  Volume                   = {25},

  Abstract                 = {Both practitioners and academics often frown on pragmatic and opportunistic reuse. Large organizations report that structured reuse methods and software product lines are often the way to go when it comes to efficient software reuse. However, opportunistic that is, nonstructured reuse has proven profitable for small to medium-sized organizations. Here, we describe two start-ups that have opportunistically and pragmatically developed their products, reusing functionality from others that they could never have built independently. We demonstrate that opportunistic and pragmatic reuse is necessary to rapidly develop innovative software products. We define pragmatic reuse as extending software with functionality from a third-party software supplier that was found without a formal search-and-procurement process and might not have been built with reuse in mind. We define opportunistic reuse as extending software with functionality from a third-party software supplier that wasn't originally intended to be integrated and reused.},
  Doi                      = {10.1109/MS.2008.155}
}

@InProceedings{2003:aosd:janzen,
  Title                    = {Navigating and querying code without getting lost},
  Author                   = {Janzen, Doug and De Volder, Kris},
  Booktitle                = aosd,
  Year                     = {2003},
  Pages                    = {178--187},

  Abstract                 = {A development task related to a crosscutting concern is challenging because a developer can easily get lost when exploring scattered elements of code and the complex tangle of relationships between them. In this paper we present a source browsing tool that improves the developer's ability to work with crosscutting concerns by providing better support for exploring code. Our tool helps the developer to remain oriented while exploring and navigating across a code base. The cognitive burden placed on a developer is reduced by avoiding disorienting view switches and by providing an explicit representation of the exploration process in terms of exploration paths. While our tool is generally useful, good navigation support is particularly important when exploring crosscutting concerns.},
  Doi                      = {10.1145/643603.643622}
}

@InProceedings{2010:issta:jaygarl,
  Title                    = {{OCAT}: Object capture-based automated testing},
  Author                   = {Jaygarl, Hojun and Kim, Sunghun and Xie, Tao and Chang, Carl K.},
  Booktitle                = issta,
  Year                     = {2010},
  Pages                    = {159--170},

  Abstract                 = {Testing object-oriented (OO) software is critical because OO languages are commonly used in developing modern software systems. In testing OO software, one important and yet challenging problem is to generate desirable object instances for receivers and arguments to achieve high code coverage, such as branch coverage, or find bugs. Our initial empirical findings show that coverage of nearly half of the difficult-to-cover branches that a state-of-the-art test-generation tool cannot cover requires desirable object instances that the tool fails to generate. Generating desirable object instances has been a significant challenge for automated test-generation tools, partly because the search space for such desirable object instances is huge, no matter whether these tools compose method sequences to produce object instances or directly construct object instances. To address this significant challenge, we propose a novel approach called Object Capture based Automated Testing (OCAT). OCAT captures object instances dynamically from program executions (e.g., ones from system testing or real use). These captured objects assist an existing automated test-generation tool, such as a random testing tool, to achieve higher code coverage. Afterwards, OCAT mutates collected instances, based on observed not-covered branches. We evaluated OCAT on three open source projects, and our empirical results show that OCAT helps a state-of-the-art random testing tool, Randoop, to achieve high branch coverage: on average 68.5\%, with 25.5\% improved from only 43.0\% achieved by Randoop alone.},
  Doi                      = {10.1145/1831708.1831729}
}

@InProceedings{2009:icpc:jeffrey,
  Title                    = {{BugFix}: A learning-based tool to assist developers in fixing bugs},
  Author                   = {Dennis Jeffrey and Min Feng and Neelam Gupta and Rajiv Gupta},
  Booktitle                = icpc,
  Year                     = {2009},
  Pages                    = {70--79},

  Abstract                 = {We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.},
  Doi                      = {10.1109/ICPC.2009.5090029}
}

@InProceedings{1995:ssr:jeng,
  Title                    = {Specification matching for software reuse: A foundation},
  Author                   = {Jeng, Jun-Jang and Cheng, Betty H. C.},
  Booktitle                = ssr,
  Year                     = {1995},
  Pages                    = {97--105},

  Abstract                 = {Using formal specifications to represent software components facilitates the determination of reusability because they more precisely characterize the functionality of the software, and the well-defined syntax makes processing amenable to automation. We present specification matching as a method for classification, retrieval, and modification of reusable components. A software component is specified in terms of order-sorted predicate logic. For both components and methods, we consider not only exact match, but also relaxed match and logical match for peforming specification matching over a library of reusable software components.},
  Doi                      = {10.1145/223427.211817}
}

@Article{2007:infoscijenkins,
  Title                    = {Software architecture graphs as complex networks: A novel partitioning scheme to measure stability and evolution},
  Author                   = {S. Jenkins and S. R. Kirk},
  Journal                  = infosci,
  Year                     = {2007},

  Month                    = {15 } # jun,
  Number                   = {12},
  Pages                    = {2587--2601},
  Volume                   = {177},

  Abstract                 = {The stability and evolution of the structure of consecutive versions of a series of software architecture graphs are analysed using the theory of complex networks. Brief comparisons are drawn between the scale-free behaviour and second order phase transitions. On this basis a software design metric $I_{cc}$ is proposed. This software metric is used to quantify the evolution of the stability vs. maintainability of the software through various releases. It is demonstrated that the classes in the software graph are acquiring more out-going calls than incoming calls as the software ages. Three examples of software applications where maintainability and continuous refactoring are an inherent part of their development process are presented, in addition to a Sun Java2 framework where growth and backward compatibility are the more important factors for the development. Further to this a projected future evolution of the software structure and maintainability is calculated. Suggestions for future applications to software engineering and the natural sciences are briefly presented. },
  Doi                      = {10.1016/j.ins.2007.01.021}
}

@Article{2000:nature:jeong,
  Title                    = {The large-scale organization of metabolic networks},
  Author                   = {H. Jeong and B. Tombor and R. Albert and Z. Oltvai and A.-L. Barab{\'a}si},
  Journal                  = nature,
  Year                     = {2000},

  Month                    = {5 } # oct,
  Number                   = {6804},
  Pages                    = {651--654},
  Volume                   = {407},

  Abstract                 = {In a cell or microorganism, the processes that generate mass, energy, information transfer and cell-fate specification are seamlessly integrated through a complex network of cellular constituents and reactions. However, despite the key role of these networks in sustaining cellular functions, their large-scale structure is essentially unknown. Here we present a systematic comparative mathematical analysis of the metabolic networks of 43 organisms representing all three domains of life. We show that, despite significant variation in their individual constituents and pathways, these metabolic networks have the same topological scaling properties and show striking similarities to the inherent organization of complex non-biological systems. This may indicate that metabolic organization is not only identical for all living organisms, but also complies with the design principles of robust and error-tolerant scale-free networks, and may represent a common blueprint for the large-scale organization of interactions among all cellular constituents.},
  Doi                      = {10.1038/35036627}
}

@InProceedings{2009:iseud:jeong,
  Title                    = {Improving documentation for {eSOA} {API}s through User Studies},
  Author                   = {Sae Young Jeong and Yingyu Xie and Jack Beaton and Brad A. Myers and Jeff Stylos and Ralf Ehret and Jan Karstens and Arkin Efeoglu and Daniela K. Busse},
  Booktitle                = iseud,
  Year                     = {2009},
  Pages                    = {86--105},

  Abstract                 = {All software today is written using libraries, toolkits, frameworks and other application programming interfaces (APIs). We performed a user study of the online documentation a large and complex API for Enterprise Service-Oriented Architecture (eSOA), which identified many issues and recommendations for making API documentation easier to use. eSOA is an appropriate testbed because the target user groups range from high-level business experts who do not have significant programming expertise (and thus are endparticipant developers), to professional programmers. Our study showed that the participants' background influenced how they navigated the documentation. Lack of familiarity with business terminology was a barrier we observed for developers without business application experience. Participants with business software experience had difficulty differentiating similarly named services. Both groups avoided areas of the documentation that had an inconsistent visual design. A new design for the documentation that supports flexible navigation strategies seem to be required to support the wide range of users for eSOA. This paper summarizes our study and provides recommendations for future documentation for developers.},
  Doi                      = {10.1007/978-3-642-00427-8\_6}
}

@InProceedings{2007:pdcat:ji,
  Title                    = {Source Code Similarity Detection Using Adaptive Local Alignment of Keywords},
  Author                   = {Ji, Jeong-Hoon and Park, Soo-Hyun and Woo, Gyun and Cho, Hwan-Gue},
  Booktitle                = pdcat,
  Year                     = {2007},
  Pages                    = {179--180},

  Abstract                 = {Program similarity checking is an important application of programming education fields. Local alignment is one of the typical algorithms for comparing two strings. However local alignment based comparison does not reflect the weights of program keywords. This paper introduces an adaptive local alignment which reflects the frequencies of keywords to the similarity matrix. We experimented this method using a set of programs submitted to more than 10 real programming contests. The experimen- tal result shows that the adaptive local alignment is more robust than greedy-string-tiling adopted in JPlag.}
}

@Article{2011:tse:jia,
  Title                    = {An analysis and survey of the development of mutation testing},
  Author                   = {Jia, Yue and Harman, Mark},
  Journal                  = tse,
  Year                     = {2011},

  Month                    = sep # {--} # oct,
  Number                   = {5},
  Pages                    = {649--678},
  Volume                   = {37},

  Doi                      = {10.1109/TSE.2010.62}
}

@InProceedings{2007:icse:jiang,
  Title                    = {{DECKARD}: Scalable and accurate tree-based detection of code clones},
  Author                   = {Jiang, Lingxiao and Misherghi, Ghassan and Su, Zhengdong and Glondu, Stephane},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {96--105},

  Abstract                 = {Detecting code clones has many software engineering applications. Existing approaches either do not scale to large code bases or are not robust against minor code modifications. In this paper, we present an efficient algorithm for identifying similar subtrees and apply it to tree representations of source code. Our algorithm is based on a novel characterization of subtrees with numerical vectors in the Euclidean space $\mathbb{R}^n$ and an efficient algorithm to cluster these vectors w.r.t. the Euclidean distance metric. Subtrees with vectors in one cluster are considered similar. We have implemented our tree similarity algorithm as a clone detection tool called DECKARD and evaluated it on large code bases written in C and Java including the Linux kernel and JDK. Our experiments show that DECKARD is both scalable and accurate. It is also language independent, applicable to any language with a formally specified grammar.},
  Doi                      = {10.1109/ICSE.2007.30}
}

@InProceedings{2009:issta:jiang,
  Title                    = {Automatic mining of functionally equivalent code fragments via random testing},
  Author                   = {Jiang, Lingxiao and Su, Zhendong},
  Booktitle                = issta,
  Year                     = {2009},
  Pages                    = {81--92},

  Abstract                 = {Similar code may exist in large software projects due to some common software engineering practices, such as copying and pasting code and $n$-version programming. Although previous work has studied syntactic equivalence and small-scale, coarse-grained program-level and function-level semantic equivalence, it is not known whether significant fine-grained, code-level semantic duplications exist. Detecting such semantic equivalence is also desirable because it can enable many applications such as code understanding, maintenance, and optimization. In this paper, we introduce the first algorithm to automatically mine functionally equivalent code fragments of arbitrary size - down to an executable statement. Our notion of functional equivalence is based on input and output behavior. Inspired by Schwartz's randomized polynomial identity testing, we develop our core algorithm using automated random testing: (1) candidate code fragments are automatically extracted from the input program; and (2) random inputs are generated to partition the code fragments based on their output values on the generated inputs. We implemented the algorithm and conducted a large-scale empirical evaluation of it on the Linux kernel 2.6.24. Our results show that there exist many functionally equivalent code fragments that are syntactically different (i.e., they are unlikely due to copying and pasting code). The algorithm also scales to million-line programs; it was able to analyze the Linux kernel with several days of parallel processing.},
  Doi                      = {10.1145/1572272.1572283}
}

@InCollection{2008:book:perner:jiang,
  Title                    = {Graph Matching},
  Author                   = {X. Jiang and H. Bunke},
  Booktitle                = {Case-Based Reasoning on Images and Signals},
  Publisher                = {Springer},
  Year                     = {2008},
  Chapter                  = {5},
  Editor                   = {Perner, Petra},
  Pages                    = {149--173},
  Series                   = sci,
  Volume                   = {73},

  Abstract                 = {Graphs are a powerful and universal tool widely used in information processing. Numerous methods for graph analysis have been developed. Examples include the detection of Hamiltonian cycles, shortest paths, vertex coloring, graph drawing, and so on [5]. In particular, graph representations are extremely useful in image processing and understanding, which is the complex process of mapping the initially numeric nature of an image (or images) into symbolic representations for subsequent semantic interpretation of the sensed world.},
  Doi                      = {10.1007/978-3-540-73180-1_5}
}

@InProceedings{2006:compsac:jing,
  Title                    = {Scale Free in Software Metrics},
  Author                   = {Liu Jing and He Keqing and Ma Yutao and Peng Rong},
  Booktitle                = compsac,
  Year                     = {2006},
  Pages                    = {229--235},

  Abstract                 = {Software has become a complex piece of work by the collective efforts of many. And it is often hard to predict what the final outcome will be. This transition poses new challenge to the software engineering (SE) community. By employing methods from the study of complex network, we investigate the object oriented (OO) software metrics from a different perspective. We incorporate the weighted methods per class (WMC) metric into our definition of the weighted OO software coupling network as the node weight. Empirical results from four open source OO software demonstrate power law distribution of weight and a clear correlation between the weight and the out degree. According to its definition, it suggests uneven distribution of function among classes and a close correlation between the functionality of a class and the number of classes it depending on. Further experiment shows similar distribution also exists between average LCOM and WMC as well as out degree. These discoveries will help uncover the underlying mechanisms of software evolution and will be useful for SE to cope with the emerged complexity in software as well as efficient test cases design.}
}

@InProceedings{2005:sigir:joachims,
  Title                    = {Accurately interpreting clickthrough data as implicit feedback},
  Author                   = {Thorsten Joachims and Laura Granka and Bing Pan and Helene Hembrooke and Geri Gay},
  Booktitle                = sigir,
  Year                     = {2005},
  Pages                    = {154--161},

  Abstract                 = {This paper examines the reliability of implicit feedback generated from clickthrough data in WWW search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average.},
  Doi                      = {10.1145/1076034.1076063}
}

@InProceedings{2002:chi:john,
  Title                    = {Automating {CPM-GOMS}},
  Author                   = {John, Bonnie and Vera, Alonso and Matessa, Michael and Freed, Michael and Remington, Roger},
  Booktitle                = chi,
  Year                     = {2002},
  Pages                    = {147--154},

  Abstract                 = {CPM-GOMS is a modeling method that combines the task decomposition of a GOMS analysis with a model of human resource usage at the level of cognitive, perceptual, and motor operations. CPM-GOMS models have made accurate predictions about skilled user behavior in routine tasks, but developing such models is tedious and error-prone. We describe a process for automatically generating CPM-GOMS models from a hierarchical task decomposition expressed in a cognitive modeling tool called Apex. Resource scheduling in Apex automates the difficult task of interleaving the cognitive, perceptual, and motor resources underlying common task operators (e.g. mouse move-and-click). Apex's UI automatically generates PERT charts, which allow modelers to visualize a model's complex parallel behavior. Because interleaving and visualization is now automated, it is feasible to construct arbitrarily long sequences of behavior. To demonstrate the process, we present a model of automated teller interactions in Apex and discuss implications for user modeling}
}

@InProceedings{1995:cuai:john,
  Title                    = {Estimating Continuous Distributions in Bayesian Classifiers},
  Author                   = {George H. John and Pat Langley},
  Booktitle                = cuai,
  Year                     = {1995},
  Pages                    = {338--345}
}

@InProceedings{1993:cascon:johnson,
  Title                    = {Identifying redundancy in source code using fingerprints},
  Author                   = {Johnson, J. Howard},
  Booktitle                = cascon,
  Year                     = {1993},
  Pages                    = {171--183},
  Volume                   = {1},

  Abstract                 = {A prototype implementation of a mechanism that uses fingerprints to identify exact repetitions of text in large program source trees has been built and successfully applied to a legacy source of over 300 megabytes. This prototype system has provided useful information as well as establishing the scalability of the technology. The approach will form the basis of a suite of tools for the visualization and understanding of programs and will complement other approaches currently under investigation.}
}

@Article{1997:cacm:johnson,
  Title                    = {Frameworks = (components + patterns)},
  Author                   = {Johnson, Ralph},
  Journal                  = cacm,
  Year                     = {1997},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {39--42},
  Volume                   = {40},

  Abstract                 = {Frameworks are an object-oriented reuse technique. They share many characteristics with reuse techniques in general [8], and object-oriented reuse techniques in particular. Although they have been used successfully for some time, and are an important part of the culture of long-time object-oriented developers, they are not well understood outside the object-oriented community and are often misused. Moreover, there is confusion about whether frameworks are large-scale patterns, or whether they are just another kind of component.},
  Doi                      = {10.1145/262793.262799}
}

@InProceedings{1992:oopsla:johnson,
  Title                    = {Documenting Frameworks Using Patterns},
  Author                   = {Ralph E. Johnson},
  Booktitle                = oopsla,
  Year                     = {1992},
  Pages                    = {63--76},

  Abstract                 = {The documentation for a framework must meet several requirements. These requirements can all be met by structuring the documentation as a set of patterns, sometimes called a ``pattern language". Patterns can describe the purpose of a framework, can let application programmers use a fmework without having to understand in detail how it works, and can teach many of the design details embodied in the frame work. This paper shows how to use patterns to document a framework, and includes a set ofpatterns for HotDraw as an example.}
}

@Article{1988:joop:johnson,
  Title                    = {Designing reuseable [\emph{sic}] classes},
  Author                   = {Johnson, Ralph E. and Foote, Brian},
  Journal                  = joop,
  Year                     = {1988},

  Month                    = jun # {/} # jul,
  Number                   = {2},
  Pages                    = {22--35},
  Volume                   = {1},

  Abstract                 = {Object-oriented programming is as much a different way of designing programs as it is a different way of designing programming languages. This paper describes what it is like to design systems in Smalltalk. In particular, since a major motivation for object-oriented programming is software reuse, this paper describes how classes are developed so that they will be reusable.},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.3954&rep=rep1&type=pdf}
}

@MastersThesis{2007:msc:jonas,
  Title                    = {Reverse Engineering of Framework {API} Changes via Mining Instantiation Code},
  Author                   = {Jan Jonas},
  School                   = {Technische Universit\"{a}t Darmstadt},
  Year                     = {2007},

  Address                  = {Darmstadt, Germany},
  Month                    = sep,
  Type                     = {Diploma Thesis}
}

@InProceedings{1997:cascon:jurisica,
  Title                    = {Similarity-based retrieval for diverse bookshelf software repository users},
  Author                   = {Igor Jurisica},
  Booktitle                = cascon,
  Year                     = {1997},
  Pages                    = {10:1--10:12},

  Abstract                 = {The paper presents a similarity-based retrieval framework for a software repository that aids the process of maintaining, understanding, and migrating legacy software systems [12]. Designing a software repository involves three issues: (1) information content; (2) information representation; and (3) strategies for accessing repository artifacts. Assuming the architecture presented in [12] we extend the retrieval system to support imprecise queries, iterative browsing, and diverse users. Because of repository size, complexity of queries and relations among artifacts, we take a performance approach to support a scalable implementation. We propose a retrieval system that uses numeric [31] and semantically rich context-based similarity [19]. Efficient iterative browsing is based on an incremental query evaluation algorithm from database management systems [20]. Explicitly defined context supports various retrieval strategies and diverse user models.}
}

@InProceedings{2014:fse:just,
  Title                    = {Are Mutants a Valid Substitute for Real Faults in Software Testing?},
  Author                   = {Just, Ren{\'e} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
  Booktitle                = fse,
  Year                     = {2014},
  Pages                    = {654--665},

  Doi                      = {10.1145/2635868.2635929}
}

@Article{2007:jsmerp:kagdi:a,
  Title                    = {A survey and taxonomy of approaches for mining software repositories in the context of software evolution},
  Author                   = {Kagdi, Huzefa and Collard, Michael L. and Maletic, Jonathan I.},
  Journal                  = jsmerp,
  Year                     = {2007},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {77--131},
  Volume                   = {19},

  Abstract                 = {A comprehensive literature survey on approaches for mining software repositories (MSR) in the context of software evolution is presented. In particular, this survey deals with those investigations that examine multiple versions of software artifacts or other temporal information. A taxonomy is derived from the analysis of this literature and presents the work via four dimensions: the type of software repositories mined (what), the purpose (why), the adopted/invented methodology used (how), and the evaluation method (quality). The taxonomy is demonstrated to be expressive (i.e., capable of representing a wide spectrum of MSR investigations) and effective (i.e., facilitates similarities and comparisons of MSR investigations). Lastly, a number of open research issues in MSR that require further investigation are identified.},
  Doi                      = {10.1002/smr.344}
}

@InProceedings{2007:msr:kagdi,
  Title                    = {Comparing approaches to mining source code for call-usage patterns},
  Author                   = {Kagdi, Huzefa and Collard, Michael L. and Maletic, Jonathan I.},
  Booktitle                = msrw,
  Year                     = {2007},
  Pages                    = {20/1--20/8},

  Abstract                 = {Two approaches for mining function-call usage patterns from source code are compared. The first approach, itemset mining, has recently been applied to this problem. The other approach, sequential-pattern mining, has not been previously applied to this problem. Here, a call-usage pattern is a composition of function calls that occur in a function definition. Both approaches look for frequently occurring patterns that represent standard usage of functions and identify possible errors. Itemset mining produces unordered patterns, i.e., sets of function calls, whereas, sequential-pattern mining produces partially ordered patterns, i.e., sequences of function calls. The trade-off between the additional ordering context given by sequential-pattern mining and the efficiency of itemset mining is investigated. The two approaches are applied to the Linux kernel v2.6.14 and results show that mining ordered patterns is worth the additional cost.}
}

@Article{2007:jsmerp:kagdi:b,
  Title                    = {Mining evolutionary dependencies from web-localization repositories},
  Author                   = {Kagdi, Huzefa and Maletic, Jonathan I.},
  Journal                  = jsmerp,
  Year                     = {2007},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {315--337},
  Volume                   = {19},

  Abstract                 = {An approach to mining repositories of web-based user documentation for patterns of evolutionary change in the context of internationalization and localization is presented. Localized web documents that are frequently co-changed (i.e., an evolutionary dependency) during the natural language translation process are uncovered to support the future evolution of the system. A sequential-pattern mining technique is used to uncover patterns from version histories. Characteristics of the uncovered patterns such as size, frequency, and occurrence within a single natural language or across multiple languages are discussed. Such patterns help provide an insight into the effort required in retranslation due to a change in the documentation. The approach is validated on the open source K Desktop Environment (KDE) system. KDE maintains documentation for over 50 different natural languages and presents a prime example of the problem. The technique accurately predicts which documents in KDE are retranslated or updated in future versions.},
  Doi                      = {10.1002/smr.v19:5}
}

@InProceedings{2007:icpc:kagdi,
  Title                    = {Mining Software Repositories for Traceability Links},
  Author                   = {Kagdi, Huzefa and Maletic, Jonathan I. and Sharif, Bonita},
  Booktitle                = icpc,
  Year                     = {2007},
  Pages                    = {145--154},

  Abstract                 = {An approach to recover/discover traceability links between software artifacts via the examination of a software system's version history is presented. A heuristic-based approach that uses sequential-pattern mining is applied to the commits in software repositories for uncovering highly frequent co-changing sets of artifacts (e.g., source code and documentation). If different types of files are committed together with high frequency then there is a high probability that they have a traceability link between them. The approach is evaluated on a number of versions of the open source system KDE. As a validation step, the discovered links are used to predict similar changes in the newer versions of the same system. The results show highly precision predictions of certain types of traceability links.}
}

@Article{2008:ese:kapser,
  Title                    = {{`Cloning considered harmful'} considered harmful: Patterns of cloning in software},
  Author                   = {Cory Kapser and Michael W. Godfrey},
  Journal                  = ese,
  Year                     = {2008},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {645--692},
  Volume                   = {13},

  Abstract                 = {Literature on the topic of code cloning often asserts that duplicating code within a software system is a bad practice, that it causes harm to the system's design and should be avoided. However, in our studies, we have found significant evidence that cloning is often used in a variety of ways as a principled engineering tool. For example, one way to evaluate possible new features for a system is to clone the affected subsystems and introduce the new features there, in a kind of sandbox testbed. As features mature and become stable within the experimental subsystems, they can be migrated incrementally into the stable code base; in this way, the risk of introducing instabilities in the stable version is minimized. This paper describes several patterns of cloning that we have observed in our case studies and discusses the advantages and disadvantages associated with using them. We also examine through a case study the frequencies of these clones in two medium-sized open source software systems, the Apache web server and the Gnumeric spreadsheet application. In this study, we found that as many as 71\% of the clones could be considered to have a positive impact on the maintainability of the software system.},
  Doi                      = {10.1007/s10664-008-9076-6}
}

@InProceedings{2009:stvv:karhu,
  Title                    = {Empirical observations on software testing automation},
  Author                   = {Karhu, Katja and Repo, Tiina and Taipale, Ossi and Smolander, Kari},
  Booktitle                = stvv,
  Year                     = {2009},
  Pages                    = {201--209}
}

@InProceedings{2006:ozchi:katsanos,
  Title                    = {{InfoScent} {E}valuator: A semi-automated tool to evaluate semantic appropriateness of hyperlinks in a web site},
  Author                   = {Katsanos, Christos and Tselios, Nikolaos and Avouris, Nikolaos},
  Booktitle                = ozchi,
  Year                     = {2006},
  Pages                    = {373--376},

  Abstract                 = {In this paper, we present InfoScent Evaluator, a tool that automatically evaluates the semantic appropriateness of the descriptions of hyperlinks in web pages. The tool is based on a theoretical model of users' behavior when engaged in information search tasks, called Information Foraging theory. A textual description of the user's search goal is compared with the textual description of each probable hyperlink, using Latent Semantic Analysis, a statistical technique that evaluates the distance between the two texts. Through this approach the most probable path that the user will follow in order to access the sought web page can be predicted. Thus, the tool can be used to evaluate the web site in terms of appropriateness of hyperlink text and of information architecture. We argue that the presented tool could substantially aid design and evaluation of a web site.}
}

@InProceedings{2005:chi:kaur,
  Title                    = {A comparison of {LSA}, {wordNet} and {PMI-IR} for predicting user click behavior},
  Author                   = {Kaur, Ishwinder and Hornof, Anthony J.},
  Booktitle                = chi,
  Year                     = {2005},
  Pages                    = {51--60},

  Abstract                 = {A predictive tool to simulate human visual search behavior would help interface designers inform and validate their design. Such a tool would benefit from a semantic component that would help predict search behavior even in the absence of exact textual matches between goal and target. This paper discusses a comparison of three semantic systems-LSA, WordNet and PMI-IR-to evaluate their performance in predicting the link that people would select given an information goal and a webpage. PMI-IR best predicted human performance as observed in a user study.}
}

@MastersThesis{2011:msc:kawrykow,
  Title                    = {Enabling Precise Interpretations of Software Change Data},
  Author                   = {David Kawrykow},
  School                   = {McGill University},
  Year                     = {2011},

  Address                  = {Montr{\'e}al, Canada},

  Abstract                 = {Numerous techniques mine change data captured in software archives to assist software engineering efforts. These change-based approaches typically analyze change sets---groups of co-committed changes---under the assumption that the development work represented by change sets is both meaningful and related to a single change task. However, we have found that change sets often violate this assumption by containing changes that we consider to be non-essential, or less likely to be representative of the kind of meaningful software development effort that is most interesting to typical change-based approaches. Furthermore, we have found many change sets addressing multiple subtasks---groups of isolated changes that are related to each other, but not to other changes within a change set. Information mined from such change sets has the potential for interfering with the analyses of various change-based approaches. We propose a catalog of non-essential changes and describe an automated technique for detecting such changes within version histories. We used our technique to conduct an empirical investigation of over 30,000 change sets capturing over 25 years of cumulative development activity in ten open-source Java systems. Our investigation found that between 3\% and 26\% of all modified code lines and between 2\% and 16\% of all method updates consisted entirely of non-essential modifications. We further found that eliminating such modifications reduces the amount of false positive recommendations that would be made by an existing association rule miner. These findings are supported by a manual evaluation of our detection technique, in which we found that our technique falsely identifies non-essential method updates in only 0.2\% of all cases. These observations should be kept in mind when interpreting insights derived from version repositories. We also propose a formal definition of ``subtasks'' and present an automated technique for detecting subtasks within change sets. We describe a new benchmark containing over 1,800 manually classified change sets drawn from seven open-source Java systems. We evaluated our technique on the benchmark and found that the technique classifies singleand multi-task change sets with a precision of 80\% and a recall of 24\%. In contrast, the current ``default strategy'' of assuming all change sets are single-task classifies single- and multi-task change sets with a precision of 95\% and a recall of 0\%. We further characterized the performance of our technique by manually assessing its false classifications. We found that in most cases (78\%), false classifications made by our technique can be further refined to produce useful recommendations for change-based approaches. Our observations should aid future change-based seeking to derive more precise representations of the changes they analyze.},
  Url                      = {http://cs.mcgill.ca/~martin/theses/msc-kawrykow-2011.pdf}
}

@Article{2006:tois:kazai,
  Title                    = {e{X}tended {C}umulated {G}ain measures for the evaluation of content-oriented {XML} retrieval},
  Author                   = {Kazai, Gabriella and Lalmas, Mounia},
  Journal                  = tois,
  Year                     = {2006},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {503--542},
  Volume                   = {24},

  Abstract                 = {We propose and evaluate a family of measures, the eXtended Cumulated Gain (XCG) measures, for the evaluation of content-oriented XML retrieval approaches. Our aim is to provide an evaluation framework that allows the consideration of dependency among XML document components. In particular, two aspects of dependency are considered: (1) near-misses, which are document components that are structurally related to relevant components, such as a neighboring paragraph or container section, and (2) overlap, which regards the situation wherein the same text fragment is referenced multiple times, for example, when a paragraph and its container section are both retrieved. A further consideration is that the measures should be flexible enough so that different models of user behavior may be instantiated within. Both system- and user-oriented aspects are investigated and both recall and precision-like qualities are measured. We evaluate the reliability of the proposed measures based on the INEX 2004 test collection. For example, the effects of assessment variation and topic set size on evaluation stability are investigated, and the upper and lower bounds of expected error rates are established. The evaluation demonstrates that the XCG measures are stable and reliable, and in particular, that the novel measures of effort-precision and gain-recall (ep/gr) show comparable behavior to established IR measures like precision and recall.},
  Doi                      = {10.1145/1185877.1185883}
}

@Article{1999:ijase:kazman,
  Title                    = {Playing Detective: Reconstructing Software Architecture from Available Evidence},
  Author                   = {Kazman, Rick and Carri\`{e}re, S. Jeromy},
  Journal                  = ijase,
  Year                     = {1999},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {107--138},
  Volume                   = {6},

  Abstract                 = {Because a system's software architecture strongly influences its quality attributes such as modifiability, performance, and security, it is important to analyze and reason about that architecture. However, architectural documentation frequently does not exist, and when it does, it is often ``out of sync'' with the implemented system. In addition, it is rare that software development begins with a clean slate; systems are almost always constrained by existing legacy code. As a consequence, we need to be able to extract information from existing system implementations and utilize this information for architectural reasoning. This paper presents Dali, an open, lightweight workbench that aids an analyst in extracting, manipulating, and interpreting architectural information. By assisting in the reconstruction of architectures from extracted information, Dali helps an analyst redocument architectures, discover the relationship between ``as-implemented'' and ``as-designed'' architectures, analyze architectural quality attributes and plan for architectural change.},
  Doi                      = {10.1023/A:1008781513258}
}

@Article{2005:bioessays:keller,
  Title                    = {Revisiting ``scale-free'' networks},
  Author                   = {Keller, Evelyn Fox},
  Journal                  = bioessays,
  Year                     = {2005},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {1060--1068},
  Volume                   = {27},

  Abstract                 = {Recent observations of power-law distributions in the connectivity of complex networks came as a big surprise to researchers steeped in the tradition of random networks. Even more surprising was the discovery that power-law distributions also characterize many biological and social networks. Many attributed a deep significance to this fact, inferring a ``universal architecture" of complex systems. Closer examination, however, challenges the assumptions that (1) such distributions are special and (2) they signify a common architecture, independent of the system's specifics. The real surprise, if any, is that power-law distributions are easy to generate, and by a variety of mechanisms. The architecture that results is not universal, but particular; it is determined by the actual constraints on the system in question.},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/16163729}
}

@InProceedings{1997:oopsla:keller,
  Title                    = {Object-oriented design quality},
  Author                   = {Rudolf K. Keller and Reinhard Schauer and Alistair Cockburn},
  Booktitle                = oopslaadd,
  Year                     = {1997},
  Pages                    = {63--67},

  Abstract                 = {Despite the burst in the availability of OO analysis and design methodologies, languages, database management systems, and tools, relatively little work has been done in the area of OO design quality assurance, assessment, and improvement. We badly need a better understanding of the properties of OO system design, in the small and in the large, and their effect on quality factors such as maintainability, evolvability, and reusability. The understanding of desirable and non-desirable properties may contribute to improvement of risk assessment and product selection, better planning, more accurate assessment of maintenance efforts, improved productivity evaluation, a better foundation of design methods, improved system evolvability, and eventually to better design practices.}
}

@Article{1984:tois:kelley,
  Title                    = {An iterative design methodology for user-friendly natural language office information applications},
  Author                   = {Kelley, J. F.},
  Journal                  = tois,
  Year                     = {1984},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {26--41},
  Volume                   = {2},

  Abstract                 = {A six-step, iterative, empirical human factors design methodology was used to develop CAL, a natural language computer application to help computer-naive business professionals manage their personal calendars. Input language is processed by a simple, nonparsing algorithm with limited storage requirements and a quick response time. CAL allows unconstrained English inputs from users with no training (except for a five minute introduction to the keyboard and display) and no manual (except for a two-page overview of the system). In a controlled test of performance, CAL correctly responded to between 86 percent and 97 percent of the storage and retrieval requests it received, according to various criteria. This level of performance could never have been achieved with such a simple processing model were it not for the empirical approach used in the development of the program and its dictionaries. The tools of the engineering psychologist are clearly invaluable in the development of user-friendly software, if that software is to accommodate the unruly language of computer-naive, first-time users. The key is to elicit the cooperation of such users as partners in an iterative, empirical development process.},
  Doi                      = {10.1145/357417.357420}
}

@Article{2006:tse:kelly,
  Title                    = {A Study of Design Characteristics in Evolving Software Using Stability as a Criterion},
  Author                   = {Diane Kelly},
  Journal                  = tse,
  Year                     = {2006},

  Month                    = may,
  Number                   = {5},
  Pages                    = {315--329},
  Volume                   = {32},

  Abstract                 = {There are many ideas in software design that are considered good practice. However, research is still needed to validate their contributions to software maintenance. This paper presents a method for examining software systems that have been actively maintained and used over the long term and are potential candidates for yielding lessons about design. The method relies on a criterion of stability and a definition of distance to flag design characteristics that have potentially contributed to long-term maintainability. It is demonstrated by application to an example of long-lived scientific software. The results from this demonstration show that the method can provide insight into the relative importance of individual elements of a set of design characteristics for the long-term evolution of software.},
  Doi                      = {10.1109/TSE.2006.42}
}

@Book{2004:book:kerievsky,
  Title                    = {Refactoring to Patterns},
  Author                   = {Joshua Kerievsky},
  Publisher                = {Addison-Wesley},
  Year                     = {2004}
}

@InProceedings{2007:cc:kerne,
  Title                    = {Promoting emergence in information discovery by representing collections with composition},
  Author                   = {Kerne, Andruid and Koh, Eunyee and Smith, Steven and Choi, Hyun and Graeber, Ross and Webb, Andrew},
  Booktitle                = cc,
  Year                     = {2007},
  Pages                    = {117--126},

  Abstract                 = {While sometimes the task that motivates searching, browsing, and collecting information resources is finding a particular fact, humans often engage in intellectual and creative tasks, such as comparison, understanding, and discovery. Information discovery tasks involve not only finding relevant information, but also seeing relationships among collected information resources, and developing new ideas. Prior studies of search have focused on time and accuracy, metrics of limited value for measuring creativity. We develop new experimental methods to evaluate the efficacy of representational systems for information discovery by measuring the emergence of new ideas. We also measure the variety of web sites that participants visit when engaging in a creative task, and gather experience report data. We compare the efficacy of the typical format for collections, the textual list with a new format, the composition of image and text surrogates. We conduct an experiment that establishes that representing collections with composition of image and text surrogates promotes emergence in information discovery.}
}

@InProceedings{2008:cccsse:kerr,
  Title                    = {Context-sensitive cut, copy, and paste},
  Author                   = {Kerr, Reid and Stuerzlinger, Wolfgang},
  Booktitle                = cccsse,
  Year                     = {2008},
  Pages                    = {159--166},

  Abstract                 = {Creating and editing source code are tedious and error-prone processes. One important source of errors in editing programs is the failure to correctly adapt a block of copied code to a new context. This occurs because several dependencies to the surrounding code usually need to be adapted for the new context and it is easy to forget one of them. Conversely, this also makes such errors hard to find. This paper presents a new method for identifying some common types of errors in cut, copy and paste operations. The method analyzes the context of the original block of code and tries to match it with the context in the new location. It utilizes a simple, pattern-based model of context, which we found to be well suited to the analysis of relocated blocks of text. Furthermore, we discuss the ability of our technique to detect semantic errors. While semantic errors are relatively difficult to recognize in a static document, our new technique can infer important information from the original context to detect some semantic mismatches. Finally, we present a proof-of-concept implementation and discuss our simple user interface for context-sensitive cut, copy and paste.}
}

@PhdThesis{2007:phd:kersten,
  Title                    = {Focusing Knowledge Work with Task Context},
  Author                   = {Kersten, Mik},
  School                   = {University of British Columbia},
  Year                     = {2007},

  Address                  = {Vancouver, Canada},

  Abstract                 = {By making information easy to browse and query, current software tools make it possible for knowledge workers to access vast amounts of information available in document repositories and on the web. However, when displaying dozens of web page search hits, hundreds of files and folders in a document hierarchy, or tens of thousands of lines of source code, these tools overload knowledge workers with information that is not relevant to the task-at-hand. The result is that knowledge workers waste time clicking, scrolling, and navigating to find the subset of information needed to complete a task. This problem is exacerbated by the fact that many knowledge workers constantly multi-task. With each task switch, they lose the context that they have built up in the browsing and query views. The combination of context loss and information overload has adverse effects on productivity because it requires knowledge workers to repeatedly locate the information that they need to complete a task. The larger the amount of information available and the more frequent the multi-tasking, the worse the problem becomes. We propose to alleviate this problem by focusing the software applications a knowledge worker uses on the information relevant to the task-at-hand. We represent the information related to the task with a task context model in which the relevant elements and relations are weighted according to their frequency and recency of access. We define operations on task context to support tailoring the task context model to different kinds of knowledge work activities. We also describe task-focused user interface mechanisms that replace the structure-centric display of information with a task-centric one. We validate the task context model with three field studies. Our preliminary feasibility study of six industry programmers tested a prototype implementation of the task context model and task-focused user interface for an integrated development environment. Our second study involved sixteen industry programmers using a production quality implementation of the task context model; these programmers experienced a statically significant increase in productivity when using task context. Our third field study tested a prototype implementation of the task context model for a file and web browsing application. The results of this study showed that task context generalizes beyond programming applications, reducing information overload and facilitating multi-tasking in a cross-section of knowledge work domains.},
  Url                      = {http://www.cs.ubc.ca/grads/resources/thesis/Nov07/Kersten_Mik.pdf}
}

@InProceedings{2006:fse:kersten,
  Title                    = {Using task context to improve programmer productivity},
  Author                   = {Kersten, Mik and Murphy, Gail C.},
  Booktitle                = fse,
  Year                     = {2006},
  Pages                    = {1--11},

  Abstract                 = {When working on a large software system, a programmer typically spends an inordinate amount of time sifting through thousands of artifacts to find just the subset of information needed to complete an assigned task. All too often, before completing the task the programmer must switch to working on a different task. These task switches waste time as the programmer must repeatedly find and identify the information relevant to the task-at-hand. In this paper, we present a mechanism that captures, models, and persists the elements and relations relevant to a task. We show how our task context model reduces information overload and focuses a programmer's work by filtering and ranking the information presented by the development environment. A task context is created by monitoring a programmer's activity and extracting the structural relationships of program artifacts. Operations on task contexts integrate with development environment features, such as structure display, search, and change management. We have validated our approach with a longitudinal field study of Mylar, our implementation of task context for the Eclipse development environment. We report a statistically significant improvement in the productivity of 16 industry programmers who voluntarily used Mylar for their daily work.}
}

@InProceedings{2005:aosd:kersten,
  Title                    = {Mylar: A degree-of-interest model for {IDE}s},
  Author                   = {Kersten, Mik and Murphy, Gail C.},
  Booktitle                = aosd,
  Year                     = {2005},
  Pages                    = {159--168},

  Abstract                 = {Even when working on a well-modularized software system, programmers tend to spend more time navigating the code than working with it. This phenomenon arises because it is impossible to modularize the code for all tasks that occur over the lifetime of a system. We describe the use of a degree-of-interest (DOI) model to capture the task context of program elements scattered across a code base. The Mylar tool that we built encodes the DOI of program elements by monitoring the programmer's activity, and displays the encoded DOI model in views of Java and AspectJ programs. We also present the results of a preliminary diary study in which professional programmers used Mylar for their daily work on enterprise-scale Java systems.}
}

@InProceedings{2003:cbr:khoshgoftaar,
  Title                    = {Detecting Outliers Using Rule-Based Modeling for Improving {CBR}-Based Software Quality Classification Models},
  Author                   = {Taghi M. Khoshgoftaar and Lofton A. Bullard and Kehan Gao},
  Booktitle                = cbr,
  Year                     = {2003},
  Pages                    = {216--230},
  Series                   = lncs,
  Volume                   = {2689},

  Abstract                 = {Deploying a software product that is of high quality is a major concern for the project management team. Significant research has been dedicated toward developing methods for improving the quality of metrics-based software quality classification models. Several studies have shown that the accuracy of such models improves when outliers and data noise are removed from the training data set. This study presents a new approach called Rule-Based Modeling (RBM) for detecting and removing training data outliers in an effort to improve the accuracy of a Case-Based Reasoning (CBR) classification model. We chose to study CBR models because of their sensitivity to outliers in the training data set. Furthermore, we wanted to affirmthe RBM technique as a viable outlier detector. We evaluate our approach by comparing the classification accuracy of CBR models built with and without removing outliers from the training data set. It is demonstrated that applying the RBM technique for eliminating outliers significantly improves the accuracy of CBR-based software quality classification models.},
  Doi                      = {10.1007/3-540-45006-8_19}
}

@InProceedings{2001:ecoop:kiczales,
  Title                    = {An overview of {AspectJ}},
  Author                   = {Gregor Kiczales and Erik Hilsdale and Jim Hugunin and Mik Kersten and Jeffrey Palm and William G. Griswold},
  Booktitle                = ecoop,
  Year                     = {2001},
  Pages                    = {327--353},

  Abstract                 = {AspectJ is a simple and practical aspect-oriented extension to Java. With just a few new constructs, AspectJ provides support for modular implementation of a range of crosscutting concerns. In AspectJ's dynamic join point model, join points are well-defined points in the execution of the program; pointcuts are collections of join points; advice are special method-like constructs that can be attached to pointcuts; and aspects are modular units of crosscutting implementation, comprising pointcuts, advice, and ordinary Java member declarations. AspectJ code is compiled into standard Java bytecode. Simple extensions to existing Java development environments make it possible to browse the crosscutting structure of aspects in the same kind of way as one browses the inheritance structure of classes. Several examples show that AspectJ is powerful, and that programs written using it are easy to understand.}
}

@InProceedings{1997:ecoop:kiczales,
  Title                    = {Aspect-Oriented Programming},
  Author                   = {Gregor Kiczales and John Lamping and Anurag Mendhekar and Chris Maeda and Cristina Videira Lopes and Jean-Marc Loingtier and John Irwin},
  Booktitle                = ecoop,
  Year                     = {1997},
  Pages                    = {220--242},
  Series                   = lncs,
  Volume                   = {1241},

  Abstract                 = {We have found many programming problems for which neither procedural nor object-oriented programming techniques are sufficient to clearly capture some of the important design decisions the program must implement. This forces the implementation of those design decisions to be scattered throughout the code, resulting in ``tangled" code that is excessively difficult to develop and maintain. We present an analysis of why certain design decisions have been so difficult to clearly capture in actual code. We call the properties these decisions address aspects, and show that the reason they have been hard to capture is that they cross-cut the system's basic functionality. We present the basis for a new programming technique, called aspectoriented programming, that makes it possible to clearly express programs involving such aspects, including appropriate isolation, composition and reuse of the aspect code. The discussion is rooted in systems we have built using aspect-oriented programming.},
  Doi                      = {10.1007/BFb0053381}
}

@Book{1991:book:kiczales,
  Title                    = {The Art of the Metaobject Protocol},
  Author                   = {Gregor Kiczales and Jim des Rivi{\`e}res and Daniel Bobrow},
  Publisher                = {MIT Press},
  Year                     = {1991}
}

@InProceedings{2006:aosd:kienzle,
  Title                    = {{AO} challenge: Implementing the {ACID} properties for transactional objects},
  Author                   = {J{\"o}rg Kienzle and Samuel G{\'e}lineau},
  Booktitle                = aosd,
  Year                     = {2006},
  Pages                    = {202--213},

  Abstract                 = {This paper presents a challenge case study to the aspectoriented community: ensuring the ACID properties (atomicity, consistency, isolation and durability) for transactional objects. We define a set of ten base aspects, each one providing a well-defined reusable functionality. The base aspects are simple, yet have complex dependencies among each other. We then show how these base aspects can be configured and composed in different ways to implement different concurrency control and recovery strategies. This composition is delicate: some aspects conflict with each other, others have to be reconfigured dynamically at run-time. We believe that this case study can serve as a benchmark for aspectoriented software development, in particular for evaluating the expressivity of aspect-oriented programming languages, the performance of aspect-oriented programming environments, and the suitability of aspect-oriented modeling notations.}
}

@Article{2002:physreve:kim,
  Title                    = {Path finding strategies in scale-free networks},
  Author                   = {Beom Jun Kim and Chang No Yoon and Seung Kee Han and Hawoong Jeong},
  Journal                  = physreve,
  Year                     = {2002},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {027103:1--027103:4},
  Volume                   = {65},

  Abstract                 = {We numerically investigate the scale-free network model of Barab{\'a}si and Albert [A. L. Barab{\'a}si and R. Albert, Science 286, 509 (1999)] through the use of various path finding strategies. In real networks, global network information is not accessible to each vertex, and the actual path connecting two vertices can sometimes be much longer than the shortest one. A generalized diameter depending on the actual path finding strategy is introduced, and a simple strategy, which utilizes only local information on the connectivity, is suggested and shown to yield small-world behavior: the diameter $D$ of the network increases logarithmically with the network size $N$, the same as is found with global strategy. If paths are sought at random, $DN^{0.5}$ is found.},
  Doi                      = {10.1103/PhysRevE.65.027103}
}

@InProceedings{2004:isese:kim,
  Title                    = {An Ethnographic Study of Copy and Paste Programming Practices in {OOPL}},
  Author                   = {Miryung Kim and Lawrence Bergman and Tessa Lau and David Notkin},
  Booktitle                = isese,
  Year                     = {2004},
  Pages                    = {83--92},

  Abstract                 = {Although programmers frequently copy and paste code when they develop software, implications of common copy and paste (C\&P) usage patterns have not been studied previously. We have conducted an ethnographic study in order to understand programmers' C\&P programming practices and discover opportunities to assist common C\&P usage patterns. We observed programmers using an instrumented Eclipse IDE and then analyzed why and how they use C\&P operations. Based on our analysis, we constructed a taxonomy of C\&P usage patterns. This paper presents our taxonomy of C\&P usage patterns and discusses our insights with examples drawn from our observations. From our insights, we propose a set of tools that both can reduce software maintenance problems incurred by C\&P and can better support the intents of commonly used C\&P scenarios.},
  Doi                      = {10.1109/ISESE.2004.1334896}
}

@InProceedings{2010:fse:kim,
  Title                    = {Ref-{F}inder: A refactoring reconstruction tool based on logic query templates},
  Author                   = {Miryung Kim and Matthew Gee and Alex Loh and Napol Rachatasumrit},
  Booktitle                = fse,
  Year                     = {2010},
  Pages                    = {371--372},

  Abstract                 = {Knowing which parts of a system underwent which types of refactoring between two program versions can help programmers better understand code changes. Though there are a number of techniques that automatically find refactorings from two input program versions, these techniques are inadequate in terms of coverage by handling only a subset of refactoring types---mostly simple rename and move refactorings at the level of classes, methods, and fields. This paper presents a Ref-Finder Eclipse plug-in that automatically identifies both atomic and composite refactorings using a template-based refactoring reconstruction approach---it expresses each refactoring type in terms of template logic queries and uses a logic programming engine to infer concrete refactoring instances. Ref-Finder currently supports sixty three types in the Fowler's catalog, showing the most comprehensive coverage among existing techniques.}
}

@InProceedings{2009:icse:kim,
  Title                    = {Discovering and representing systematic code changes},
  Author                   = {Kim, Miryung and Notkin, David},
  Booktitle                = icse,
  Year                     = {2009},
  Pages                    = {309--319},

  Abstract                 = {Software engineers often inspect program differences when reviewing others' code changes, when writing check-in comments, or when determining why a program behaves differently from expected behavior after modification. Program differencing tools that support these tasks are limited in their ability to group related code changes or to detect potential inconsistencies in those changes. To overcome these limitations and to complement existing approaches, we built Logical Structural Diff (LSdiff), a tool that infers systematic structural differences as logic rules. LSdiff notes anomalies from systematic changes as exceptions to the logic rules. We conducted a focus group study with professional software engineers in a large E-commerce company; we also compared LSdiff's results with textual differences and with structural differences without rules. Our evaluation suggests that LSdiff complements existing differencing tools by grouping code changes that form systematic change patterns regardless of their distribution throughout the code, and its ability to discover anomalies shows promise in detecting inconsistent changes.}
}

@InProceedings{2006:msr:kim:a,
  Title                    = {Program element matching for multi-version program analyses},
  Author                   = {Miryung Kim and David Notkin},
  Booktitle                = msrw,
  Year                     = {2006},
  Pages                    = {58--64},

  Abstract                 = {Multi-version program analyses require that elements of one version of a program be mapped to the elements of other versions of that program. Matching program elements between two versions of a program is a fundamental building block for multi-version program analyses and other software evolution research such as profile propagation, regression testing, and software version merging.In this paper, we survey matching techniques that can be used for multi-version program analyses and evaluate them based on hypothetical change scenarios. This paper also lists challenges of the matching problem, identifies open problems, and proposes future directions.},
  Doi                      = {10.1145/1137983.1137999}
}

@InProceedings{2007:icse:kim:a,
  Title                    = {Automatic Inference of Structural Changes for Matching across Program Versions},
  Author                   = {Kim, Miryung and Notkin, David and Grossman, Dan},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {333--343},

  Abstract                 = {Mapping code elements in one version of a program to corresponding code elements in another version is a fundamental building block for many software engineering tools. Existing tools that match code elements or identify structural changes--refactorings and API changes--between two versions of a program have two limitations that we overcome. First, existing tools cannot easily disambiguate among many potential matches or refactoring candidates. Second, it is difficult to use these tools' results for various software engineering tasks due to an unstructured representation of results. To overcome these limitations, our approach represents structural changes as a set of high-level change rules, automatically infers likely change rules and determines method-level matches based on the rules. By applying our tool to several open source projects, we show that our tool identifies matches that are difficult to find using other approaches and produces more concise results than other approaches. Our representation can serve as a better basis for other software engineering tools.},
  Doi                      = {10.1109/ICSE.2007.20}
}

@InProceedings{2005:esec_fse:kim,
  Title                    = {An empirical study of code clone genealogies},
  Author                   = {Miryung Kim and Vibha Sazawal and David Notkin and Gail Murphy},
  Booktitle                = esec_fse,
  Year                     = {2005},
  Pages                    = {187--196},

  Abstract                 = {It has been broadly assumed that code clones are inherently bad and that eliminating clones by refactoring would solve the problems of code clones. To investigate the validity of this assumption, we developed a formal denition of clone evolution and built a clone genealogy tool that automatically extracts the history of code clones from a source code repository. Using our tool we extracted clone genealogy information for two Java open source projects and analyzed their evolution. Our study contradicts some conventional wisdom about clones. In particular, refactoring may not always improve software with respect to clones for two reasons. First, many code clones exist in the system for only a short time; extensive refactoring of such short-lived clones may not be worthwhile if they are likely diverge from one another very soon. Second, many clones, especially long-lived clones that have changed consistently with other elements in the same group, are not easily refactorable due to programming language limitations. These insights show that refactoring will not help in dealing with some types of clones and open up opportunities for complementary clone maintenance tools that target these other classes of clones.},
  Doi                      = {10.1145/1081706.1081737}
}

@InProceedings{2012:fse:kim,
  Title                    = {A field study of refactoring challenges and benefits},
  Author                   = {Kim, Miryung and Zimmermann, Thomas and Nagappan, Nachiappan},
  Booktitle                = fse,
  Year                     = {2012},
  Pages                    = {50:1--50:11},

  Abstract                 = {It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds that the binary modules refactored by this team experienced significant reduction in the number of inter-module dependencies and post-release defects, indicating a visible benefit of refactoring.},
  Doi                      = {10.1145/2393596.2393655}
}

@InProceedings{2005:wcre:kim,
  Title                    = {When Functions Change Their Names: Automatic Detection of Origin Relationships},
  Author                   = {Kim, Sunghun and Pan, Kai and Whitehead, Jr., E. James},
  Booktitle                = wcre,
  Year                     = {2005},
  Pages                    = {143--152},

  Abstract                 = {It is a common understanding that identifying the same entity such as module, file, and function between revisions is important for software evolution related analysis. Most software evolution researchers use entity names, such as file names and function names, as entity identifiers based on the assumption that each entity is uniquely identifiable by its name. Unfortunately names change over time. In this paper we propose an automated algorithm that identifies entity mapping at the function level across revisions even when an entity's name changes in the new revision. This algorithm is based on computing function similarities. We introduce eight similarity factors to determine if a function is renamed from a function. To find out which similarity factors are dominant, a significance analysis is performed on each factor. To validate our algorithm and for factor significance analysis, ten human judges manually identified renamed entities across revisions for two open source projects: Subversion and Apache2. Using the manually identified result set we trained weights for each similarity factor and measured the accuracy of our algorithm. We computed the accuracies among human judges. We found our algorithm's accuracy is better than the average accuracy among human judges. We also show that trained weights for similarity factors from one period in one project are reusable for other periods and/or other projects. Finally we combined all possible factor combinations and computed the accuracy of each combination. We found that adding more factors does not necessarily improve the accuracy of origin detection.}
}

@InProceedings{2006:msr:kim:b,
  Title                    = {How long did it take to fix bugs?},
  Author                   = {Kim, Sunghun and Whitehead, Jr., E. James},
  Booktitle                = msrw,
  Year                     = {2006},
  Pages                    = {173--174},

  Abstract                 = {The number of bugs (or fixes) is a common factor used to measure the quality of software and assist bug related analysis. For example, if software files have many bugs, they may be unstable. In comparison, the bug-fix time---the time to fix a bug after the bug was introduced---is neglected. We believe that the bug-fix time is an important factor for bug related analysis, such as measuring software quality. For example, if bugs in a file take a relatively long time to be fixed, the file may have some structural problems that make it difficult to make changes. In this report, we compute the bug-fix time of files in ArgoUML and PostgreSQL by identifying when bugs are introduced and when the bugs are fixed. This report includes bug-fix time statistics such as average bug-fix time, and distributions of bug-fix time. We also list the top 20 bug-fix time files of two projects.},
  Doi                      = {10.1145/1137983.1138027}
}

@Article{2008:tse:kim,
  Title                    = {Classifying Software Changes: Clean or Buggy?},
  Author                   = {Sunghun Kim and Whitehead, Jr., E. James and Yi Zhang},
  Journal                  = tse,
  Year                     = {2008},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {181--196},
  Volume                   = {34},

  Abstract                 = {This paper introduces a new technique for predicting latent software bugs, called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent buggy change recall on average. Change classification has several desirable qualities: 1) The prediction granularity is small (a change to a single file), 2) predictions do not require semantic information about the source code, 3) the technique works for a broad array of project types and programming languages, and 4) predictions can be made immediately upon the completion of a change. Contributions of this paper include a description of the change classification approach, techniques for extracting features from the source code and change histories, a characterization of the performance of change classification across 12 open source projects, and an evaluation of the predictive power of different groups of features.},
  Doi                      = {10.1109/TSE.2007.70773}
}

@InProceedings{2011:icse:kim,
  Title                    = {Dealing with noise in defect prediction},
  Author                   = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
  Booktitle                = icse,
  Year                     = {2011},
  Pages                    = {481--490},

  Abstract                 = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20\%--35\% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
  Doi                      = {10.1145/1985793.1985859}
}

@InProceedings{2006:ase:kim,
  Title                    = {Automatic Identification of Bug-Introducing Changes},
  Author                   = {Sunghun Kim and Thomas Zimmermann and Kai Pan and Whitehead, Jr., E. James},
  Booktitle                = ase,
  Year                     = {2006},
  Pages                    = {81--90},

  Doi                      = {10.1109/ASE.2006.23}
}

@InProceedings{2007:icse:kim:b,
  Title                    = {Predicting Faults from Cached History},
  Author                   = {Sunghun Kim and Thomas Zimmermann and Whitehead, Jr., E. James and Andreas Zeller},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {489--498},

  Abstract                 = {We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10\% of the source code files; these files account for 73\%--95\% of faults---a significant advance beyond the state of the art.},
  Doi                      = {10.1109/ICSE.2007.66}
}

@InProceedings{1998:apsec:kim,
  Title                    = {An Experience Report Related to Restructuring {OODesigner}: A {CASE} Tool for {OMT}},
  Author                   = {Kim, Taegyun and Boudjlida, Nacer},
  Booktitle                = apsec,
  Year                     = {1998},
  Pages                    = {220--227},

  Abstract                 = {This paper describes experience gained and lessons learned from restructuring OODesigner, a Computer Aided Software Engineering (CASE) tool for Object Modeling Technique (OMT). This tool supports a wide range of features such as constructing the three models in OMT, managing information repository, documenting class resources, automatically generating C++ and Java code, reverse engineering C++ and Java code, searching and reusing classes in the corresponding repository and collecting metrics data. We had developed the version 1.x of OODesigner during 3 pears since 1994. Although we had developed this version using OMT and C++, we recognized the potential maintenance problem that originated from the ill-designed class architecture. Thus we totally restructured that version during 12 months, and we got a new version that is much easier to maintain than the old one. In this paper, we present what we did to restructure it and what we gained after the restructuring, and we also present a brief overview of the major features of OODesigner.},
  Doi                      = {10.1109/APSEC.1998.733723}
}

@InProceedings{2011:ase:kimmig,
  Title                    = {Querying source code with natural language},
  Author                   = {Kimmig, Markus and Monperrus, Martin and Mezini, Mira},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {376--379},

  Abstract                 = {One common task of developing or maintaining software is searching the source code for information like specific method calls or write accesses to certain fields. This kind of information is required to correctly implement new features and to solve bugs. This paper presents an approach for querying source code with natural language.},
  Doi                      = {10.1109/ASE.2011.6100076}
}

@InProceedings{2004:icse:kitchenham,
  Title                    = {Evidence-Based Software Engineering},
  Author                   = {Kitchenham, Barbara A. and Dyb{\aa}, Tore and J{\o}rgensen, Magne},
  Booktitle                = icse,
  Year                     = {2004},
  Pages                    = {273--281},

  Abstract                 = {Objective: Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. Method: We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). Results: EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Conclusions: Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
  Doi                      = {10.1109/ICSE.2004.1317449}
}

@Article{2002:tse:kitchenham,
  Title                    = {Preliminary guidelines for empirical research in software engineering},
  Author                   = {Kitchenham, Barbara A. and Pfleeger, Shari Lawrence and Pickard, Lesley M. and Jones, Peter W. and Hoaglin, David C. and El Emam, Khaled and Rosenberg, Jarrett},
  Journal                  = tse,
  Year                     = {2002},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {721--734},
  Volume                   = {28},

  Abstract                 = {Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.},
  Doi                      = {10.1109/TSE.2002.1027796}
}

@Article{2007:software:kitchenham,
  Title                    = {Misleading Metrics and Unsound Analyses},
  Author                   = {Barbara Kitchenham and David Ross Jeffery and Colin Connaughton},
  Journal                  = software,
  Year                     = {2007},

  Month                    = mar # {/} # apr,
  Number                   = {2},
  Pages                    = {73--78},
  Volume                   = {24},

  Abstract                 = {Measurement is a valuable software-management support tool. Unfortunately, software measures are often not as useful as practitioners hope. In this article, we describe how an international standard, ISO/IEC 15393, gives inappropriate advice for measuring software engineering processes. We also show how this advice, when combined with the CMM/CMMI level 4 requirement for statistical process control, can encourage the use of misleading metrics and the adoption of inappropriate data aggregation and analysis techniques.},
  Doi                      = {10.1109/MS.2007.49}
}

@Article{1998:tpami:kittler,
  Title                    = {On combining classifiers},
  Author                   = {Josef Kittler and Mohamad Hatef and Robert P. W. Huin and Jiri Matas},
  Journal                  = tpami,
  Year                     = {1998},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {226--239},
  Volume                   = {20},

  Abstract                 = {We develop a common theoretical framework for combining classifiers which use distinct pattern representations and show that many existing schemes can be considered as special cases of compound classification where all the pattern representations are used jointly to make a decision. An experimental comparison of various classifier combination schemes demonstrates that the combination rule developed under the most restrictive assumptions-the sum rule-outperforms other classifier combinations schemes. A sensitivity analysis of the various schemes to estimation errors is carried out to show that this finding can be justified theoretically.},
  Doi                      = {10.1109/34.667881}
}

@Article{2002:physreve:klemm,
  Title                    = {Highly clustered scale-free networks},
  Author                   = {Konstantin Klemm and Victor M. Egu{\'\i}luz},
  Journal                  = physreve,
  Year                     = {2002},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {036123:1--036123:5},
  Volume                   = {65},

  Abstract                 = {We propose a model for growing networks based on a finite memory of the nodes. The model shows stylized features of real-world networks: power-law distribution of degree, linear preferential attachment of new links, and a negative correlation between the age of a node and its link attachment rate. Notably, the degree distribution is conserved even though only the most recently grown part of the network is considered. As the network grows, the clustering reaches an asymptotic value larger than that for regular lattices of the same average connectivity and similar to the one observed in the networks of movie actors, coauthorship in science, and word synonyms. These highly clustered scale-free networks indicate that memory effects are crucial for a correct description of the dynamics of growing networks.},
  Doi                      = {10.1103/PhysRevE.65.036123}
}

@Book{2003:book:kleppe,
  Title                    = {{MDA} Explained: The Model Driven Architecture: Practice and Promise},
  Author                   = {Kleppe, Anneke G. and Warmer, Jos and Bast, Wim},
  Publisher                = {Addison-Wesley Longman},
  Year                     = {2003}
}

@Article{2005:tosem:klint,
  Title                    = {Toward an engineering discipline for grammarware},
  Author                   = {Paul Klint and Ralf L{\"a}mmel and Chris Verhoef},
  Journal                  = tosem,
  Year                     = {2005},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {331--380},
  Volume                   = {14},

  Abstract                 = {Grammarware comprises grammars and all grammar-dependent software. The term grammar is meant here in the sense of all established grammar formalisms and grammar notations including context-free grammars, class dictionaries, and XML schemas as well as some forms of tree and graph grammars. The term grammar-dependent software refers to all software that involves grammar knowledge in an essential manner. Archetypal examples of grammar-dependent software are parsers, program converters, and XML document processors. Despite the pervasive role of grammars in software systems, the engineering aspects of grammarware are insufficiently understood. We lay out an agenda that is meant to promote research on increasing the productivity of grammarware development and on improving the quality of grammarware. To this end, we identify the problems with the current grammarware practices, the barriers that currently hamper research, and the promises of an engineering discipline for grammarware, its principles, and the research challenges that have to be addressed.},
  Doi                      = {10.1145/1072997.1073000}
}

@Article{2004:fcds:kmiecik,
  Title                    = {Transformations for Software Architecture Model Change},
  Author                   = {Kmiecik, A. and V. Ambriola},
  Journal                  = fcds,
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {329--344},
  Volume                   = {29},

  Abstract                 = {Architectural transformations make up the core of architectural design and play a crucial role in user requirements satisfaction. Because of their impact to the software structure and behavior, they directly influence the values of software quality attributes. For this reason they can become not only a way for well specified and automated software architecture development but also a mechanism for early stage software quality management. This paper introduces the notion of architectural transformations and their impact to the software architecture model. It points at the relationship among transformations, non-functional requirements and architectural styles. It gives the arguments for architectural change automation. A motivating example of how transformations can be employed to the custom software architect activities is also given.},
  Url                      = {http://fcds.cs.put.poznan.pl/FCDS2/ArticleDetails.aspx?articleId=69}
}

@InProceedings{2002:iwpc:knight,
  Title                    = {Program Comprehension Experiences with {GXL}: Comprehension for Comprehension},
  Author                   = {Knight, Claire and Munro, Malcolm},
  Booktitle                = iwpc,
  Year                     = {2002},
  Pages                    = {147--156},

  Abstract                 = {Tools are vital to support the various activities that form the many tasks that are part of the program comprehension process. In order that these tools are used and useful it is necessary that they support the activities of the user. This support must complement the work methods and activities of the user and not hinder them. Whilst features of good tools have been identified tool builders do not always adhere them to. It is important to consider whether needs have changed, and if those desirable properties need augmenting or revising. From experience of maintaining and enhancing an existing program comprehension tool for the purposes of participating in a re-engineering activity many lessons on tool support have been learned.}
}

@InProceedings{2009:gbrpr:knossow,
  Title                    = {Inexact Matching of Large and Sparse Graphs Using {L}aplacian Eigenvectors},
  Author                   = {Knossow, David and Sharma, Avinash and Mateus, Diana and Horaud, Radu},
  Booktitle                = gbrpr,
  Year                     = {2009},
  Pages                    = {144--153},

  Abstract                 = {In this paper we propose an inexact spectral matching algorithm that embeds large graphs on a low-dimensional isometric space spanned by a set of eigenvectors of the graph Laplacian. Given two sets of eigenvectors that correspond to the smallest non-null eigenvalues of the Laplacian matrices of two graphs, we project each graph onto its eigenenvectors. We estimate the histograms of these one-dimensional graph projections (eigenvector histograms) and we show that these histograms are well suited for selecting a subset of significant eigenvectors, for ordering them, for solving the sign-ambiguity of eigenvector computation, and for aligning two embeddings. This results in an inexact graph matching solution that can be improved using a rigid point registration algorithm. We apply the proposed methodology to match surfaces represented by meshes.},
  Doi                      = {10.1007/978-3-642-02124-4_15}
}

@InProceedings{2007:icse:ko,
  Title                    = {Information needs in collocated software development teams},
  Author                   = {Andrew J. Ko and Robert DeLine and Gina Venolia},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {344--353},

  Abstract                 = {Previous research has documented the fragmented nature of software development work. To explain this in more detail, we analyzed software developers' day-to-day information needs. We observed seventeen developers at a large software company and transcribed their activities in 90-minute sessions. We analyzed these logs for the information that developers sought, the sources that they used, and the situations that prevented information from being acquired. We identified twenty-one information types and cataloged the outcome and source when each type of information was sought. The most frequently sought information included awareness about artifacts and coworkers. The most often deferred searches included knowledge about design and program behavior, such as why code was written a particular way, what a program was supposed to do, and the cause of a program state. Developers often had to defer tasks because the only source of knowledge was unavailable coworkers.}
}

@Article{2010:tosem:ko,
  Title                    = {Extracting and answering why and why not questions about {J}ava program output},
  Author                   = {Ko, Andrew J. and Myers, Brad A.},
  Journal                  = tosem,
  Year                     = {2010},

  Month                    = aug,
  Number                   = {2},
  Pages                    = {4:1--4:36},
  Volume                   = {20},

  Abstract                 = {When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of ``why did and why didn't" questions extracted from the program's code and execution. The tool then finds one or more possible explanations for the output in question. These explanations are derived using a static and dynamic slicing, precise call graphs, reachability analyses, and new algorithms for determining potential sources of values. Evaluations of the tool on two debugging tasks showed that developers with the Whyline were three times more successful and twice as fast at debugging, compared to developers with traditional breakpoint debuggers. The tool has the potential to simplify debugging and program understanding in many software development contexts.},
  Doi                      = {10.1145/1824760.1824761}
}

@InProceedings{2006:vlhcc:ko,
  Title                    = {A Linguistic Analysis of How People Describe Software Problems},
  Author                   = {Ko, Andrew J. and Myers, Brad A. and Chau, Duen Horng},
  Booktitle                = vlhcc,
  Year                     = {2006},
  Pages                    = {127--134},

  Abstract                 = {There is little understanding of how people describe software problems, but a variety of tools solicit, manage, and analyze these descriptions in order to streamline software development. To inform the design of these tools and generate ideas for new ones, an study of nearly 200,000 bug report titles was performed. The titles of the reports generally described a software entity or behavior, its inadequacy, and an execution context, suggesting new designs for more structured report forms. About 95\% of noun phrases referred to visible software entities, physical devices, or user actions, suggesting the feasibility of allowing users to select these entities in debuggers and other tools. Also, the structure of the titles exhibited sufficient regularity to parse with an accuracy of 89\%, enabling a number of new automated analyses. These findings and others have many implications for tool design and software engineering.},
  Doi                      = {10.1109/VLHCC.2006.3}
}

@Article{2006:tseko,
  Title                    = {An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks},
  Author                   = {Ko, Andrew J. and Myers, Brad A. and Coblenz, Michael J. and Aung, Htet Htet},
  Journal                  = tse,
  Year                     = {2006},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {971--987},
  Volume                   = {32},

  Abstract                 = {Much of software developers' time is spent understanding unfamiliar code. To better understand how developers gain this understanding and how software development environments might be involved, a study was performed in which developers were given an unfamiliar program and asked to work on two debugging tasks and three enhancement tasks for 70 minutes. The study found that developers interleaved three activities. They began by searching for relevant code both manually and using search tools; however, they based their searches on limited and misrepresentative cues in the code, environment, and executing program, often leading to failed searches. When developers found relevant code, they followed its incoming and outgoing dependencies, often returning to it and navigating its other dependencies; while doing so, however, Eclipse's navigational tools caused significant overhead. Developers collected code and other information that they believed would be necessary to edit, duplicate, or otherwise refer to later by encoding it in the interactive state of Eclipse's package explorer, file tabs, and scroll bars. However, developers lost track of relevant code as these interfaces were used for other tasks, and developers were forced to find it again. These issues caused developers to spend, on average, 35 percent of their time performing the mechanics of navigation within and between source files. These observations suggest a new model of program understanding grounded in theories of information foraging and suggest ideas for tools that help developers seek, relate, and collect information in a more effective and explicit manner},
  Doi                      = {10.1109/TSE.2006.116}
}

@InProceedings{1991:chi:koenemann,
  Title                    = {Expert problem solving strategies for program comprehension},
  Author                   = {J{\"u}rgen Koenemann and Scott P. Robertson},
  Booktitle                = chi,
  Year                     = {1991},
  Pages                    = {125--130},

  Abstract                 = {Program comprehension is a complex problem solving process. We report on an experiment that studies expert programmers' comprehension behavior in the context of modifying a complex PASCAL program. Our data suggests that program comprehension is best understood as a goal-oriented, hypotheses-driven problem-solving process. Programmers follow a pragmatic as-needed rather than a systematic strategy, they restrict their understanding to those parts of a program they find relevant for a given task, and they use bottom-up comprehension only for directly relevant code and in cases of missing, insufficient, or failing hypotheses. These findings have important consequences for the design of cognitively adequate computer-aided software engineering tools.},
  Doi                      = {10.1145/108844.108863}
}

@Article{1995:joop:koenig,
  Title                    = {Patterns and antipatterns},
  Author                   = {Andrew Koenig},
  Journal                  = joop,
  Year                     = {1995},

  Month                    = mar # {/} # apr,
  Number                   = {1},
  Pages                    = {46--48},
  Volume                   = {8},

  Abstract                 = {TBD}
}

@InProceedings{1995:ecml:kohavi,
  Title                    = {The Power of Decision Tables},
  Author                   = {Ron Kohavi},
  Booktitle                = ecml,
  Year                     = {1995},
  Pages                    = {174--189}
}

@Article{2006:jsmerp:kolb,
  Title                    = {Refactoring a legacy component for reuse in a software product line: A case study},
  Author                   = {Kolb, Ronny and Muthig, Dirk and Patzke, Thomas and Yamauchi, Kazuyuki},
  Journal                  = jsmerp,
  Year                     = {2006},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {109--132},
  Volume                   = {18},

  Abstract                 = {Product lines are a promising approach to improve conceptually the productivity of the software development process and thus to reduce both the cost and time of developing and maintaining increasingly complex systems. An important issue in the adoption of the product-line approach is the migration of legacy software components, which have not been designed for reuse, systematically into reusable product-line components. This article describes activities performed to improve systematically the design and implementation of an existing software component in order to reuse it in a software product line. The activities are embedded in the application of Fraunhofer PuLSE-DSSA---an approach for defining domain-specific software architectures (DSSA) and product-line architectures. The component under investigation is the so-called Image Memory Handler (IMH), which is used in Ricoh's current products of office appliances such as copier machines, printers, and multi-functional peripherals. It is responsible for controlling memory usage and compressing and decompressing image data. Improvement of both the component's design and implementation are based on a systematic analysis and focused on increasing maintainability and reusability and hence suitability for use in a product line. As a result of the analysis and refactoring activities, the documentation and implementation of the component has been considerably improved as shown by quantitative data collected at the end of the activities. Despite a number of changes to the code, the external behavior of the component has been preserved without significantly affecting the performance.},
  Doi                      = {10.1002/smr.329}
}

@InProceedings{2006:gimm:kolovos,
  Title                    = {Model comparison: A foundation for model composition and model transformation testing},
  Author                   = {Kolovos, Dimitrios S. and Paige, Richard F. and Polack, Fiona A. C.},
  Booktitle                = gimm,
  Year                     = {2006},
  Pages                    = {13--20},

  Abstract                 = {In the context of Model Driven Development, Model Transformation and Model Composition are two essential model management tasks. In this paper, we demonstrate how both tasks can benefit, in different ways, from the automation of another fundamental task: Model Comparison. We derive requirements for a model comparison solution incrementally, and demonstrate a concrete rule-based model comparison approach we have developed in the context of a generic model merging language.},
  Doi                      = {10.1145/1138304.1138308}
}

@InProceedings{2000:popl:komondoor,
  Title                    = {Semantics-preserving procedure extraction},
  Author                   = {Komondoor, Raghavan and Horwitz, Susan},
  Booktitle                = popl,
  Year                     = {2000},
  Pages                    = {155--169},

  Abstract                 = {Procedure extraction is an important program transformation that can be used to make programs easier to understand and maintain, to facilitate code reuse, and to convert ``monolithic'' code to modular or object-oriented code. Procedure extraction involves the following steps: The statements to be extracted are identified (by the programmer or by a programming tool). If the statements are not contiguous, they are moved together so that they form a sequence that can be extracted into a procedure, and so that the semantics of the original code is preserved. The statements are extracted into a new procedure, and are replaced with an appropriate call. This paper addresses step 2: in particular, the conditions under which it is possible to move a set of selected statements together so that they become ``extractable'', while preserving semantics. Since semantic equivalence is, in general, undecidable, we identify sufficient conditions based on control and data dependences, and define an algorithm that moves the selected statements together when the conditions hold. We also include an outline of a proof that our algorithm is semantics-preserving. While there has been considerable previous work on procedure extraction, we believe that this is the first paper to provide an algorithm for semantics-preserving procedures extraction given an arbitrary set of selected statements in an arbitrary control-flow graph.},
  Doi                      = {10.1145/325694.325713}
}

@InProceedings{2012:vlhcc:kononenko,
  Title                    = {Automatically locating relevant programming help online},
  Author                   = {Oleksii Kononenko and David Dietrich and Rahul Sharma and Reid Holmes},
  Booktitle                = vlhcc,
  Year                     = {2012},
  Pages                    = {127--134},

  Abstract                 = {While maintaining software systems, developers often encounter compilation errors and runtime exceptions that they do not know how to solve. Solutions to these errors can often be found through discussions with other developers on the Internet. Unfortunately, many of these online discussions do not contain relevant answers. We have developed an approach to automatically query and analyze online discussions to locate relevant solutions to programming problems. Our tool, called Dora, is integrated into Visual Studio and allows developers to query and evaluate solutions within their development environment, enabling them to reduce context switching between their development tasks and their search sessions. We have performed a semi-controlled experiment to validate the utility of our search approach with 18 tasks, finding that our approach provides 55\% more relevant results than traditional web searching approaches.},
  Doi                      = {10.1109/VLHCC.2012.6344497}
}

@InProceedings{1997:wcre:kontogiannis,
  Title                    = {Evaluation Experiments on the Detection of Programming Patterns Using Software Metrics},
  Author                   = {Kontogiannis, Kostas A.},
  Booktitle                = wcre,
  Year                     = {1997},
  Pages                    = {44--54},

  Doi                      = {10.1109/WCRE.1997.624575}
}

@InProceedings{2006:icsm:kontogiannis,
  Title                    = {Comprehension and Maintenance of Large-Scale Multi-Language Software Applications},
  Author                   = {Kostas Kontogiannis and Panos Linos and Kenny Wong},
  Booktitle                = icsm,
  Year                     = {2006},
  Pages                    = {497--500},

  Abstract                 = {During the last decade, the number of software applications that have been deployed as a set of components built using different programming languages and paradigms has increased considerably. When such applications are maintained, traditional program comprehension and reengineering techniques may not be adequate. Hence, this working session aims to stimulate discussion around key issues relating to the comprehension, re engineering, and maintenance of multi-language software applications. Such issues include, but are not limited to, the formalization, management, exploration, and presentation of multi-language program dependencies, as well as the development of practical toolsets to automate and ease the comprehension and maintenance of multi-language software.}
}

@InProceedings{2000:ipdps:konuru,
  Title                    = {Deterministic replay of distributed {J}ava applications},
  Author                   = {Konuru, Ravi and Srinivasan, Harini and Choi, Jong-Deok},
  Booktitle                = ipdps,
  Year                     = {2000},
  Pages                    = {219--227}
}

@Article{1997:spe:koppler,
  Title                    = {A systematic approach to fuzzy parsing},
  Author                   = {Rainer Koppler},
  Journal                  = spe,
  Year                     = {1997},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {637--649},
  Volume                   = {27},

  Abstract                 = {This paper presents a systematic approach to implementing fuzzy parsers. A fuzzy parser is a form of syntax analyzer that performs analysis on selected portions of its input rather than performing a detailed analysis of a complete source text. Fuzzy parsers are often components of software tools and also of program development environments that extract information from source texts. This paper clarifies the term `fuzzy parser' and introduces an object-oriented framework for implementing reusable and efficient fuzzy parsers. Applications of this framework are described via examples of two software tools. These tools exploit the facilities provided by fuzzy parsers for different purposes, and also for different languages.},
  Doi                      = {10.1002/(SICI)1097-024X(199706)27:6<637::AID-SPE99>3.0.CO;2-3}
}

@Article{2010:software:koru,
  Title                    = {The Theory of Relative Dependency: Higher Coupling Concentration in Smaller Modules},
  Author                   = {Koru, A. G{\"u}ne{\c{s}} and El Emam, Khaled},
  Journal                  = software,
  Year                     = {2010},

  Month                    = mar # {/} # apr,
  Number                   = {2},
  Pages                    = {81--89},
  Volume                   = {27},

  Abstract                 = {Our recent research on several large-scale software products has consistently shown that smaller modules are proportionally more defect prone. These findings challenge the common recommendations from the literature suggesting that quality assurance (QA) and quality control (QC) resources should focus on larger modules. Those recommendations are based on the unfounded assumption that a monotonically increasing linear relationship exists between module size and defects. Given that complexity is correlated with size, following such recommendations ensures that the more complex modules receive greater scrutiny. However, our recent findings imply that, given limited and a fixed amount of resources, focusing on the smaller modules would lead to more effective defect detection. So, it's important to understand the mechanisms behind those findings to make the case for amending current practices.},
  Doi                      = {10.1109/MS.2010.58}
}

@Article{2007:jmir:koru,
  Title                    = {A Survey of Quality Assurance Practices in Biomedical Open Source Software Projects},
  Author                   = {Koru, G{\"u}nes and El Emam, Khaled and Neisa, Angelica and Umarji, Medha},
  Journal                  = jmir,
  Year                     = {2007},

  Month                    = apr # {--} # jun,
  Number                   = {2},
  Volume                   = {9},

  Abstract                 = {Background: Open source (OS) software is continuously gaining recognition and use in the biomedical domain, for example, in health informatics and bioinformatics.
Objectives: Given the mission critical nature of applications in this domain and their potential impact on patient safety, it is important to understand to what degree and how effectively biomedical OS developers perform standard quality assurance (QA) activities such as peer reviews and testing. This would allow the users of biomedical OS software to better understand the quality risks, if any, and the developers to identify process improvement opportunities to produce higher quality software.
Methods: A survey of developers working on biomedical OS projects was conducted to examine the QA activities that are performed. We took a descriptive approach to summarize the implementation of QA activities and then examined some of the factors that may be related to the implementation of such practices.
Results: Our descriptive results show that 63\% (95\% CI, 54--72) of projects did not include peer reviews in their development process, while 82\% (95\% CI, 75--89) did include testing. Approximately 74\% (95\% CI, 67--81) of developers did not have a background in computing, 80% (95% CI, 74-87) were paid for their contributions to the project, and 52\% (95\% CI, 43--60) had PhDs. A multivariate logistic regression model to predict the implementation of peer reviews was not significant (likelihood ratio test = 16.86, 9 df, P = .051) and neither was a model to predict the implementation of testing (likelihood ratio test = 3.34, 9 df, P = .95).
Conclusions: Less attention is paid to peer review than testing. However, the former is a complementary, and necessary, QA practice rather than an alternative. Therefore, one can argue that there are quality risks, at least at this point in time, in transitioning biomedical OS software into any critical settings that may have operational, financial, or safety implications. Developers of biomedical OS applications should invest more effort in implementing systemic peer review practices throughout the development and maintenance processes.},
  Doi                      = {10.2196/jmir.9.2.e8}
}

@InProceedings{2006:wcre:koschke,
  Title                    = {Clone Detection Using Abstract Syntax Suffix Trees},
  Author                   = {Koschke, Rainer and Falke, Raimar and Frenzel, Pierre},
  Booktitle                = wcre,
  Year                     = {2006},
  Pages                    = {253--262},

  Abstract                 = {Reusing software through copying and pasting is a continuous plague in software development despite the fact that it creates serious maintenance problems. Various techniques have been proposed to find duplicated redundant code (also known as software clones). A recent study has compared these techniques and shown that token-based clone detection based on suffix trees is extremely fast but yields clone candidates that are often no syntactic units. Current techniques based on abstract syntax trees---on the other hand---find syntactic clones but are considerably less efficient. This paper describes how we can make use of suffix trees to find clones in abstract syntax trees. This new approach is able to find syntactic clones in linear time and space. The paper reports the results of several large case studies in which we empirically compare the new technique to other techniques using the Bellon benchmark for clone detectors.}
}

@Article{2008:dma:kostylev,
  Title                    = {On the complexity of the anti-unification problem},
  Author                   = {Kostylev, E. V. and Zakharov, V. A.},
  Journal                  = dma,
  Year                     = {2008},

  Month                    = may,
  Number                   = {1},
  Pages                    = {85--98},
  Volume                   = {18},

  Abstract                 = {In this paper we suggest a new algorithm of anti-unification of logic terms represented by acyclic directed graphs and estimate its complexity. The anti-unification problem consists of the following: for two given terms find the most specific term that has the given terms as instances. We suggest an anti-unification algorithm whose complexity linearly depends on the size of the most specific term it computes. It is thus established that the anti-unification problem is of almost the same complexity as the unification problem. It is also shown that there exist terms whose most specific term is of size $O(n^2)$, where $n$ is the size of the graphs representing these terms.},
  Doi                      = {10.1515/DMA.2008.007}
}

@Article{1992:tse:kozaczynski,
  Title                    = {Program concept recognition and transformation},
  Author                   = {Kozaczynski, Wojtek and Ning, Jim and Engberts, Andre},
  Journal                  = tse,
  Year                     = {1992},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {1065--1075},
  Volume                   = {18},

  Abstract                 = {The automated recognition of abstract high-level conceptual information or concepts, which can greatly aid the understanding of programs and therefore support many software maintenance and reengineering activities, is considered. An approach to automated concept recognition and its application to maintenance-related program transformations are described. A unique characteristic of this approach is that transformations of code can be expressed as transformations of abstract concepts. This significantly elevates the level of transformation specifications.},
  Doi                      = {10.1109/32.184761}
}

@InProceedings{2009:wcre:kpodjedo,
  Title                    = {Approximate Graph Matching in Software Engineering},
  Author                   = {Kpodjedo, Segla},
  Booktitle                = wcre,
  Year                     = {2009},
  Pages                    = {295--298},

  Abstract                 = {Graph representations are widely adopted in many different areas to modelize objects or problems. In software engineering, many produced artifacts can be thought of as graphs and generic graph algorithms may be useful in many different contexts. Our research project is aimed at addressing the generic approximate graph matching and apply developed algorithms to software engineering problems.},
  Doi                      = {10.1109/WCRE.2009.49}
}

@InProceedings{2010:evocop:kpodjedo,
  Title                    = {Enhancing a tabu algorithm for approximate graph matching by using similarity measures},
  Author                   = {Kpodjedo, Segla and Galinier, Philippe and Antoniol, Giulio},
  Booktitle                = evocop,
  Year                     = {2010},
  Pages                    = {119--130},

  Abstract                 = {In this paper, we investigate heuristics in order to solve the Approximated Matching Problem (AGM). We propose a tabu search algorithm which exploits a simple neighborhood but is initialized by a greedy procedure which uses a measure of similarity between the vertices of the two graphs. The algorithm is tested on a large collection of graphs of various sizes (from 300 vertices and up to 3000 vertices) and densities. Computing times range from less than 1 second up to a few minutes. The algorithm obtains consistently very good results, especially on labeled graphs. The results obtained by the tabu algorithm alone (without the greedy procedure) were very poor, illustrating the importance of using vertex similarity during the early steps of the search process.},
  Doi                      = {10.1007/978-3-642-12139-5_11}
}

@InProceedings{2008:wcre:kpodjedo,
  Title                    = {Error Correcting Graph Matching Application to Software Evolution},
  Author                   = {Kpodjedo, Segla and Ricca, Filippo and Galinier, Philippe and Antoniol, Giuliano},
  Booktitle                = wcre,
  Year                     = {2008},
  Pages                    = {289--293},

  Abstract                 = {Graph representations and graph algorithms are widely adopted to model and resolve problems in many different areas from telecommunications, to bio-informatics, to civil and software engineering. Many software artefacts such as the class diagram can be thought of as graphs and thus, many software evolution problems can be reformulated as a graph matching problem.In this paper, we investigate the applicability of an error-correcting graph matching algorithm to object-oriented software evolution and report results, obtained on a small system---the Latazza application---supporting applicability and usefulness of our proposal.},
  Doi                      = {10.1109/WCRE.2008.48}
}

@Article{2007:cacm:kramer,
  Title                    = {Is Abstraction the Key to Computing?},
  Author                   = {Kramer, Jeff},
  Journal                  = cacm,
  Year                     = {2007},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {36--42},
  Volume                   = {50},

  Doi                      = {10.1145/1232743.1232745}
}

@InProceedings{1999:icsm:krikhaar,
  Title                    = {A Two-Phase Process for Software Architecture Improvement},
  Author                   = {Krikhaar, R. and Postma, A. and Sellink, A. and Stroucken, M. and Verhoef, C.},
  Booktitle                = icsm,
  Year                     = {1999},
  Pages                    = {371--380},

  Abstract                 = {Software architecture is important for large systems in which it is the main means for, among other things, controlling complexity. Current ideas on software architectures were not available more than ten years ago. Software developed at that time has been deteriorating from an architectural point of view over the years, as a result of adaptations made in the software because of changing system requirements. Parts of the old software are nevertheless still being used in new product lines. To make changes in that software, like adding features, it is imperative to first adapt the software to accommodate those changes. Architecture improvement of existing software is therefore becoming more and more important. This paper describes a two-phase process for software architecture improvement, which is the synthesis of two research areas: the architecture visualisation and analysis area of Philips Research, and the transformation engines and renovation factories area of the University of Amsterdam. Software architecture transformation plays an important role, and is to our knowledge a new research topic. Phase one of the process is based on Relation Partition Algebra (RPA). By lifting the information to higher levels of abstraction and calculating metrics over the system, all kinds of quality aspects can be investigated. Phase two is based on formal transformation techniques on abstract syntax trees. The software architecture improvement process allows for a fast feedback loop on results, without the need to deal with the complete software and without any interference with the normal development process.}
}

@InProceedings{2007:scam:krinke,
  Title                    = {Statement-level cohesion metrics and their visualization},
  Author                   = {Krinke, Jens},
  Booktitle                = scam,
  Year                     = {2007},
  Pages                    = {37--46},

  Abstract                 = {Slice-based metrics for cohesion have been defined and examined for years. However, if a module with low cohesion has been identified, the metrics cannot help the maintainer to restructure the module to improve the cohesion. This work presents statement-level cohesion metrics based on slices and chops. When visualized, the statement-level cohesion metrics can show which parts of a module have a low cohesion and thus help the maintainer to identify the parts that should be restructured.},
  Doi                      = {10.1109/SCAM.2007.28}
}

@InProceedings{2010:msr:krinke,
  Title                    = {Cloning and copying between {GNOME} projects},
  Author                   = {Krinke, Jens and Gold, Nicolas and Jia, Yue and Binkley, David},
  Booktitle                = msrwc,
  Year                     = {2010},
  Pages                    = {98--101},

  Abstract                 = {This paper presents an approach to automatically distinguish the copied clone from the original in a pair of clones. It matches the line-by-line version information of a clone to the pair's other clone. A case study on the GNOME Desktop Suite revealed a complex flow of reused code between the different subprojects. In particular, it showed that the majority of larger clones (with a minimal size of 28 lines or higher) exist between the subprojects and more than 60\% of the clone pairs can be automatically separated into original and copy.},
  Doi                      = {10.1109/MSR.2010.5463290}
}

@Article{2004:spe:krissinel,
  Title                    = {Common subgraph isomorphism detection by backtracking search},
  Author                   = {Krissinel, Evgeny B. and Henrick, Kim},
  Journal                  = spe,
  Year                     = {2004},

  Month                    = may,
  Number                   = {6},
  Pages                    = {591--607},
  Volume                   = {34},

  Abstract                 = {Graph theory offers a convenient and highly attractive approach to various tasks of pattern recognition. Provided there is a graph representation of the object in question (e.g. a chemical structure or protein fold), the recognition procedure is reduced to the problem of common subgraph isomorphism (CSI). Complexity of this problem shows combinatorial dependence on the size of input graphs, which in many practical cases makes the approach computationally intractable. Among the optimal algorithms for CSI, the leading place in practice belongs to algorithms based on maximal clique detection in the association graph. Backtracking algorithms for CSI, first developed two decades ago, are rarely used. We propose an improved backtracking algorithm for CSI, which differs from its predecessors by better search strategy and is therefore more efficient. We found that the new algorithm outperforms the traditional maximal clique approach by orders of magnitude in computational time.},
  Doi                      = {10.1002/spe.588}
}

@Book{2012:book:krogmann,
  Title                    = {Reconstruction of Software Component Architectures and Behaviour Models Using Static and Dynamic Analysis},
  Author                   = {Klaus Krogmann},
  Publisher                = {KIT Scientific Publishing},
  Year                     = {2012}
}

@Article{2009:software:kruchten,
  Title                    = {The Decision View's Role in Software Architecture Practice},
  Author                   = {Kruchten, Philippe and Capilla, Rafael and Due{\~n}as, Juan Carlos},
  Journal                  = software,
  Year                     = {2009},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {36--42},
  Volume                   = {26},

  Abstract                 = {Software development has to deal with many challenges---increasing system complexity, requests for better quality, the burden of maintenance operations, distributed production, and high staff turnover, to name just a few. Increasingly, software companies that strive to reduce their products' maintenance costs demand flexible, easy-to-maintain designs. Software architecture constitutes the cornerstone of software design, key for facing these challenges. Several years after the "software crisis" began in the mid-1970s, software architecture practice emerged as a mature (although still growing) discipline, capable of addressing the increasing complexity of new software systems.},
  Doi                      = {10.1109/MS.2009.52}
}

@Article{1995:software:kruchten,
  Title                    = {The 4+1 View Model of architecture},
  Author                   = {Phillippe B. Kruchten},
  Journal                  = software,
  Year                     = {1995},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {42--50},
  Volume                   = {12},

  Abstract                 = {The 4 + 1 View Model describes software architecture using five concurrent views, each of which addresses a specific set of concerns: The logical view describes the design's object model, the process view describes the design's concurrency and synchronization aspects; the physical view describes the mapping of the software onto the hardware and shows the system's distributed aspects, and the development view describes the software's static organization in the development environment. Software designers can organize the description of their architectural decisions around these four views and then illustrate them with a few selected use cases, or scenarios, which constitute a fifth view. The architecture is partially evolved from these scenarios.The 4+1 View Model allows various stakeholders to find what they need in the software architecture. System engineers can approach it first from the physical view, then the process view; end users, customers, and data specialists can approach it from the logical view; and project managers and software-configuration staff members can approach it from the development view.},
  Doi                      = {10.1109/52.469759}
}

@InProceedings{2004:pfe:krueger,
  Title                    = {Towards a Taxonomy for Software Product Lines},
  Author                   = {Charles W. Krueger},
  Booktitle                = pfe,
  Year                     = {2004},
  Pages                    = {323--331},
  Series                   = lncs,
  Volume                   = {3014},

  Abstract                 = {Drawing from the growing number of software product line experiences and case studies, this report describes a taxonomy for characterizing different software product line approaches. The taxonomy helps to illuminate the general principles of software product line engineering and the effectiveness of different software product line solutions in different situations.},
  Doi                      = {10.1007/978-3-540-24667-1\_25}
}

@InProceedings{2000:asset:krueger,
  Title                    = {Software product line reuse in practice},
  Author                   = {Charles W. Krueger},
  Booktitle                = asset,
  Year                     = {2000},
  Pages                    = {117--118},

  Abstract                 = {The study of software product line addresses the issues of engineering software systems families, or collections of similar software systems. The objective of a software product line is to reduce the overall engineering effort required to produce a collection of similar systems by capitalizing on the commonality among the systems and by formally managing the variation among the systems. This is a classic software reuse problem (C. Krueger, 1992). The primary focus of software product line research has been on domain analysis and modeling, architecture modeling, software reuse repositories, generators, and process definition. In contrast, for engineering single systems, these technologies and techniques provide significant value, while for engineering software product lines the conventional wisdom suggests they are essential. We hypothesize, however, that domain analysis, architecture modeling, software reuse repositories, generators, and process definition are simply compensating for a void in the existing software engineering technology space and that this void can be filled by a more simple, powerful, and concise technology for engineering software product lines. This new technology then redefines the status of domain analysis and the other existing technologies so that they have the same status in software product line development that they do in single system development: valuable engineering technologies but not necessarily requisite.},
  Doi                      = {10.1109/ASSET.2000.888062}
}

@Article{1992:csur:krueger,
  Title                    = {Software reuse},
  Author                   = {Charles W. Krueger},
  Journal                  = csur,
  Year                     = {1992},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {131--183},
  Volume                   = {24},

  Abstract                 = {Software reuse is the process of creating software systems from existing software rather than building software systems from scratch. This simple yet powerful vision was introduced in 1968. Software reuse has, however, failed to become a standard software engineering practice. In an attempt to understand why, researchers have renewed their interest in software reuse and in the obstacles to implementing it. This paper surveys the different approaches to software reuse found in the research literature. It uses a taxonomy to describe and compare the different approaches and make generalizations about the field of software reuse. The taxonomy characterizes each reuse approach in terms of its reusable artifacts and the way these artifacts are abstracted, selected, specialized, and integrated. Abstraction plays a central role in software reuse. Concise and expressive abstractions are essential if software artifacts are to be effectively reused. The effectiveness of a reuse technique can be evaluated in terms of cognitive distance---an intuitive gauge of the intellectual effort required to use the technique. Cognitive distance is reduced in two ways: (1) Higher level abstractions in a reuse technique reduce the effort required to go from the initial concept of a software system to representations in the reuse technique, and (2) automation reduces the effort required to go from abstractions in a reuse technique to an executable implementation. This survey will help answer the following questions: What is software reuse? Why reuse software? What are the different approaches to reusing software? How effective are the different approaches? What is required to implement a software reuse technology? Why is software reuse difficult? What are the open areas for research in software reuse?},
  Doi                      = {10.1145/130844.130856}
}

@InProceedings{2007:ajcai:krumnack,
  Title                    = {Restricted higher-order anti-unification for analogy making},
  Author                   = {Krumnack, Ulf and Schwering, Angela and Gust, Helmar and K\"{u}hnberger, Kai-Uwe},
  Booktitle                = ajcai,
  Year                     = {2007},
  Pages                    = {273--282},

  Abstract                 = {Anti-unification has often be used as a tool for analogy making. But while first-order anti-unification is too simple for many applications, general higher-order anti-unification is too complex and leads into theoretical difficulties. In this paper we present a restricted framework for higher-order substitutions and show that anti-unification is well-defined in this setting. A complexity measure for generalizations can be introduced in a quite natural way, which allows for selecting preferred generalizations. An algorithm for computing such generalizations is presented and the utility of complexity for anti-unifying sets of terms is discussed by an extended example.}
}

@Article{1952:jasa:kruskal,
  Title                    = {Use of ranks in one-criterion variance analysis},
  Author                   = {William H. Kruskal and W. Allen Wallis},
  Journal                  = jasa,
  Year                     = {1952},
  Number                   = {260},
  Pages                    = {583--621},
  Volume                   = {47},

  Abstract                 = {Given $C$ samples, with $n_i$ observations in the $i$th sample, a test of the hypothesis that the samples are from the same population may be made by ranking the observations from from 1 to $\Sigma n_i$ (giving each observation in a group of ties the mean of the ranks tied for), finding the $C$ sums of ranks, and computing a statistic $H$. Under the stated hypothesis, $H$ is distributed approximately as $\chi^2(C - 1)$, unless the samples are too small, in which case special approximations or exact tables are provided. One of the most important applications of the test is in detecting differences among the population means.},
  Doi                      = {10.1080/01621459.1952.10483441}
}

@Article{2006:pcs:ksenzov,
  Title                    = {Architectural refactoring of corporate program systems},
  Author                   = {Ksenzov, M. V.},
  Journal                  = pcs,
  Year                     = {2006},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {31--43},
  Volume                   = {32},

  Abstract                 = {This paper is devoted to the maintenance and improvement of inherited software. A technique of iterative transformations of the software architecture (also known as architectural refactoring) is used as a key organizing principle of the software maintenance and improvement. Basic problems associated with scaling this technique and methods for solving them are considered. Particular emphasis is placed on corporate software, specific features of its architecture, and on the application of the architectural refactoring to this class of software.},
  Doi                      = {10.1134/S036176880601004X}
}

@Article{2007:ist:kuhn,
  Title                    = {Semantic clustering: Identifying topics in source code},
  Author                   = {Kuhn, Adrian and Ducasse, St{\'e}phane and G{\^\i}rba, Tudor},
  Journal                  = ist,
  Year                     = {2007},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {230--243},
  Volume                   = {49},

  Abstract                 = {Many of the existing approaches in Software Comprehension focus on program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce Semantic Clustering, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper. Note: Some of the visualizations presented make heavy use of colors. Please obtain a color copy of the article for better understanding.},
  Doi                      = {10.1016/j.infsof.2006.10.017}
}

@Manual{2011:manual:caret,
  Title                    = {caret: Classification and Regression Training},
  Author                   = {Max Kuhn},
  Note                     = {R package version 4.76},

  Url                      = {http://cran.rproject.org/web/packages/caret/caret.pdf}
}

@InProceedings{2011:isec:kumar,
  Title                    = {Architectural refactoring of a mission critical integration application: A case study},
  Author                   = {Kumar, M. Raveendra and Kumar, R. Hari},
  Booktitle                = isec,
  Year                     = {2011},
  Pages                    = {77--83},

  Abstract                 = {Architecture refactoring can be thought of as a process of changing the architecture of an existing application without altering the functional behavior. It is to improve the operational and developmental quality attributes such as performance, stability, complexity and maintainability. Due to changing business expectations, as well as runtime environments, architecture refactoring may need to be done incrementally, over multiple software release cycles. In this industry experience report we present an architectural refactoring of a mission critical integration application, after it was originally implemented. We discuss the drivers behind the refactoring and the approach. Initial results from the refactoring show significant improvement of stability and throughput of the system. We also present recommendations on architectural refactoring, and discuss the importance of key performance indicators that drive architecture refactoring.}
}

@InProceedings{2009:psi:kumar,
  Title                    = {Using {AOP} for Discovering and Defining Executable Test Cases},
  Author                   = {Philipp Kumar and Thomas Baar},
  Booktitle                = psi,
  Year                     = {2010},
  Pages                    = {269--281},
  Series                   = lncs,
  Volume                   = {5947},

  Abstract                 = {The functional specification of software systems is often given in form of use cases. The compliance of a system implementation to a use case specification is validated by system tests, which can nowadays be automated. Unfortunately, system tests run slowly and assume that all needed external systems such as databases and authentication servers are accessible and run correctly. For these reasons, software developers rather rely on module tests, which test the functionality of individual modules. Module tests, however, have the disadvantage of being defined independently of the customer's system specification. In this paper, we describe an approach to combine the advantages of system and module tests. The core technique is (i) to record the complete information flow between modules during the execution of system tests and (ii) to generate, based on this data, corresponding module tests afterwards. The resulting module tests are fast, they are independent of external systems, and they reflect test scenarios defined in use cases. Our approach is heavily based on aspect-oriented programming (AOP) and implemented by the open-source framework jautomock.},
  Doi                      = {10.1007/978-3-642-11486-1_23}
}

@InProceedings{1999:wicsa:kuusela,
  Title                    = {Architectural Evolution},
  Author                   = {Kuusela, Juha},
  Booktitle                = wicsa,
  Year                     = {1999},
  Pages                    = {471--478},

  Abstract                 = {TBD}
}

@InProceedings{2001:esec_fse:luer,
  Title                    = {Wren: An Environment for Component-Based Development},
  Author                   = {Chris L{\"u}er and David S. Rosenblum},
  Booktitle                = esec_fse,
  Year                     = {2001},
  Pages                    = {207--217},

  Abstract                 = {Prior research in software environments focused on three important problems---tool integration, artifact management, and process guidance. The context for that research, and hence the orientation of the resulting environments, was a traditional model of development in which an application is developed completely from scratch by a single organization. A notable characteristic of component-based development is its emphasis on integrating independently developed components produced by multiple organizations. Thus, while component-based development can benefit from the capabilities of previous generations of environments, its special nature induces requirements for new capabilities not found in previous environments. This paper is concerned with the design of component-based development environments, or CBDEs. We identify seven important requirements for CBDEs and discuss their rationale, and we describe a prototype environment called WREN that we are building to implement these requirements and to further evaluate and study the role of environment technology in component-based development. Important capabilities of the environment include the ability to locate potential components of interest from component distribution sites, to evaluate the identified components for suitability to an application, to incorporate selected components into application design models, and to physically integrate selected components into the application.}
}

@InProceedings{2005:icse:lago,
  Title                    = {Explicit assumptions enrich architectural models},
  Author                   = {Patricia Lago and van Vliet, Hans},
  Booktitle                = icse,
  Year                     = {2005},
  Pages                    = {206--214},

  Abstract                 = {Design for change is a well-known adagium in software engineering. We separate concerns, employ well-designed interfaces, and the like to ease evolution of the systems we build. We model and build in changeability through parameterization and variability points (as in product lines). These all concern places where we explicitly consider variability in our systems. We conjecture that it is helpful to also think of and explicitly model invariability, things in our systems and their environment that we assume will not change. We give examples from the literature and our own experience to illustrate how evolution can be seriously hampered because of tacit assumptions made. In particular, we show how we can explicitly model assumptions in an existing product family. From this, we derive a metamodel to document assumptions. Finally, we show how this type of modeling adds to our understanding of the architecture and the decisions that led to it.}
}

@InProceedings{1999:mdsoc:lai,
  Title                    = {The Structure of Features in {J}ava Code: An Exploratory Investigation},
  Author                   = {Albert Lai and Gail C. Murphy},
  Booktitle                = mdsoc,
  Year                     = {1999},
  Note                     = {6~pages},

  Abstract                 = {Techniques to help software developers explicitly separate different concerns in their programs have been gaining increasing attention in the last few years. Three examples of such techniques are composition filters [ABV92], aspect-oriented programming [KLM+97], and hyperspaces [TOHSJ99]. A central idea behind these techniques is that software systems would be easier to change if various design and programming decisions were modularized separately and simultaneously. But what is a concern? And how do different concerns interact within a code base? To gain some insight into these questions, we have conducted an exploratory investigation of concerns in two existing Java [GJS96] packages: gnu.regexp, and jFTPd. Each of the packages was marked for concerns by each author of this position paper. We then compared the concerns identified and analyzed how the concerns interacted with each other and across the existing structure of the Java package.}
}

@InCollection{2003:book:juristo:laitenberger,
  Title                    = {({Q}uasi-)experimental studies in industrial settings},
  Author                   = {Laitenberger, Oliver and Rombach, Dieter},
  Booktitle                = {Lecture Notes on Empirical Software Engineering},
  Publisher                = {World Scientific},
  Year                     = {2003},
  Editor                   = {Juristo, Natalia and Moreno, Ana M.},
  Pages                    = {167--227},

  Abstract                 = {Software engineering research primarily deals with technologies that promise software organisations to deliver high quality products on time and within budget. However, in many cases researchers do not investigate the validity of the promises and, therefore, information about the relative strengths and weaknesses of those technologies in comparison with the already existing ones is often missing. Although experimentation and experimental software engineering have been suggested to address this issue and significant progress has been made throughout the last couple of years in this area, there is still a lack of experimental work in industrial settings. The reasons for this poor situation range from practical constraints, such as, the costs associated with a study and the benefits for a single company, to more methodological ones, such as the level of control that can be imposed on the different treatment conditions in an industrial setting. In this chapter we present an practical approach that helps overcome most of the objections. The approach represents a balance between the benefits for practitioners and methodological rigor. In essence, it uses training situations to set up and run empirical studies. While this procedure disqualifies the study as a pure controlled experiment, the characteristics of a quasi-experiment can often be preserved. The chapter explains the principle of the approach, differences between controlled experiments and quasi-experiments and finally, presents an example of a quasi-experiment in an industrial setting.},
  Url                      = {http://dl.acm.org/citation.cfm?id=899834.899840}
}

@InProceedings{2008:wicsa:lamantia,
  Title                    = {Analyzing the Evolution of Large-Scale Software Systems Using Design Structure Matrices and Design Rule Theory: Two Exploratory Cases},
  Author                   = {LaMantia, Matthew J. and Yuanfang Cai and MacCormack, Alan D. and Rusnak, John},
  Booktitle                = wicsa,
  Year                     = {2008},
  Pages                    = {83--92},

  Abstract                 = {Designers have long recognized the value of modularity, but important software modularity principles have remained informal. According to Baldwin and Clark's [1] design rule theory (DRT) , modular architectures add value to system designs by creating options to improve the system by substituting or experimenting on individual modules. In this paper, we examine the design evolution of two software product platforms through the modeling lens of DRT and design structure matrices (DSMs). We show that DSM models and DRT precisely explain how real-world modularization activities in one case allowed for different rates of evolution in different software modules and in another case conferred distinct strategic advantages on a firm by permitting substitution of an at-risk software module without substantial change to the rest of the system. Our results provide positive evidence that DSM and DRT can inform important aspects of large-scale software structure and evolution, having the potential to guide software architecture design activities.}
}

@InProceedings{2011:csmr:lamkanfi,
  Title                    = {Comparing Mining Algorithms for Predicting the Severity of a Reported Bug},
  Author                   = {Lamkanfi, Ahmed and Demeyer, Serge and Soetens, Quinten David and Verdonck, Tim},
  Booktitle                = csmr,
  Year                     = {2011},
  Pages                    = {249--258},

  Abstract                 = {A critical item of a bug report is the so-called ``severity", i.e. the impact the bug has on the successful execution of the software system. Consequently, tool support for the person reporting the bug in the form of a recommender or verification system is desirable. In previous work we made a first step towards such a tool: we demonstrated that text mining can predict the severity of a given bug report with a reasonable accuracy given a training set of sufficient size. In this paper we report on a follow-up study where we compare four well-known text mining algorithms (namely, Naive Bayes, Naive Bayes Multinomial, K-Nearest Neighbor and Support Vector Machines) with respect to accuracy and training set size. We discovered that for the cases under investigation (two open source systems: Eclipse and GNOME) Naive Bayes Multinomial performs superior compared to the other proposed algorithms.},
  Doi                      = {10.1109/CSMR.2011.31}
}

@Article{1992:loplas:landi,
  Title                    = {Undecidability of static analysis},
  Author                   = {William Landi},
  Journal                  = loplas,
  Year                     = {1992},

  Month                    = dec,
  Number                   = {4},
  Pages                    = {323--337},
  Volume                   = {1},

  Abstract                 = {Static analysis of programs is indispensable to any software tool, environment, or system that requires compile-time information about the semantics of programs. With the emergence of languages like C and LISP, static analysis of programs with dynamic storage and recursive data structures has become a field of active research. Such analysis is difficult, and the static-analysis community has recognized the need for simplifying assumptions and approximate solutions. However, even under the common simplifying assumptions, such analyses are harder than previously recognized. Two fundamental static-analysis problems are may alias and must alias. The former is not recursive (is undecidable), and the latter is not recursively enumerable (is uncomputable), even when all paths are executable in the program being analyzed for languages with if statements, loops, dynamic storage, and recursive data structures.},
  Doi                      = {Undecidability of static analysis}
}

@InProceedings{1989:chi:lange,
  Title                    = {Some strategies of reuse in an object-oriented programming environment},
  Author                   = {Lange, Beth M. and Moher, Thomas G.},
  Booktitle                = chi,
  Year                     = {1989},
  Pages                    = {69--73},

  Abstract                 = {In a single-subject study of a software developer working in an object-oriented programming environment, we found evidence of a development style characterized by pervasive software reuse. The subject employed regular strategies for template selection and coding in her work, and avoided techniques requiring deep understanding of code details or symbolic execution whenever possible. Within the limits of the design of the study, the subject's performance is related to attributes of object-oriented programming and our interpretation of the mature mental model with which she approached her task.},
  Doi                      = {10.1145/67449.67465}
}

@Article{1997:tse:lanubile,
  Title                    = {Extracting reusable functions by flow graph-based program slicing},
  Author                   = {Filippo Lanubile and Giuseppe Visaggio},
  Journal                  = tse,
  Year                     = {1997},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {246--259},
  Volume                   = {23},

  Abstract                 = {An alternative approach to developing reusable components from scratch is to recover them from existing systems. We apply program slicing, a program decomposition method, to the problem of extracting reusable functions from ill structured programs. As with conventional slicing first described by M. Weiser (1984), a slice is obtained by iteratively solving data flow equations based on a program flow graph. We extend the definition of program slice to a transform slice, one that includes statements which contribute directly or indirectly to transform a set of input variables into a set of output variables. Unlike conventional program slicing, these statements do not include either the statements necessary to get input data or the statements which test the binding conditions of the function. Transform slicing presupposes the knowledge that a function is performed in the code and its partial specification, only in terms of input and output data. Using domain knowledge we discuss how to formulate expectations of the functions implemented in the code. In addition to the input/output parameters of the function, the slicing criterion depends on an initial statement, which is difficult to obtain for large programs. Using the notions of decomposition slice and concept validation we show how to produce a set of candidate functions, which are independent of line numbers but must be evaluated with respect to the expected behavior. Although human interaction is required, the limited size of candidate functions makes this task easier than looking for the last function instruction in the original source code.},
  Doi                      = {10.1109/32.588543}
}

@Article{2001:software:larman,
  Title                    = {Protected variation: The importance of being closed},
  Author                   = {Larman, C.},
  Journal                  = software,
  Year                     = {2001},

  Month                    = may,
  Number                   = {3},
  Pages                    = {89--91},
  Volume                   = {18},

  Abstract                 = {The Pattern Almanac 2000 (Addison Wesley, 2000) lists around 500 software-related patterns, and given this reading list, the curious developer has no time to program! Of course, there are underlying, simplifying themes and principles to this pattern plethora that developers have long considered and discussed. One example is L. Constantine's (1974) coupling and cohesion guidelines. Yet, these principles must continually resurface to help each new generation of developers and architects cut through the apparent disparity in myriad design ideas and help them see the underlying and unifying forces. One such principle, which B. Meyer (1988) describes is the Open-Closed Principle (OCP): modules should be both open (for extension and adaptation) and closed (to avoid modification that affect clients). OCP is essentially equivalent to the Protected Variation (PV) pattern: identify points of predicted variation and create a stable interface around them. OCP and PV formalize and generalize a common and fundamental design principle described in many guises. OCP and PV are two expressions of the same principle: protection against change to the existing code and design at variation and evolution points, with minor differences in emphasis.},
  Doi                      = {10.1109/52.922731}
}

@InProceedings{1992:icsm:laski,
  Title                    = {Identification of Program Modifications and Its Applications in Software Maintenance},
  Author                   = {Janusz Laski and Wojciech Szermer},
  Booktitle                = icsm,
  Year                     = {1992},
  Pages                    = {282--290},

  Abstract                 = {It is pointed out that a major problem in software maintenance is the revalidation of a modified code. It is economically desirable to restrict that process only to those parts of the program that are affected by the modifications. Towards that goal, a formal method is needed to identify the modifications in an automatic way. Such a method is proposed in the present work. The modifications are localized within clusters in the flow graphs of the original and modified programs. Both flow graphs are transformed into reduced flow graphs, between which an isomorphic correspondence is established. Cluster-nodes in the reduced graphs encapsulate modifications to the original program. An algorithm to derive the reduced flow graphs has been implemented as an extension to the recently developed system for testing and debugging (STAD 1.0) and early experiments with the algorithm are reported. Potential applications in regression testing and reasoning about the program are discussed.},
  Doi                      = {10.1109/ICSM.1992.242533}
}

@InProceedings{2007:esec_fse:latoza,
  Title                    = {Program comprehension as fact finding},
  Author                   = {{LaToza}, Thomas D. and Garlan, David and Herbsleb, James D. and Myers, Brad A.},
  Booktitle                = esec_fse,
  Year                     = {2007},
  Pages                    = {361--370},

  Abstract                 = {Little is known about how developers think about design during code modification tasks or how experienced developers' design knowledge helps them work more effectively. We performed a lab study in which thirteen developers worked for 3 hours under-standing the design of a 54 KLOC open source application. Par-ticipants had from 0 to 10.5 years of industry experience and were grouped into three ``experts" and ten ``novices." We observed that participants spent their time seeking, learning, critiquing, explain-ing, proposing, and implementing facts about the code such as ``getFoldLevel has effects". These facts served numerous roles, such as suggesting changes, constraining changes, and predicting the amount of additional investigation necessary to make a change. Differences between experts and novices included that the experts explained the root cause of the design problem and made changes to address it, while novice changes addressed only the symptoms. Experts did not read more methods but also did not visit some methods novices wasted time understanding. Experts talked about code in terms of abstractions such as ``caching" while novices more often described code statement by statement. Experts were able to implement a change faster than novices. Experts perceived problems novices did not and were able to explain facts novices could not. These findings have interesting implications for future tools.}
}

@InProceedings{2006:icse:latoza,
  Title                    = {Maintaining mental models: A study of developer work habits},
  Author                   = {Thomas D. LaToza and Gina Venolia and Robert DeLine},
  Booktitle                = icse,
  Year                     = {2006},
  Pages                    = {492--501},

  Abstract                 = {To understand developers' typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.}
}

@Article{2007:tse:lau,
  Title                    = {Software component models},
  Author                   = {Kung-Kiu Lau and Zheng Wang},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {709--724},
  Volume                   = {33},

  Abstract                 = {Component-based development (CBD) is an important emerging topic in software engineering, promising long-sought-after benefits like increased reuse, reduced time to market, and, hence, reduced software production cost. The cornerstone of a CBD technology is its underlying software component model, which defines components and their composition mechanisms. Current models use objects or architectural units as components. These are not ideal for component reuse or systematic composition. In this paper, we survey and analyze current component models and classify them into a taxonomy based on commonly accepted desiderata for CBD. For each category in the taxonomy, we describe its key characteristics and evaluate them with respect to these desiderata.},
  Doi                      = {10.1109/TSE.2007.70726}
}

@InProceedings{2008:chi:lawrance,
  Title                    = {Using information scent to model the dynamic foraging behavior of programmers in maintenance tasks},
  Author                   = {Lawrance, Joseph and Bellamy, Rachel and Burnett, Margaret and Rector, Kyle},
  Booktitle                = chi,
  Year                     = {2008},
  Pages                    = {1323--1332},

  Abstract                 = {In recent years, the software engineering community has begun to study program navigation and tools to support it. Some of these navigation tools are very useful, but they lack a theoretical basis that could reduce the need for ad hoc tool building approaches by explaining what is fundamentally necessary in such tools. In this paper, we present PFIS (Programmer Flow by Information Scent), a model and algorithm of programmer navigation during software maintenance. We also describe an experimental study of expert programmers debugging real bugs described in real bug reports for a real Java application. We found that PFIS' performance was close to aggregated human decisions as to where to navigate, and was significantly better than individual programmers' decisions.}
}

@InProceedings{2008:vlhcc:lawrance,
  Title                    = {Can information foraging pick the fix?: A field study},
  Author                   = {Lawrance, Joseph and Bellamy, Rachel and Burnett, Margaret and Rector, Kyle},
  Booktitle                = vlhcc,
  Year                     = {2008},
  Pages                    = {57--64},

  Abstract                 = {Previous findings have revealed the ability of information foraging to model or predict where developers will navigate within source code. However, the previous investigation did not consider whether the places developers went were the right places to go. In this paper, we present afield study in which we investigated over 200 open source bug reports and feature requests. We analyzed the textual similarity of these issues in relation to the source code, and determined what files developers had changed to fix these issues. Our results demonstrate that information scent can narrow down quite well where developers should make fixes, implying that future software navigation tools can predict the appropriate places to make fixes based solely on the contents of the issue and the source code.}
}

@Article{1985:sen:lawrence,
  Title                    = {Why is software always late?},
  Author                   = {J. L. Lawrence},
  Journal                  = sen,
  Year                     = {1985},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {19--30},
  Volume                   = {10},

  Abstract                 = {Despite all of the advances in software engineering practice, despite all the newly developed languages and software tools, and despite case study after case study, software is almost always late. It does not seem to matter what the product is or what the industry is. The cry of frustration is almost always the same: ``Why is software always late?" In this article, the author discusses the software development cycle and the LOC/day productivity measure in an attempt to explore some of the reasons for failure and some of the factors contributing to success. The opinions expressed herein are soley those of the author and do not necessarily reflect views or procedures of the General Electric Company or its employees.},
  Doi                      = {10.1145/1012443.1012445}
}

@InProceedings{2010:seke:legoaer,
  Title                    = {Evolution Styles to Capitalize Evolution Expertise within Software Architectures},
  Author                   = {Le Goaer, Olivier and Dalila Tamzalit and Mourad Oussalah},
  Booktitle                = seke,
  Year                     = {2010},
  Pages                    = {159--164},

  Abstract                 = {Evolution is an increasingly important aspect in the software architecture community. At this level of abstraction, evolution is mainly concerned with changes brought to specifications of components and connectors and to their configuration. Such an evolution activity may be very hard or easy; the degree of difficulty largely relying on the competence and skill of the architect who performs it. In order to decrease time and cost associated to evolution, this paper suggests a new evolution model relying on styles to capitalise recurring evolutions in order to re-apply them whenever a similar problem arises again within architectures. Ultimately, this paper argues in favor of off-the-shelf evolutions to tackle the complex problem of architectural evolution.},
  Url                      = {http://www.ksi.edu/seke/Proceedings/seke/SEKE2010_Proceedings.pdf}
}

@InProceedings{2008:compsac:legoaer,
  Title                    = {Evolution Shelf: Reusing Evolution Expertise within Component-Based Software Architectures},
  Author                   = {Le Goaer, Olivier and Tamzalit, Dalila and Oussalah, Mourad and Seriai, Abdelhak-Djamel},
  Booktitle                = compsac,
  Year                     = {2008},
  Pages                    = {311--318},

  Abstract                 = {Despite that reuse libraries are now well adopted during software development step, software evolution step is not yet covered by this kind of beneficial approach. In this paper we present the ``evolution shelf", a generic infrastructure to achieve for-reuse and by-reuse techniques within the field of software evolution. The basic idea behind that is to propose and encourage the reuse of recurring and reliable evolution expertises to achieve the structural evolution of a software system at the architectural level. For that purpose, the shelf assists architects in classifying, storing and selecting reusable architectural evolution operations. The underlying concept that we propose to capitalize the expertises is called ``evolution style" and it mixes a syntactic and a semantic description format. These ideas form a core for a long-term vision in which it is possible to build a business model of evolution-of-the-shelf (EOTS) with the special objective to decrease the efforts and the risks related to the evolution activities.},
  Doi                      = {10.1109/COMPSAC.2008.104}
}

@InProceedings{2008:shark:legoaer,
  Title                    = {Evolution styles to the rescue of architectural evolution knowledge},
  Author                   = {Le Goaer, Olivier and Tamzalit, Dalila and Oussalah, Mourad Chabane and Seriai, Abdelhak-Djamel},
  Booktitle                = shark,
  Year                     = {2008},
  Pages                    = {31--36},

  Abstract                 = {The core idea is to consider software-architecture evolution tasks as a knowledge that must be clearly modeled and properly managed. The main expected benefit is the reuse of existing and already available evolution expertise rather than reinventing it, sometimes awkwardly and thus avoid time-consuming redundant evolution activities. For this purpose, we propose to use the evolution style concept as a neutral interchange format to capitalize and transfer knowledge about domain-specific evolution tasks. In this paper we put the focus on how it is possible to reason on evolution-styles description libraries through a classification scheme. Specifically, we present the ``evolution shelf", an infrastructure to perform (a) incremental acquisition of new evolution descriptions and (b) retrieval of evolution descriptions matching with a given context. Our shelf, dedicated to software architects, relies on well-known repository techniques while updating them to support and exploit the evolution-style concept.},
  Doi                      = {10.1145/1370062.1370071}
}

@Article{2006:tvcg:lee,
  Title                    = {{TreePlus}: Interactive exploration of networks with enhanced tree layouts},
  Author                   = {Bongshin Lee and Cynthia S. Parr and Catherine Plaisant and Benjamin B. Bederson and Vladislav D. Veksler and Wayne D. Gray and Christopher Kotfila},
  Journal                  = tvcg,
  Year                     = {2006},

  Month                    = nov # {/} # dec,
  Number                   = {6},
  Pages                    = {1414--1426},
  Volume                   = {12},

  Abstract                 = {Despite extensive research, it is still difficult to produce effective interactive layouts for large graphs. Dense layout and occlusion make food Webs, ontologies and social networks difficult to understand and interact with. We propose a new interactive visual analytics component called TreePlus that is based on a tree-style layout. TreePlus reveals the missing graph structure with visualization and interaction while maintaining good readability. To support exploration of the local structure of the graph and gathering of information from the extensive reading of labels, we use a guiding metaphor of ``plant a seed and watch it grow." It allows users to start with a node and expand the graph as needed, which complements the classic overview techniques that can be effective at (but often limited to) revealing clusters. We describe our design goals, describe the interface and report on a controlled user study with 28 participants comparing TreePlus with a traditional graph interface for six tasks. In general, the advantage of TreePlus over the traditional interface increased as the density of the displayed data increased. Participants also reported higher levels of confidence in their answers with TreePlus and most of them preferred TreePlus.},
  Doi                      = {10.1109/TVCG.2006.106}
}

@InProceedings{2008:ssspr:lee,
  Title                    = {An Inexact Graph Comparison Approach in Joint Eigenspace},
  Author                   = {Lee, Wan-Jui and Duin, Robert P.},
  Booktitle                = ssspr,
  Year                     = {2008},
  Pages                    = {35--44},

  Abstract                 = {In graph comparison, the use of (dis)similarity measurements between graphs is an important topic. In this work, we propose an eigendecomposition based approach for measuring dissimilarities between graphs in the \emph{joint} eigenspace (JoEig). We will compare our JoEig approach with two other eigendecomposition based methods that compare graphs in \emph{different} eigenspaces. To calculate the dissimilarity between graphs of different sizes and perform inexact graph comparison, we further develop three different ways to resize the eigenspectra and study their performance in different situations.},
  Doi                      = {10.1007/978-3-540-89689-0_8}
}

@InProceedings{1995:icsq:lee,
  Title                    = {Measuring the Coupling and Cohesion of an Object-Oriented Program Based on Information Flow},
  Author                   = {Y.-S. Lee and B.-S. Liang and S.-F. Wu and F.-J. Wang},
  Booktitle                = icsq,
  Year                     = {1995},
  Pages                    = {81--90},

  Abstract                 = {TBD}
}

@InProceedings{1996:ewspt:lehman,
  Title                    = {Laws of Software Evolution Revisited},
  Author                   = {Lehman, M. M.},
  Booktitle                = ewspt,
  Year                     = {1996},
  Series                   = lncs,
  Volume                   = {1149},

  Abstract                 = {Data obtained during a 1968 study of the software process [leh69] led to an investigation of the evolution of OS/360 [leh85] and and, over a period of twenty years, to formulation of eight Laws of Software Evolution.. The FEAST project recently initiated (see sections 4 - 6 below) is expected to throw additional light on the phenomenology underlying these laws, to increase understanding of them, to explore their finer detail, to expose their wider relevance and implications and to develop means for their beneficial exploitation. This paper is intended to trigger wider interest in the laws and in the FEAST study of feedback and feedback control in the context of the software process and its improvement to ensure beneficial exploitation of their potential.},
  Page                     = {108--124}
}

@Article{1980:ieee:lehman,
  Title                    = {Programs, life cycles, and laws of software evolution},
  Author                   = {Lehman, M. M.},
  Journal                  = ieee,
  Year                     = {1980},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {1060--1076},
  Volume                   = {68},

  Abstract                 = {By classifying programs according to their relationship to the environment in which they are executed, the paper identifies the sources of evolutionary pressure on computer applications and programs and shows why this results in a process of never ending maintenance activity. The resultant life cycle processes are then briefly discussed. The paper then introduces laws of Program Evolution that have been formulated following quantitative studies of the evolution of a number of different systems. Finally an example is provided of the application of Evolution Dynamics models to program release planning.},
  Doi                      = {10.1109/PROC.1980.11805}
}

@Book{1985:book:lehman,
  Title                    = {Program Evolution: Processes of Software Change},
  Author                   = {Lehman, M. M. and Belady, L. A.},
  Publisher                = {Academic Press},
  Year                     = {1985}
}

@InProceedings{1997:metrics:lehman,
  Title                    = {Metrics and Laws of Software Evolution: The Nineties View},
  Author                   = {M. M. Lehman and J. F. Ramil and P. D. Wernick and D. E. Perry and W. M. Turski},
  Booktitle                = metrics,
  Year                     = {1997},
  Pages                    = {20--32},

  Abstract                 = {The process of E-type software development and evolution has proven most difficult to improve, possibly due to the fact that the process is a multi-input, multi-output system involving feedback at many levels. This observation, first recorded in the early 70s during an extended study of OS/360 evolution, was recently captured in a FEAST hypothesis; a hypothesis being studied in on-going two-year project, FEAST/l. Preliminary conclusions based on a study of a financial transaction system, FW, are outlined and compared with those reached during the earlier OS/360 study. The new analysis supports, or better does not contradict, the laws of software evolution, suggesting that the 1970s approach to metric analysis of software evolution is still relevant today. It is hoped that FEAST/l will provide a foundation for mastering the feedback aspects of the software evolution process, opening up new paths for process modelling and improvement.}
}

@InProceedings{2012:ecbs:lehnert,
  Title                    = {A taxonomy of change types and its application in software evolution},
  Author                   = {Lehnert, Steffen and Farooq, Q. and Riebisch, Matthias},
  Booktitle                = ecbs,
  Year                     = {2012},
  Pages                    = {98--107},

  Doi                      = {10.1109/ECBS.2012.9}
}

@Article{2005:ipl:leino,
  Title                    = {Efficient weakest preconditions},
  Author                   = {Leino, K. Rustan M.},
  Journal                  = ipl,
  Year                     = {2005},

  Month                    = {31 } # mar,
  Number                   = {6},
  Pages                    = {281--288},
  Volume                   = {93},

  Abstract                 = {Desired computer-program properties can be described by logical formulas called verification conditions. Different mathematically-equivalent forms of these verification conditions can have a great impact on the performance of an automatic theorem prover that tries to discharge them. This paper presents a simple weakest-precondition understanding of the ESC/Java technique for generating verification conditions. This new understanding of the technique spotlights the program property that makes the technique work.},
  Doi                      = {10.1016/j.ipl.2004.10.015}
}

@InProceedings{2007:esec_fse:leitner,
  Title                    = {Contract driven development = test driven development - writing test cases},
  Author                   = {Leitner, Andreas and Ciupa, Ilinca and Oriol, Manuel and Meyer, Bertrand and Fiva, Arno},
  Booktitle                = esec_fse,
  Year                     = {2007},
  Pages                    = {425--434},

  Abstract                 = {Although unit tests are recognized as an important tool in software development, programmers prefer to write code, rather than unit tests. Despite the emergence of tools like JUnit which automate part of the process, unit testing remains a time-consuming, resource-intensive, and not particularly appealing activity.This paper introduces a new development method, called Contract Driven Development. This development method is based on a novel mechanism that extracts test cases from failure-producing runs that the programmers trigger. It exploits actions that developers perform anyway as part of their normal process of writing code. Thus, it takes the task of writing unit tests off the developers' shoulders, while still taking advantage of their knowledge of the intended semantics and structure of the code. The approach is based on the presence of contracts in code, which act as the oracle of the test cases. The test cases are extracted completely automatically, are run in the background, and can easily be maintained over versions. The tool implementing this methodology is called Cdd and is available both in binary and in source form.}
}

@Article{2011:ist:lemos,
  Title                    = {A test-driven approach to code search and its application to the reuse of auxiliary functionality},
  Author                   = {Lemos, Ot\'{a}vio Augusto Lazzarini and Bajracharya, Sushil and Ossher, Joel and Masiero, Paulo Cesar and Lopes, Cristina},
  Journal                  = ist,
  Year                     = {2011},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {294--306},
  Volume                   = {53},

  Abstract                 = {Context: Software developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g., compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and integrated into the workspace. Objective: In this paper we introduce and evaluate the use of test cases as an interface for automating code search and reuse. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We also describe CodeGenie, an Eclipse plugin we have developed that performs TDCS using a code search engine called Sourcerer. Method: Our evaluation consists of two studies: an applicability study with 34 different features that were searched using CodeGenie; and a performance study comparing CodeGenie, Google Code Search, and a manual approach. Results: Both studies present evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality. Conclusion: This paper presents an approach to source code search and its application to the reuse of auxiliary functionality. Our exploratory evaluation shows promising results, which motivates the use and further investigation of TDCS.},
  Doi                      = {10.1016/j.infsof.2010.11.009}
}

@InProceedings{2009:sac:lemos,
  Title                    = {Applying test-driven code search to the reuse of auxiliary functionality},
  Author                   = {Lemos, Ot\'{a}vio Augusto Lazzarini and Bajracharya, Sushil and Ossher, Joel and Masiero, Paulo Cesar and Lopes, Cristina},
  Booktitle                = sac,
  Year                     = {2009},
  Pages                    = {476--482},

  Abstract                 = {Software developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g. compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and extracted into the workspace. In this paper we introduce the use of test cases as an interface for automating code search and reuse and evaluate its applicability and performance in the reuse of auxiliary functionality. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We present CodeGenie, an Eclipse plugin that performs TDCS using a code search engine called Sourcerer. Our evaluation presents evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality.},
  Doi                      = {10.1145/1529282.1529384}
}

@InProceedings{2007:oopsla:lemos,
  Title                    = {{CodeGenie}: A tool for test-driven source code search},
  Author                   = {Lemos, Ot\'{a}vio Augusto Lazzarini and Sushil Krishna Bajracharya and Joel Ossher},
  Booktitle                = oopsla,
  Year                     = {2007},
  Pages                    = {917--918},

  Abstract                 = {We present CodeGenie, a tool that implements a test-driven approach to search and reuse of code available on largescale code repositories. With CodeGenie, developers designtest cases for a desired feature first, similar to Test-driven Development (TDD). However, instead of implementing the feature from scratch, CodeGenie automatically searches foran existing implementation based on information available in the tests. To check the suitability of the candidate results in the local context, each result is automatically woven into the developer's project and tested using the original tests. The developer can then reuse the most suitable result. Later, reused code can also be unwoven from the project as wished. For the code searching and wrapping facilities, CodeGenie relies on Sourcerer, an Internet-scale source code infrastructure that we have developed.},
  Doi                      = {10.1145/1297846.1297944}
}

@InProceedings{2007:ase:lemos,
  Title                    = {{CodeGenie}: Using test-cases to search and reuse source code},
  Author                   = {Lemos, Ot\'{a}vio Augusto Lazzarini and Bajracharya, Sushil Krishna and Ossher, Joel and Morla, Ricardo Santos and Masiero, Paulo Cesar and Baldi, Pierre and Lopes, Cristina Videira},
  Booktitle                = ase,
  Year                     = {2007},
  Pages                    = {525--526},

  Abstract                 = {We present CodeGenie, a tool that implements a test-driven approachto search and reuse of code available on large-scale coderepositories. While using CodeGenie developers design test cases fora desired feature first, similar to Test-driven Development (TDD).However, instead of implementing the feature as in TDD, CodeGenieautomatically searches for it based on information available in thetests. To check the suitability of the candidate results in thelocal context, each result is automatically woven into thedeveloper's project and tested using the original tests. Thedeveloper can then reuse the most suitable result. Later, reusedcode can also be unwoven from the project as wished. For the codesearching and wrapping facilities, CodeGenie relies on Sourcerer, anInternet-scale source code infrastructure that we have developed.},
  Doi                      = {10.1145/1321631.1321726}
}

@InProceedings{2013:wcre:leotta,
  Title                    = {Capture-replay vs. programmable web testing: An empirical assessment during test case evolution},
  Author                   = {Leotta, Maurizio and Clerissi, Diego and Ricca, Filippo and Tonella, Paolo},
  Booktitle                = wcre,
  Year                     = {2013},
  Pages                    = {272--281},

  Doi                      = {10.1109/WCRE.2013.6671302}
}

@Book{2005:book:lethbridge,
  Title                    = {Object-Oriented Software Engineering: Practical Software Development using UML and Java},
  Author                   = {Timothy C. Lethbridge and Robert Lagani{\`e}re},
  Publisher                = {McGraw-Hill},
  Year                     = {2005},
  Edition                  = {2nd}
}

@Article{2005:ese:lethbridge,
  Title                    = {Studying Software Engineers: Data Collection Techniques for Software Field Studies},
  Author                   = {Lethbridge, Timothy C. and Sim, Susan Elliott and Singer, Janice},
  Journal                  = ese,
  Year                     = {2005},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {311--341},
  Volume                   = {10},

  Acmid                    = {1068447},
  Doi                      = {10.1007/s10664-005-1290-x}
}

@Article{1966:spd:levenshtein,
  Title                    = {Binary ccode capable of correcting deletions, insertions, and reversals},
  Author                   = {Levenshtein, Vladimir I.},
  Journal                  = {Soviet Physics Doklady},
  Year                     = {1966},

  Month                    = feb,
  Number                   = {8},
  Pages                    = {707--710},
  Volume                   = {10}
}

@InProceedings{2010:simutools:leye,
  Title                    = {A flexible and extensible architecture for experimental model validation},
  Author                   = {Leye, Stefan and Uhrmacher, Adelinde M.},
  Booktitle                = simutools,
  Year                     = {2010},
  Pages                    = {65:1--65:10},

  Abstract                 = {With the rising number and diversity of validation methods, the need for a tool supporting an easy exploitation of those methods emerges. We introduce FAMVal, a validation architecture that supports the seamless integration of different validation techniques. We structure a validation experiment into the tasks specification of requirements, configuration of the model, model execution, observation, analysis, and evaluation. This structuring improves the flexibility of the approach, by facilitating the combination of methods for different tasks. In addition to the overall architecture, basic components and their interactions are presented. The usage of FAMVal is illuminated by several validation experiments with a small chemical model. The architecture has been realized using the plug-in based design of the modeling and simulation framework JAMES II.},
  Doi                      = {10.4108/ICST.SIMUTOOLS2010.8833}
}

@InProceedings{2008:iccsse:li,
  Title                    = {Complex Network Thinking in Software Engineering},
  Author                   = {Deyi Li and Yanni Han and Jun Hu},
  Booktitle                = iccsse,
  Year                     = {2008},
  Pages                    = {264--268},

  Abstract                 = {The booming of Internet has brought new challenges to software engineering. In the network age, software industry has undergone a transition from manufacturing industry to the service trade, software doesnpsilat have clear hierarchy structure, specific life cycle and definite system border any longer. On the contrary, both software itself and the environment it works in are all networked. In this paper, we explore an idea of software engineering from the perspective of complex networks. First, we present a skeletal description to abstract software structure into network topology. Evidence shows that software systems have the characteristics of small world, scale-free and high clustering. Then we point out three aspects of the influence to software engineering based on the characteristics of complex network. Finally, the most promising development trend of software engineering in the future is predicted.}
}

@InProceedings{1996:icsm:li,
  Title                    = {Algorithmic analysis of the impact of changes to object-oriented software},
  Author                   = {Li, Li and Offutt, A. Jefferson},
  Booktitle                = icsm,
  Year                     = {1996},
  Pages                    = {171--184},

  Abstract                 = {As the software industry has matured, we have shifted our resources from being primarily devoted to developing new software systems to primarily making modifications in evolving software systems. A major problem for developers in an evolutionary environment is that seemingly small changes can ripple throughout the system to have major unintended impacts elsewhere. As a result, software developers need mechanisms to understand how a change to a software system will affect the rest of the system. Although the effects of changes in object-oriented are restricted, they are also more subtle and more difficult to detect. This paper presents algorithms to analyze the potential impacts of changes to object-oriented software, taking into account encapsulation, inheritance, and polymorphism. This technique allows software developers to perform ``what if" analysis on the effect of proposed changes, and thereby choose the change that has the least influence on the rest of the system. The analysis also adds valuable information to regression testing, by suggesting what classes and methods need to be retested, and to project managers, who can use the results for cost estimation and schedule planning.}
}

@Article{1993:jss:li,
  Title                    = {Object-Oriented Metrics that Predict Maintainability},
  Author                   = {Wei Li and Sallie Henry},
  Journal                  = jss,
  Year                     = {1993},

  Month                    = nov,
  Number                   = {2},
  Pages                    = {111--122},
  Volume                   = {23},

  Abstract                 = {Software metrics have been studied in the procedural paradigm as a quantitative means of assessing the software development process as well as the quality of software products. Several studies have validated that various metrics are useful indicators of maintenance effort in the procedural paradigm. However, software metrics have rarely been studied in the object-oriented paradigm. Very few metrics have been proposed to measure object-oriented systems, and the proposed ones have not been validated. This research concentrates on several object-oriented software metrics and the validation of these metrics with maintenance effort in two commercial systems. Statistical analyses of a prediction model incorporating 10 metrics were performed. In addition, a more compact model with fewer metrics is presented.},
  Doi                      = {10.1016/0164-1212(93)90077-B,}
}

@InCollection{2009:book:babar:liang,
  Title                    = {Tools and Technologies for Architecture Knowledge Management},
  Author                   = {Liang, Peng and Avgeriou, Paris},
  Booktitle                = {Software Architecture Knowledge Management},
  Publisher                = {Springer},
  Year                     = {2009},
  Editor                   = {Ali Babar, Muhammad and Dings{\o}yr, Torgeir and Lago, Patricia and van Vliet, Hans},
  Pages                    = {91--111},

  Abstract                 = {As management of architectural knowledge becomes vital for improving an organization's architectural capabilities, support for (semi-) automating this management is required. There exist already several tools that specialize in architecture knowledge management, as well as generic technologies that can potentially be used for this purpose. Both tools and technologies cover a wide number of potential use cases for architecture knowledge management. In this chapter, we survey the existing tool support and related technologies for different architecture knowledge management strategies, and present them according to the use cases they offer.},
  Doi                      = {10.1007/978-3-642-02374-3_6}
}

@Book{1999:book:liang,
  Title                    = {Java Native Interface: Programmer's Guide and Reference},
  Author                   = {Liang, Sheng},
  Publisher                = {Addison-Wesley},
  Year                     = {1999},

  ISBN                     = {0201325772}
}

@InProceedings{2008:promise:liebchen,
  Title                    = {Data sets and data quality in software engineering},
  Author                   = {Liebchen, Gernot A. and Shepperd, Martin},
  Booktitle                = promise,
  Year                     = {2008},
  Pages                    = {39--44},

  Abstract                 = {OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.},
  Doi                      = {10.1145/1370788.1370799}
}

@Article{1994:cacm:lieberherr,
  Title                    = {Adaptive object-oriented programming using graph-based customization},
  Author                   = {Karl J. Lieberherr and Ignacio Silva-Lepe and Cun Xiao},
  Journal                  = cacm,
  Year                     = {1994},

  Month                    = may,
  Number                   = {5},
  Pages                    = {94--101},
  Volume                   = {37},

  Abstract                 = {Object-oriented programs are easier to extend than programs that are not written in an object-oriented style, but object-oriented programs are still very rigid and difficult to adapt and maintain. A key feature of most popular approaches to object-oriented programming is that methods are attached to classes---C++, Smalltalk, Eiffel, Beta---or to groups of classes---CLOS. This feature is both a blessing and curse. On the brighter side, attaching methods to classes is at the core of 1) objects being able to receive messages, 2) different classes of objects responding differently to a given message, and 3) the ability to define standard protocols. On the darker side, by explicitly attaching every single method to a specific class, the details of the class structure are encoded into the program unnecessarily. This leads to programs that are difficult to evolve and maintain. In other words, today's object-oriented programs often contain more redundant application-specific information than is necessary, thus limiting their reusability.},
  Doi                      = {10.1145/175290.175303}
}

@Book{1980:book:lientz,
  Title                    = {Software Maintenance Management: A Study of the Maintenance of Computer Application Software in 487~Data Processing Organizations},
  Author                   = {Lientz, Bennet P. and Swanson, E. Burton},
  Publisher                = {Addison-Wesley},
  Year                     = {1980}
}

@Article{2001:bioscience:limpert,
  Title                    = {Log-Normal Distributions across the Sciences: Keys and Clues},
  Author                   = {Eckhard Limpert and Werner A. Stahel and Markus Abbt},
  Journal                  = bioscience,
  Year                     = {2001},

  Month                    = may,
  Number                   = {5},
  Pages                    = {341--352},
  Volume                   = {51},

  Abstract                 = {As the need grows for conceptualization, formalization, and abstraction in biology, so too does mathematics' relevance to the field (Fagerstr{\"o}m et al. 1996). Mathematics is particularly important for analyzing and characterizing random variation of, for example, size and weight of individuals in populations, their sensitivity to chemicals, and time-to-event cases, such as the amount of time an individual needs to recover from illness. The frequency distribution of such data is a major factor determining the type of statistical analysis that can be validly carried out on any data set. Many widely used statistical methods, such as ANOVA (analysis of variance) and regression analysis, require that the data be normally distributed, but only rarely is the frequency distribution of data tested when these techniques are used.},
  Doi                      = {10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2}
}

@InProceedings{2010:wetsom:lindsay,
  Title                    = {Does size matter?: A preliminary investigation of the consequences of powerlaws in software},
  Author                   = {Lindsay, Joshua and Noble, James and Tempero, Ewan},
  Booktitle                = wetsom,
  Year                     = {2010},
  Pages                    = {16--23},

  Abstract                 = {There is increasing evidence that many object-oriented software size metrics are characterised by scale-free, powerlaw distributions. This means programs will have arbitrarily large components, and the size of the largest component will increase as programs' overall size increases. This directly contradicts a crucial assumption of object-oriented design---that large programs can be build by combining many small components. In this paper, we present a preliminary study of this contradiction. We illustrate the distribution of several size metrics over a corpus of 100 Java systems, and then investigate the largest classes (according to five size and complexity metrics) from one of those systems. We find that, while some large classes may be explained by code-generation or design patterns, most large classes were examples of poor object-oriented design.}
}

@Article{1998:jss:lindvall,
  Title                    = {How well do experienced software developers predict software change?},
  Author                   = {Lindvall, Mikael and Sandahl, Kristian},
  Journal                  = jss,
  Year                     = {1998},

  Month                    = oct,
  Number                   = {1},
  Pages                    = {19--27},
  Volume                   = {43},

  Abstract                 = {Requirements-driven impact analysis (rdia) identifies the set of software entities needed to be changed to implement a new requirement in an existing system. The input is a set of requirements and the existing system. The output is, for each requirement, a set of software entities that have to be changed. The output is used as input to many project-planning activities, for example cost estimation based on change volume. This paper quantifies how well experienced software developers predict change by conducting rdia, where rdia in this case is the general activity of predicting changes based on change request. The means has been an empirical study of rdia in the industrial object-oriented pmr-project. rdia has been carried out in two releases, R4 and R6, of this project as a normal part of project developers' work. This in-depth case-study has been carried out over four years and in close contact with project developers. The correctness of the prediction is high while problems with underprediction have been identified---many more classes than predicted are changed. We have also found that project developers are unaware of their own positive and negative capabilities in predicting change.},
  Doi                      = {10.1016/S0164-1212(98)10019-5}
}

@Article{2009:dmkd:linstead,
  Title                    = {Sourcerer: Mining and searching internet-scale software repositories},
  Author                   = {Linstead, Erik and Bajracharya, Sushil and Ngo, Trung and Rigor, Paul and Lopes, Cristina and Baldi, Pierre},
  Journal                  = dmkd,
  Year                     = {2009},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {300--336},
  Volume                   = {18},

  Abstract                 = {Large repositories of source code available over the Internet, or within large organizations, create new challenges and opportunities for data mining and statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, fingerprinting, and database storage of open source software on an Internet-scale. In one experiment, we gather 4,632 Java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, method call, and lexical containment distributions. We then develop and apply unsupervised, probabilistic, topic and author-topic (AT) models to automatically discover the topics embedded in the code and extract topic-word, document-topic, and AT distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing source file similarity, developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering an software development staffing. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the area under the curve (AUC) retrieval metric to 0.92---roughly 10--30\% better than previous approaches based on text alone. A prototype of the system is available at: http://sourcerer.ics.uci.edu.},
  Doi                      = {10.1007/s10618-008-0118-x}
}

@TechReport{1996:tr:lions,
  Title                    = {Ariane 5: Flight 501 Failure},
  Author                   = {Lions, Jacques-Louis and L{\"u}beck, Lennart and Fauquembergue, Jean-Luc and Kahn, Gilles and Kubbat, Wolfgang and Levedag, Stefan and Mazzini, Leonardo and Merle, Didier and O'Halloran, Colin},
  Institution              = {Ariane 501 Inquiry Board},
  Year                     = {1996},

  Abstract                 = {On 4 June 1996, the maiden flight of the Ariane 5 launcher ended in a failure. Only about 40 seconds after initiation of the flight sequence, at an altitude of about 3700 m, the launcher veered off its flight path, broke up and exploded. Engineers from the Ariane 5 project teams of CNES and Industry immediately started to investigate the failure. Over the following days, the Director General of ESA and the Chairman of CNES set up an independent Inquiry Board and nominated the following members: - Prof. Jacques-Louis Lions (Chairman), Acad\'{e}mie des Sciences (France) - Dr. Lennart L\"{u}beck (Vice-Chairman), Swedish Space Corporation (Sweden) - Mr. Jean-Luc Fauquembergue, D\'{e}l\'{e}gation G\'{e}n\'{e}rale pour l'Armement (France) - Mr. Gilles Kahn, Institut National de Recherche en Informatique et en Automatique (INRIA), (France) - Prof. Dr. Ing. Wolfgang Kubbat, Technical University of Darmstadt (Germany) - Dr. Ing. Stefan Levedag, Daimler Benz Aerospace (Germany) - Dr. Ing. Leonardo Mazzini, Alenia Spazio (Italy) - Mr. Didier Merle, Thomson CSF (France) - Dr. Colin O'Halloran, Defence Evaluation and Research Agency (DERA), (U.K.) The terms of reference assigned to the Board requested it - to determine the causes of the launch failure, - to investigate whether the qualification tests and acceptance tests were appropriate in relation to the problem encountered, - to recommend corrective action to remove the causes of the anomaly and other possible weaknesses of the systems found to be at fault. The Board started its work on 13 June 1996. It was assisted by a Technical Advisory Committee composed of: - Dr Mauro Balduccini (BPD) - Mr Yvan Choquer (Matra Marconi Space) - Mr Remy Hergott (CNES) - Mr Bernard Humbert (Aerospatiale) - Mr Eric Lefort (ESA) In accordance with its terms of reference, the Board concentrated its investigations on the causes of the failure, the systems supposed to be responsible, any failures of similar nature in similar systems, and events that could be linked to the accident. Consequently, the recommendations made by the Board are limited to the areas examined. The report contains the analysis of the failure, the Board's conclusions and its recommendations for corrective measures, most of which should be undertaken before the next flight of Ariane 5. There is in addition a report for restricted circulation in which the Board's findings are documented in greater technical detail. Although it consulted the telemetry data recorded during the flight, the Board has not undertaken an evaluation of those data. Nor has it made a complete review of the whole launcher and all its systems. This report is the result of a collective effort by the Commission, assisted by the members of the Technical Advisory Committee. We have all worked hard to present a very precise explanation of the reasons for the failure and to make a contribution towards the improvement of Ariane 5 software. This improvement is necessary to ensure the success of the programme. The Board's findings are based on thorough and open presentations from the Ariane 5 project teams, and on documentation which has demonstrated the high quality of the Ariane 5 programme as regards engineering work in general and completeness and traceability of documents.}
}

@InProceedings{1992:sde:lippe,
  Title                    = {Operation-based merging},
  Author                   = {Lippe, Ernst and van Oosterom, Norbert},
  Booktitle                = sde,
  Year                     = {1992},
  Pages                    = {78--87},

  Abstract                 = {Existing approaches for merging the results of parallel development activities are limited. These approaches can be characterised as state-based: only the initial and final states are considered. This paper introduces operation-based merging, which uses the operations that were performed during development. In many cases operation-based merging has advantages over state-based merging, because it automatically respects the data-type invariants of the objects, is extensible for arbitrary object types, provides better conflict detection and allows for better support for solving these conflicts. Several algorithms for conflict detection are described and compared.},
  Doi                      = {10.1145/142868.143753}
}

@InProceedings{2004:xp:lippert,
  Title                    = {Towards a Proper Integration of Large Refactorings in Agile Software Development},
  Author                   = {Martin Lippert},
  Booktitle                = xp,
  Year                     = {2004},
  Pages                    = {113--122},

  Abstract                 = {Refactoring is a key element of many agile software development methods. While most developers associate small design changes with the term refactoring (as described by Martin Fowler and William F. Opdyke), everyday development practice in medium- to large-sized projects calls for more than fine-grained refactorings. Such projects involve more complex refactorings, running for several hours or days and sometimes consisting of a huge number of steps. This paper discusses the problems posed by large refactorings and presents an approach that allows agile teams to integrate large refactorings into their daily work.}
}

@InProceedings{2000:icse:lippert,
  Title                    = {A Study on Exception Detection and Handling Using Aspect-Oriented Programming},
  Author                   = {Martin Lippert and Cristina Videira Lopes},
  Booktitle                = icse,
  Year                     = {2000},
  Pages                    = {418--427},

  Abstract                 = {Aspect-Oriented Programming (AOP) is intended to ease situations that involve many kinds of code tangling. This paper reports on a study to investigate AOP's ability to ease tangling related to exception detection and handling. We took an existing framework written in Java, the JWAM framework, and partially reengineered its exception detection and handling aspects using AspectJ, an aspect-oriented programming extension to Java. We found that AspectJ supported implementations that drastically reduced the portion of the code related to exception detection and handling. In one scenario, we were able to reduce that code by a factor of 4. We also found that, with respect to the original implementation in plain Java, AspectJ provided better support for different configurations of exceptional behaviors, more tolerance for changes in the specifications of exceptional behaviors, better support for incremental development, better reuse, automatic enforcement of contracts in applications that use the framework, and cleaner program texts. We also found some weaknesses of AspectJ that should be addressed in the future.},
  Doi                      = {10.1145/337180.337229}
}

@Book{2006:book:lippert,
  Title                    = {Refactorings in Large Software Projects: How to Successfully Execute Complex Restructurings},
  Author                   = {Martin Lippert and Stefan Roock},
  Publisher                = {Wiley},
  Year                     = {2006}
}

@Article{2009:ijase:little,
  Title                    = {Keyword programming in {J}ava},
  Author                   = {Little, Greg and Miller, Robert},
  Journal                  = ijase,
  Year                     = {2009},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {37--71},
  Volume                   = {16},

  Abstract                 = {Keyword programming is a novel technique for reducing the need to remember details of programming language syntax and APIs, by translating a small number of unordered keywords provided by the user into a valid expression. In a sense, the keywords act as a query that searches the space of expressions that are valid in the given context. Prior work has demonstrated the feasibility and merit of this approach in limited domains. This paper explores the potential for employing this technique in much larger domains, specifically general-purpose programming languages like Java. We present an algorithm for translating keywords into Java method call expressions. When tested on keywords extracted from existing method calls in Java code, the algorithm can accurately reconstruct over 90\% of the original expressions. We tested the algorithm on keywords provided by users in a web-based study. The results suggest that users can obtain correct Java code using keyword queries as accurately as they can write the correct Java code themselves. We implemented the algorithm in an Eclipse plug-in as an extension to the autocomplete mechanism and deployed it in a preliminary field study of several users, with mixed results. One interesting result of this work is that most of the information in Java method call expressions lies in the keywords, and details of punctuation and even parameter ordering can often be inferred automatically.},
  Doi                      = {10.1007/s10515-008-0041-9}
}

@Article{1987:jss:littman,
  Title                    = {Mental models and software maintenance},
  Author                   = {David Littman and Jeannine Pinto and Stan Letovsky and Elliot Soloway},
  Journal                  = jss,
  Year                     = {1987},

  Month                    = dec,
  Number                   = {4},
  Pages                    = {341--355},
  Volume                   = {7},

  Abstract                 = {Understanding how a program is constructed and how it functions are significant components of the task of maintaining or enhancing a computer program. We have analyzed vidoetaped protocols of experienced programmers as they enhanced a personnel data base program. Our analysis suggests that there are two strategies for program understanding, the systematic strategy and the as-needed strategy. The programmer using the systematic strategy traces data flow through the program in order to understand global program behavior. The programmer using the as-needed strategy focuses on local program behavior in order to localize study of the program. Our empirical data show that there is a strong relationship between using a systematic approach to acquire knowledge about the program and modifying the program successfully. Programmers who used the systematic approach to study the program constructed successful modifications; programmers who used the as-needed approach failed to construct successful modifications. Programmers who used the systematic strategy gathered knowledge about the causal interactions of the program's functional components. Programmers who used the as-needed strategy did not gather such causal knowledge and therefore failed to detect interactions among components of the program.},
  Doi                      = {10.1016/0164-1212(87)90033-1}
}

@Article{2013:ijase:liu,
  Title                    = {Identification of generalization refactoring opportunities},
  Author                   = {Hui Liu and Zhendong Niu and Zhiyi Ma and Weizhong Shao},
  Journal                  = ijase,
  Year                     = {2013},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {81--110},
  Volume                   = {20},

  Abstract                 = {Generalization refactoring helps relate classes and share functions, including both interfaces and implementation, by inheritance. To apply generalization refactoring, developers should first identify potential generalization refactoring opportunities, i.e., software entities that might benefit from generalization refactoring. For non-trivial software systems, manual identification of these opportunities is challenging and time-consuming. However, to the best of our knowledge, no existing tools have been specifically designed for this task. As a result, people have to identify these opportunities manually or with the help of tools designed for other purposes, e.g., clone detectors. To this end, we propose a tool GenReferee (Generalization Referee) to identify potential refactoring opportunities according to conceptual relationship, implementation similarity, structural correspondence, and inheritance hierarchies. It was first calibrated on two non-trivial open source applications, and then evaluated on another three. Evaluation results suggest that the proposed approach is effective and efficient.},
  Doi                      = {10.1007/s10515-012-0100-0}
}

@InProceedings{2002:icfem:liu,
  Title                    = {A Specification-Based Software Construction Framework for Reuse},
  Author                   = {Liu, Jing and Miao, Huaikou and Gao, Xiaolei},
  Booktitle                = icfem,
  Year                     = {2002},
  Pages                    = {69--79},
  Series                   = lncs,
  Volume                   = {2495},

  Abstract                 = {Software reuse includes low-level components reuse, high-level components reuse and system architecture reuse. High-level components reuse and software architecture reuse are still limited to some domain specific models, while low-level components reuse is constrained by machine's retrieve ability. This paper proposes a mechanism that builds software in three levels, namely software system, high-level components and low-level components. Each level has a unique structure and organization manner. The focus of the paper is on the construction of high-level components and their matching and composition approaches. Design pattern is proposed for building generic high-level components with large number of alternatives. Once a pattern model of high-level component is constructed, it can be directly used or generalized. Design space incorporated with formal specification technology is introduced to not only precisely describe the relationship between high-level components but also easily analyze components matching and composing. The method is illustrated with a debugger example.},
  Doi                      = {10.1007/3-540-36103-0_9}
}

@InProceedings{2012:acmse:liu,
  Title                    = {{MMDiff}: A modeling tool for metamodel comparison},
  Author                   = {Liu, Qichao and Mernik, Marjan and Bryant, Barrett R.},
  Booktitle                = acmse,
  Year                     = {2012},
  Pages                    = {118--123},

  Abstract                 = {In the field of DSM (Domain-Specific Modeling), a popular software development technique, the metamodel plays an important role as it represents a schema definition of the syntax and static semantics to which a model conforms. In model-driven engineering (MDE), issues on metamodel-based research arise, such as metamodel-based model transformation, metamodel-based model refactoring, metamodel-based code generation and metamodel-based inference. Therefore metamodels need to be well studied as an infrastructure to advance these technologies. Although model comparison techniques are widely researched and presented in the community, metamodels are higher level than models resulting in the failure of directly applying current model comparison tools. According to our knowledge, there is not much work addressing metamodel comparison. In this paper, we present a modeling tool named MMDiff for metamodel comparison, detecting the matching and differencing between two metamodels. Two applications are illustrated as a proof of the necessity of conducting metamodel comparison research.},
  Doi                      = {10.1145/2184512.2184540}
}

@Article{2008:jsmerp:lo,
  Title                    = {Mining temporal rules for software maintenance},
  Author                   = {Lo, David and Khoo, Siau-Cheng and Liu, Chao},
  Journal                  = jsmerp,
  Year                     = {2008},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {227--247},
  Volume                   = {20},

  Abstract                 = {Software evolution incurs difficulties in program comprehension and software verification, and hence it increases the cost of software maintenance. In this study, we propose a novel technique to mine from program execution traces a sound and complete set of statistically significant temporal rules of arbitrary lengths. The extracted temporal rules reveal invariants that the program observes, and will consequently guide developers to understand the program behaviors, and facilitate all downstream applications such as verification and debugging. Different from previous studies that were restricted to mining two-event rules (e.g., (lock) $\rightarrow$ (unlock)), our algorithm discovers rules of arbitrary lengths. In order to facilitate downstream applications, we represent the mined rules as temporal logic expressions, so that existing model checkers or other formal analysis toolkit can readily consume our mining results. Performance studies on benchmark data sets and a case study on an industrial system have been performed to show the scalability and utility of our approach. We performed case studies on JBoss application server and a buggy concurrent versions system application, and the result clearly demonstrates the usefulness of our technique in recovering underlying program designs and detecting bugs.},
  Doi                      = {10.1002/smr.v20:4}
}

@InProceedings{2010:icse:loh,
  Title                    = {{LSdiff}: A program differencing tool to identify systematic structural differences},
  Author                   = {Loh, Alex and Kim, Miryung},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {263--266},
  Volume                   = {2},

  Abstract                 = {Program differencing tools such as GNU diff identify individual differences but do not determine how those differences are related to each other. For example, an extract superclass refactoring on several subclasses will be represented by diff as a scattered collection of line additions and deletions which must be manually pieced together. In our previous work, we developed LSdiff, a novel program differencing technique that automatically identifies systematic structural differences as logic rules. This paper presents an LSdiff Eclipse plug-in that provides a summary of systematic structural differences along with textual differences within an Eclipse integrated development environment. This plugin provides several additional features to allow developers to interpret LSdiff rules easily, to select the abstraction level of program differencing analysis, and to reduce its running time through incremental program analysis.}
}

@Article{1984:jss:lohse,
  Title                    = {Experimental evaluation of software design principles: An investigation into the effect of module coupling on system modifiability},
  Author                   = {Lohse, John B. and Zweben, Stuart H.},
  Journal                  = jss,
  Year                     = {1984},

  Month                    = nov,
  Number                   = {4},
  Pages                    = {301--308},
  Volume                   = {4},

  Abstract                 = {The importance of the scientific investigations of software design principles is discussed, and an experimental investigation of the importance of the design principle of module coupling is described. One important dimension of coupling, as promoted by the authors of the structured design methodology, is that of global variable vs. parameterized methods of intermodule communication. It is shown that different proposed software metrics provide conflicting conclusions as to the preferred method of intermodule communication. The three experiments reported herein were performed in university software engineering courses taken by graduate students and upper level undergraduate majors in computer science. They address the effect of global vs. parameterized interfaces on system modifiability. While the type of modification being performed significantly influenced the modifiability of the system, there were no consistent effects due to the type of coupling present in the system.},
  Doi                      = {10.1016/0164-1212(84)90029-3}
}

@InProceedings{2005:aosd:lopes,
  Title                    = {An analysis of modularity in aspect oriented design},
  Author                   = {Lopes, Cristina Videira and Bajracharya, Sushil Krishna},
  Booktitle                = aosd,
  Year                     = {2005},
  Pages                    = {15--26},

  Abstract                 = {We present an analysis of modularity in aspect oriented design using the theory of modular design developed by Baldwin and Clark [10]. We use the three major elements of that theory, namely: i) Design Structure Matrix (DSM), an analysis and modeling tool; ii) Modular Operators, units of variations for design evolution; and iii) Net Options Value (NOV), a quantitative approach to evaluate design. We study the design evolution of a Web Services application where we observe the effects of applying aspect oriented modularization.Based on our analysis we get to the following three main conclusions. First, on the structural part, it is possible to apply the DSM to aspect oriented modularizations in a straightforward manner, i.e. without modifications to DSMs basic model. This shows that aspects can, in fact, be treated as modules of design. Second, the evolution of a design into including aspect modules uses the modular operators proposed by Baldwin and Clark, with a variant of the Inversion operator. This variant captures taking redundant, scattered information hidden in modules and moving it down or keeping it at the same level in the design hierarchy. Third, when calculating and comparing NOVs of the different designs of our application, we obtained higher NOV for the design with aspects than for the design without aspects. This shows that, under this theory of modularity, certain aspect oriented modularizations can add value to the design.}
}

@InProceedings{2007:fase:lopezherrejon,
  Title                    = {Measuring and Characterizing Crosscutting in Aspect-Based Programs: Basic Metrics and Case Studies},
  Author                   = {Roberto E. Lopez-Herrejon and Sven Apel},
  Booktitle                = fase,
  Year                     = {2007},
  Pages                    = {423--437},
  Series                   = lncs,
  Volume                   = {4422},

  Abstract                 = {Aspects are defined as well-modularized crosscutting concerns. Despite being a core tenet of Aspect Oriented Programming, little research has been done in characterizing and measuring crosscutting concerns. Some of the issues that have not been fully explored are: What kinds of crosscutting concerns exist? What language constructs do they use? And what is the impact of crosscutting in actual Aspect Oriented programs? In this paper we present basic code metrics that categorize crosscutting according to the number of classes crosscut and the language constructs used. We applied the metrics to four non-trivial open source programs implemented in AspectJ. We found that for these systems, the number of classes crosscut by advice per crosscutting is small in relation to the number of classes in the program. We argue why we believe this result is not atypical for Aspect Oriented programs and draw a relation to other non-AOP techniques that provide crosscutting.}
}

@Misc{2001:wda:lopresti,
  Title                    = {Web document analysis and information retrieval},

  Author                   = {Daniel Lopresti},
  HowPublished             = {\url{http://cgi.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti.html}},
  Note                     = {International Workshop on Web Document Analysis},
  Year                     = {2001}
}

@Article{2008:tosem:louridas,
  Title                    = {Power laws in software},
  Author                   = {Louridas, Panagiotis and Spinellis, Diomidis and Vlachos, Vasileios},
  Journal                  = tosem,
  Year                     = {2008},

  Month                    = sep,
  Number                   = {1},
  Pages                    = {2:1--2:26},
  Volume                   = {18},

  Abstract                 = {A single statistical framework, comprising power law distributions and scale-free networks, seems to fit a wide variety of phenomena. There is evidence that power laws appear in software at the class and function level. We show that distributions with long, fat tails in software are much more pervasive than previously established, appearing at various levels of abstraction, in diverse systems and languages. The implications of this phenomenon cover various aspects of software engineering research and practice.},
  Doi                      = {10.1145/1391984.1391986}
}

@InProceedings{2004:scam:lung,
  Title                    = {Program restructuring through clustering techniques},
  Author                   = {Chung-Horng Lung and Xia Xu and Zaman, Marzia and Srinivasan, Anand},
  Booktitle                = scam,
  Year                     = {2004},
  Pages                    = {75--84},

  Abstract                 = {Program restructuring is a key method for improving the quality of ill-structured programs, thereby increasing the understandability and reducing the maintenance cost. It is a challenging task and a great deal of research is still ongoing. This work presents an approach to program restructuring at the function level, based on clustering techniques with cohesion as the major concern. Clustering has been widely used to group related entities together. The approach focuses on automated support for identifying ill-structured or low-cohesive functions and providing heuristic advice in both the development and evolution phases. A new similarity measure is defined and studied intensively. The approach is applied to restructure a real industrial program. The empirical observations show that the heuristic advice provided by the approach can help software designers make better decision of why and how to restructure a program. Specific source code level software metrics are presented to demonstrate the value of the approach.}
}

@Article{2006:patrecog:luo,
  Title                    = {A spectral approach to learning structural variations in graphs},
  Author                   = {Bin Luo and Richard C.Wilson and Edwin R. Hancock},
  Journal                  = patrecog,
  Year                     = {2006},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {1188--1198},
  Volume                   = {39},

  Abstract                 = {This paper shows how to construct a linear deformable model for graph structure by performing principal components analysis (PCA) on the vectorised adjacency matrix. We commence by using correspondence information to place the nodes of each of a set of graphs in a standard reference order. Using the correspondences order, we convert the adjacency matrices to long-vectors and compute the long-vector covariance matrix. By projecting the vectorised adjacency matrices onto the leading eigenvectors of the covariance matrix, we embed the graphs in a pattern-space. We illustrate the utility of the resulting method for shape-analysis.},
  Doi                      = {10.1016/j.patcog.2006.01.001}
}

@Article{1990:tse:luqi,
  Title                    = {A Graph Model for Software Evolution},
  Author                   = {Luqi},
  Journal                  = tse,
  Year                     = {1990},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {917--927},
  Volume                   = {16},

  Abstract                 = {A graph model of software evolution is presented. The author seeks to formalize the objects and activities involved in software evolution in sufficient detail to enable automatic assistance for maintaining the consistency and integrity of an evolving software system. This includes automated support for propagating the consequences of a change to a software system. The evolution of large and complex software systems receives particular attention.},
  Doi                      = {10.1109/32.57627}
}

@InProceedings{2007:scam:muller_molina,
  Title                    = {Fast Approximate Matching of Programs for Protecting Libre/Open Source Software by Using Spatial Indexes},
  Author                   = {M{\"u}ller Molina, Arnoldo Jos{\'e} and Shinohara, Takeshi},
  Booktitle                = scam,
  Year                     = {2007},
  Pages                    = {111--122},

  Abstract                 = {To encourage open source/libre software development, it is desirable to have tools that can help to identify open source license violations. This paper describes the implementation of a tool that matches open source programs embedded inside pirate programs. The problem of binary program matching can be approximated by analyzing the similarity of program fragments generated from low-level instructions. These fragments are syntax trees that can be compared by using a tree distance function. Tree distance functions are generally very costly. Sequentially calculating the similarities of fragments with them becomes prohibitively expensive. In this paper we experimentally demonstrate how a spatial index can be used to substantially increase matching performance. These techniques allowed us to do exhaustive experiments that confirmed previous results on the subject. The paper also introduces the novel idea of using information retrieval techniques for calculating the similarity of bags of program fragments. It is possible to identify programs even when they are heavily obfuscated with the innovative approach described here.},
  Doi                      = {10.1109/SCAM.2007.15}
}

@InProceedings{2006:cascon:muller_molina,
  Title                    = {On approximate matching of programs for protecting libre software},
  Author                   = {M{\"u}ller Molina, Arnoldo Jos{\'e} and Shinohara, Takeshi},
  Booktitle                = cascon,
  Year                     = {2006},
  Pages                    = {21:1--21:15},

  Abstract                 = {Libre software licensing schemes are sometimes abused by companies or individuals. In order to encourage open source development it is necessary to build tools that can help in the rapid identification of open source licensing violations. This paper describes an attempt to build such tool. We introduce a framework for approximate matching of programs, and describe an implementation for Java byte-code programs. First, we statically analyze a program to remove dead code, simplify expressions and then extract slices which are generated from assignment statements. We then compare programs by matching between sets of slices based on a distance function. We demonstrate the effectiveness of our method by running experiments on programs generated from two compilers and transformed by two commercial grade control flow obfuscators. Our method achieves an acceptable level of precision.},
  Articleno                = {21},
  Doi                      = {10.1145/1188966.1188994}
}

@InCollection{2008:book:shull:muller,
  Title                    = {Simulation Methods},
  Author                   = {M{\"u}ller, Mark and Pfahl, Dietmar},
  Booktitle                = {Guide to Advanced Empirical Software Engineering},
  Publisher                = {Springer},
  Year                     = {2008},
  Chapter                  = {5},
  Editor                   = {Forrest Shull and Janice Singer and Dag I. K. Sj{\o}berg},
  Pages                    = {117--152},

  Abstract                 = {This chapter aims to raise awareness about the usefulness and importance of simulation in support of software engineering. Simulation is applied in many critical engineering areas and enables one to address issues before they become problems. Simulation---in particular process simulation---is a state of the art technology to analyze process behaviour, risks and complex systems with their inherent uncertainties. Simulation provides insights into the designs of development processes and projects before significant time and cost has been invested, and can be of great benefit in support of training. The systematic combination of simulation methods with empirical research has the potential for becoming a powerful tool in applied software engineering research. The creation of virtual software engineering laboratories helps to allocate available resources of both industry and academia more effectively.},
  Doi                      = {10.1007/978-1-84800-044-5_5}
}

@InProceedings{2010:iccsa:muller-molina,
  Title                    = {On the configuration of the similarity search data structure d-index for high dimensional objects},
  Author                   = {M{\"u}ller-Molina, Arnoldo Jos{\'e} and Shinohara, Takeshi},
  Booktitle                = iccsa,
  Year                     = {2010},
  Pages                    = {443--457},
  Series                   = lncs,
  Volume                   = {6018},

  Abstract                 = {Among similarity search indexes, the D-index introduced by Gennaro et al. in 2001 is regarded as an efficient metric access method. The performance of this index depends on several parameters, and their optimal configuration remains an open problem. We study two performance issues that occur when the D-index handles high dimensional objects. To solve these problems, we introduce an optimization that simplifies the D-index. By doing this, we remove two configuration parameters and improve performance.},
  Doi                      = {10.1007/978-3-642-12179-1\_37},
  Part                     = {3}
}

@InProceedings{1988:icse:muller,
  Title                    = {Rigi: A system for programming-in-the-large},
  Author                   = {H. A. M\"{u}ller and K. Klashinsky},
  Booktitle                = icse,
  Year                     = {1988},
  Pages                    = {80--86},

  Abstract                 = {This paper describes Rigi, a model and a tool for programming-in-the-large. Rigi uses a graph model and abstraction mechanisms to structure and represent the information accumulated during the development process. The objects and relationships of the graph model represent system components and their dependencies. The objects can be arranged in aggregation and generalization hierarchies. The Rigi editor assists the designers, programmers, integrators, and maintainers in defining, manipulating, exploring, and understanding, the structure of large, integrated, evolving software systems. Rigi was designed to address three of the most difficult problems in the area of programming-in-the-large: the mastery of the structural complexity of large software systems, the effective presentation of development information, and the definition of procedures for checking and maintaining the completeness, consistency, and traceability of system descriptions. Thus, the major objective of Rigi is to effectively represent and manipulate the building blocks of a software system and their myriad dependencies, thereby aiding the development phases of the project.}
}

@InProceedings{2001:pfe:maccari,
  Title                    = {Architectural Evolution of Legacy Product Families},
  Author                   = {Alessandro Maccari and Claudio Riva},
  Booktitle                = pfe,
  Year                     = {2001},
  Pages                    = {64--69},
  Series                   = lncs,
  Volume                   = {2290},

  Abstract                 = {Recent research has focused on the concept of product family architecture. We address the more specific case of legacy product families, whose life spans across several years and product generations. We illustrate the method we use to describe legacy product family architecture and manage its evolution. To describe of the family architecture we use two separate documents. The reference architecture, which describes the abstract architecture that is instantiated in every product, and contains architecturally significant rules for adding new components to the system. And the configuration architecture, which maps the product family features into the various products, thus, allowing to model commonality and variability. The concept of a family is an abstraction that automatically generates a new layer in every product. This layer includes all the software that is common to other products in the family, and is, naturally, less prone to change than the layer constituted by software which is specific to the product. In certain domains like mobile telecommunications, when new products are added to the family, they tend to share most of the stable features that belong to legacy products. This phenomenon abstracts the issues of architectural evolution from the single products to the entire family scope. We also sketch the process we follow to maintain the documents that model the product family architecture. Our approach combines reverse and forward architecting activities, and is currently applied in Nokia Mobile Phones. Research on the issues of architectural modelling is still insufficient: we propose some hints for future work.}
}

@Article{2006:mansci:maccormack,
  Title                    = {Exploring the Structure of Complex Software Designs: An Empirical Study of Open Source and Proprietary Code},
  Author                   = {MacCormack, Alan and Rusnak, John and Baldwin, Carliss Y.},
  Journal                  = mansci,
  Year                     = {2006},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {1015--1030},
  Volume                   = {52},

  Abstract                 = {Much recent research has pointed to the critical role of architecture in the development of a firm's products, services and technical capabilities. A common theme in these studies is the notion that specific characteristics of a product's design---for example, the degree of modularity it exhibits---can have a profound effect on among other things, its performance, the flexibility of the process used to produce it, the value captured by its producer, and the potential for value creation at the industry level. Unfortunately, this stream of work has been limited by the lack of appropriate tools, metrics and terminology for characterizing key attributes of a product's architecture in a robust fashion. As a result, there is little empirical evidence that the constructs emerging in the literature have power in predicting the phenomena with which they are associated. This paper reports data from a research project which seeks to characterize the differences in design structure between complex software products. In particular, we adopt a technique based upon Design Structure Matrices (DSMs) to map the dependencies between different elements of a design then develop metrics that allow us to compare the structures of these different DSMs. We demonstrate the power of this approach in two ways: First, we compare the design structures of two complex software products---the Linux operating system and the Mozilla web browser---that were developed via contrasting modes of organization: specifically, open source versus proprietary development. We find significant differences in their designs, consistent with an interpretation that Linux possesses a more ``modular" architecture. We then track the evolution of Mozilla, paying particular attention to a major ``re-design" effort that took place several months after its release as an open source product. We show that this effort resulted in a design structure that was significantly more modular than its predecessor, and indeed, more modular than that of a comparable version of Linux. Our findings demonstrate that it is possible to characterize the structure of complex product designs and draw meaningful conclusions about the precise ways in which they differ. We provide a description of a set of tools that will facilitate this analysis for software products, which should prove fruitful for researchers and practitioners alike. Empirically, the data we provide, while exploratory, is consistent with a view that different modes of organization may tend to produce designs possessing different architectural characteristics. However, we also find that purposeful efforts to re-design a product's architecture can have a significant impact on the structure of a design, at least for products of comparable complexity to the ones we examine here.},
  Doi                      = {10.1287/mnsc.1060.0552}
}

@InProceedings{2007:iceud:maceli,
  Title                    = {From human crafters to human factors to human actors and back again: Bridging the design time--use time divide},
  Author                   = {Maceli, Monica and Atwood, Michael E.},
  Booktitle                = iceud,
  Year                     = {2011},
  Pages                    = {76--91},

  Abstract                 = {Meta-design theory emphasizes that future use can never be entirely anticipated at design time, as users shape their environments in response to emerging needs; systems should therefore be designed to adapt to future conditions in the hands of end users. For most of human history, all design was metadesign; designers were also users, and the environments of design and use were one and the same. Technology introduced a divide between the skilled producers and unskilled consumers of technology, and between design time and use time. In our increasingly complex technological environments, tomorrow's meta-designers must be able to anticipate the environment in which the end users will work in order to provide the flexibility for users to craft their tools. By exploring and projecting forward current trends in technology use, we have identified key principles for meta-designers and suggest that using them as design heuristics will aid meta-designers in crafting systems for future end-users.}
}

@InProceedings{1996:fse:magee,
  Title                    = {Dynamic Structure in Software Architectures},
  Author                   = {Magee, Jeff and Kramer, Jeff},
  Booktitle                = fse,
  Year                     = {1996},
  Pages                    = {3--13},

  Abstract                 = {Much of the recent work on Architecture Description Languages (ADL) has concentrated on specifying organisations of components and connectors which are static. When the ADL specification is used to drive system construction, then the structure of the resulting system in terms of its component instances and their interconnection is fixed. This paper examines ADL features which permit the description of dynamic software architectures in which the organisation of components and connectors may change during system execution.The paper outlines examples of language features which support dynamic structure. These examples are taken from Darwin, a language used to describe distributed system structure. An operational semantics for these features is presented in the $\pi$-calculus, together with a discussion of their advantages and limitations. The paper discusses some general approaches to dynamic architecture description suggested by these examples.}
}

@Article{1991:sej:maiden,
  Title                    = {Analogy as a paradigm for specification reuse},
  Author                   = {Maiden, N.},
  Journal                  = sej,
  Year                     = {1991},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {3--15},
  Volume                   = {6},

  Abstract                 = {Specification reuse by analogy is proposed as an alternative paradigm to support requirements analysis. The paradigm is founded on a descriptive model of analogy in software engineering problems, which suggests critical determinants of software engineering analogies. This model is discussed in terms of two non-simple software engineering analogies, which also suggest the potential of specification reuse by analogy. The requirements of a CASE tool to support reuse of specifications are proposed.},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=73705}
}

@Article{1992:cacm:maiden,
  Title                    = {Exploiting reusable specifications through analogy},
  Author                   = {Maiden, Neil and Sutcliffe, Alistair},
  Journal                  = cacm,
  Year                     = {1992},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {55--64},
  Volume                   = {35},

  Abstract                 = {The potential of reusing existing specifications to develop new systems has been brought closer by the CASE tool revolution. It has been suggested that successful specification reuse can assist requirements software engineers to develop more complete, consistent and clearly defined specifications. Analogy may be a powerful paradigm with which to exploit specification reuse, but it has received little attention in the literature [7, 16]. This article investigates the potential of specification reuse by analogy and its possible benefits for requirements analysis. A method for successful specification reuse is reported, and implications for CASE tool support are discussed.},
  Doi                      = {10.1145/129852.129857}
}

@InProceedings{1999:ase:maletic,
  Title                    = {Automatic Software Clustering via Latent Semantic Analysis},
  Author                   = {Maletic, Jonathan I. and Valluri, Naveen},
  Booktitle                = ase,
  Year                     = {1999},
  Pages                    = {251--254},

  Abstract                 = {The paper describes the initial results of applying Latent Semantic Analysis (LSA) to program source code and associated documentation. Latent Semantic Analysis is a corpus-based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. This methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). The intent of applying Latent Semantic Analysis to software components is to automatically induce a specific semantic meaning of a given component. Here LSA is used as the basis to cluster software components. Results of applying this method to the LEDA library and MINIX operating system are given. Applying Latent Semantic Analysis to the domain of source code and internal documentation for the support of software reuse is a new application of this method and a departure from the normal application domain of natural language.},
  Doi                      = {10.1109/ASE.1999.802296}
}

@Article{2003:ijase:malpohl,
  Title                    = {Renaming detection},
  Author                   = {Guido Malpohl and James J. Hunt and Walter F. Tichy},
  Journal                  = ijase,
  Year                     = {2003},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {183--202},
  Volume                   = {10},

  Abstract                 = {Finding changed identifiers is important for understanding the difference between two versions of a program and for detecting and resolving conflicts while merging variants of a program together. Standard practice for differencing and merging relies on line based techniques that do not recognize renamed identifiers. The design and implementation of a tool to automatically detect renamed identifiers between two versions of a program is presented. The system uses an abstract representation of language constructs to enable language awareness without introducing language dependence. Modules for Java and Scheme have been written. The detector works with multiple file pairs, taking into account renamings that span several files. A case study is presented that demonstrates proof of concept. The detector is part of a suite of intelligent differencing and merging programs that exploit the static semantics of programming languages.},
  Doi                      = {10.1023/A:1022968013020}
}

@InProceedings{2005:pldi:mandelin,
  Title                    = {Jungloid mining: Helping to navigate the {API} jungle},
  Author                   = {David Mandelin and Lin Xu and Rastislav Bod\'{\i}k and Doug Kimelman},
  Booktitle                = pldi,
  Year                     = {2005},
  Pages                    = {48--61},

  Abstract                 = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs.We implemented a tool, prospector, based on these techniques. prospector is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested prospector on a set of real programming problems involving APIs; prospector found the desired solution for 18 of 20 problems. We also evaluated prospector in a user study, finding that programmers solved programming problems more quickly and with more reuse when using prospector than without prospector.},
  Doi                      = {10.1145/1065010.1065018}
}

@Article{1947:ams:mann,
  Title                    = {On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other},
  Author                   = {Mann, Henry B. and Whitney, Donald R.},
  Journal                  = ams,
  Year                     = {1947},
  Number                   = {1},
  Pages                    = {50--60},
  Volume                   = {18},

  Abstract                 = {Let $x$ and $y$ be two random variables with continuous cumulative distribution functions $f$ and $g$. A statistic $U$ depending on the relative ranks of the $x$'s and $y$'s is proposed for testing the hypothesis $f=g$. Wilcoxon proposed an equivalent test in the Biometrics Bulletin, December, 1945, but gave only a few points of the distribution of his statistic. Under the hypothesis $f=g$ the probability of obtaining a given $U$ in a sample of $nx$'s and $my$'s is the solution of a certain recurrence relation involving $n$ and $m$. Using this recurrence relation tables have been computed giving the probability of $U$ for samples up to $n=m=8$. At this point the distribution is almost normal. From the recurrence relation explicit expressions for the mean, variance, and fourth moment are obtained. The 2rth moment is shown to have a certain form which enabled us to prove that the limit distribution is normal if $m$,$n$ go to infinity in any arbitrary manner. The test is shown to be consistent with respect to the class of alternatives $f(x)>g(x)$ for every $x$.},
  Doi                      = {10.1214/aoms/1177730491}
}

@Book{2008:book:manning,
  Title                    = {Introduction to Information Retrieval},
  Author                   = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Sch{\"u}tze},
  Publisher                = {Cambridge University Press},
  Year                     = {2008}
}

@Article{2009:tse:mika,
  Title                    = {What Types of Defects Are Really Discovered in Code Reviews?},
  Author                   = {Mika V. Mantyla and Casper Lassenius},
  Journal                  = tse,
  Year                     = {2009},

  Month                    = may,
  Number                   = {3},
  Pages                    = {430--448},
  Volume                   = {35},

  Abstract                 = {Research on code reviews has often focused on defect counts instead of defect types, which offers an imperfect view of code review benefits. In this paper, we classified the defects of nine industrial (C/C++) and 23 student (Java) code reviews, detecting 388 and 371 defects, respectively. First, we discovered that 75 percent of defects found during the review do not affect the visible functionality of the software. Instead, these defects improved software evolvability by making it easier to understand and modify. Second, we created a defect classification consisting of functional and evolvability defects. The evolvability defect classification is based on the defect types found in this study, but, for the functional defects, we studied and compared existing functional defect classifications. The classification can be useful for assigning code review roles, creating checklists, assessing software evolvability, and building software engineering tools. We conclude that, in addition to functional defects, code reviews find many evolvability defects and, thus, offer additional benefits over execution-based quality assurance methods that cannot detect evolvability defects. We suggest that code reviews may be most valuable for software products with long life cycles as the value of discovering evolvability defects in them is greater than for short life cycle systems.},
  Doi                      = {10.1109/TSE.2008.71}
}

@InProceedings{2011:ecoop:maoz,
  Title                    = {{CDDiff}: Semantic differencing for class diagrams},
  Author                   = {Maoz, Shahar and Ringert, Jan Oliver and Rumpe, Bernhard},
  Booktitle                = ecoop,
  Year                     = {2011},
  Pages                    = {230--254},
  Series                   = lncs,
  Volume                   = {6813},

  Abstract                 = {Class diagrams (CDs), which specify classes and the relationships between them, are widely used for modeling the structure of object-oriented systems. As models, programs, and systems evolve over time, during the development lifecycle and beyond it, effective change management is a major challenge in software development, which has attracted much research efforts in recent years. In this paper we present cddiff, a semantic diff operator for CDs. Unlike most existing approaches to model comparison, which compare the concrete or the abstract syntax of two given diagrams and output a list of syntactical changes or edit operations, cddiff considers the semantics of the diagrams at hand and outputs a set of diff witnesses, each of which is an object model that is possible in the first CD and is not possible in the second. We motivate the use of cddiff, formally define it, and show how it is computed. The computation is based on a reduction to Alloy. The work is implemented in a prototype Eclipse plug-in. Examples show the unique contribution of our approach to the state-of-the-art in version comparison and evolution analysis.},
  Doi                      = {10.1007/978-3-642-22655-7_12}
}

@InProceedings{2011:esec_fse:maoz,
  Title                    = {{ADDiff}: Semantic differencing for activity diagrams},
  Author                   = {Shahar Maoz and Jan Oliver Ringert and Bernhard Rumpe},
  Booktitle                = esec_fse,
  Year                     = {2011},
  Pages                    = {179--189},

  Abstract                 = {Activity diagrams (ADs) have recently become widely used in the modeling of workflows, business processes, and web-services, where they serve various purposes, from documentation, requirement definitions, and test case specifications, to simulation and code generation. As models, programs, and systems evolve over time, understanding changes and their impact is an important challenge, which has attracted much research efforts in recent years. In this paper we present addiff, a semantic differencing operator for ADs. Unlike most existing approaches to model comparison, which compare the concrete or the abstract syntax of two given diagrams and output a list of syntactical changes or edit operations, addiff considers the semantics of the diagrams at hand and outputs a set of diff witnesses, each of which is an execution trace that is possible in the first AD and is not possible in the second. We motivate the use of addiff, formally define it, and show two algorithms to compute it, a concrete forward-search algorithm and a symbolic fixpoint algorithm, implemented using BDDs and integrated into the Eclipse IDE. Empirical results and examples demonstrate the feasibility and unique contribution of addiff to the state-of-the-art in version comparison and evolution analysis.},
  Doi                      = {10.1145/2025113.2025140}
}

@Article{2007:tse:maqbool,
  Title                    = {Hierarchical Clustering for Software Architecture Recovery},
  Author                   = {Maqbool, Onaiza and Babri, Haroon},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {759--780},
  Volume                   = {33},

  Abstract                 = {Gaining an architectural level understanding of a software system is important for many reasons. When the description of a system's architecture does not exist, attempts must be made to recover it. In recent years, researchers have explored the use of clustering for recovering a software system's architecture, given only its source code. The main contributions of this paper are given as follows. First, we review hierarchical clustering research in the context of software architecture recovery and modularization. Second, to employ clustering meaningfully, it is necessary to understand the peculiarities of the software domain, as well as the behavior of clustering measures and algorithms in this domain. To this end, we provide a detailed analysis of the behavior of various similarity and distance measures that may be employed for software clustering. Third, we analyze the clustering process of various well-known clustering algorithms by using multiple criteria, and we show how arbitrary decisions taken by these algorithms during clustering affect the quality of their results. Finally, we present an analysis of two recently proposed clustering algorithms, revealing close similarities in their apparently different clustering approaches. Experiments on four legacy software systems provide insight into the behavior of well-known clustering algorithms and their characteristics in the software domain.},
  Doi                      = {10.1109/TSE.2007.70732}
}

@InProceedings{2004:esug:marchesi,
  Title                    = {Power laws in {S}malltalk},
  Author                   = {Michele Marchesi and Sandro Pinna and Nicola Serra and Stefano Tuveri},
  Booktitle                = esug,
  Year                     = {2004},
  Note                     = {17~pages},

  Abstract                 = {Many real systems have been described as complex networks, where nodes represent specific parts of the system and connections represent relationships among them. Examples of such networks come from very different contexts. Traditional theories suggest to represent complex systems as random graphs, according to the models proposed by Erd\H{o}s and R\'{e}nyi. However, there is an increasing evidence that many real world systems behave in different ways displaying scale-free distributions of nodes degree. Random graphs are not suitable to describe such behaviors, thus new models have been proposed in recent years. Recently, it has emerged the interest in applying complex network theories to represent large software systems. Software applications are made of modules and relationships among them. Thus, a representation based on graph theory is natural. This work presents our study in this context. We analyzed four large Smalltalk systems. For each one we built the graph representing all system classes and the relationships among them. We show that these graphs display scale free characteristics, in accordance to recent studies on other large software systems.}
}

@InProceedings{2003:icse:marcus,
  Title                    = {Recovering documentation-to-source-code traceability links using latent semantic indexing},
  Author                   = {Marcus, Andrian and Maletic, Jonathan I.},
  Booktitle                = icse,
  Year                     = {2003},
  Pages                    = {125--135},

  Abstract                 = {An information retrieval technique, latent semantic indexing, is used to automatically identify traceability links from system documentation to program source code. The results of two experiments to identify links in existing software systems (i.e., the LEDA library, and Albergate) are presented. These results are compared with other similar type experimental results of traceability link identification using different types of information retrieval techniques. The method presented proves to give good results by comparison and additionally it is a low cost, highly flexible method to apply with regards to preprocessing and/or parsing of the source code and documentation.},
  Doi                      = {10.1109/ICSE.2003.1201194}
}

@InProceedings{2001:ase:marcus,
  Title                    = {Identification of High-Level Concept Clones in Source Code},
  Author                   = {Marcus, Andrian and Maletic, Jonathan I.},
  Booktitle                = ase,
  Year                     = {2001},
  Pages                    = {107--114},

  Abstract                 = {Source code duplication occurs frequently within large software systems. Pieces of source code, functions, and data types are often duplicated in part, or in whole, for a variety of reasons. Programmers may simply be reusing a piece of code via copy and paste or they may be ``re-inventing the wheel". Previous research on the detection of clones is mainly focused on identifying pieces of code with similar (or nearly similar) structure. Our approach is to examine the source code text (comments and identifiers) and identify implementations of similar high-level concepts (e.g., abstract data types). The approach uses an information retrieval technique (i.e., latent semantic indexing) to statically analyze the software system and determine semantic similarities between source code documents (i.e., functions, files, or code segments). These similarity measures are used to drive the clone detection process. The intention of our approach is to enhance and augment existing clone detection methods that are based on structural analysis. This synergistic use of methods will improve the quality of clone detection. A set of experiments is presented that demonstrate the usage of semantic similarity measure to identify clones within a version of NCSA Mosaic.}
}

@InProceedings{2005:icsm:marcus,
  Title                    = {The Conceptual Cohesion of Classes},
  Author                   = {Andrian Marcus and Denys Poshyvanyk},
  Booktitle                = icsm,
  Year                     = {2005},
  Pages                    = {133--142},

  Abstract                 = {While often defined in informal ways, software cohesion reflects important properties of modules in a software system. Cohesion measurement has been used for quality assessment, fault proneness prediction, software modularization, etc. Existing approaches to cohesion measurement in Object-Oriented software are largely based on the structural information of the source code, such as attribute references in methods. These measures reflect particular interpretations of cohesion and try to capture different aspects of cohesion and no single cohesion metric or suite is accepted as standard measurement for cohesion. The paper proposes a new set of measures for the cohesion of individual classes within an OO software system, based on the analysis of the semantic information embedded in the source code, such as comments and identifiers. A case study on open source software is presented, which compares the new measures with an extensive set of existing metrics. The differences and similarities among the approaches and results are discussed and analyzed.}
}

@InProceedings{2004:wcre:marcus,
  Title                    = {An Information Retrieval Approach to Concept Location in Source Code},
  Author                   = {Marcus, Andrian and Sergeyev, Andrey and Rajlich, V{\'a}clav and Maletic, Jonathan I.},
  Booktitle                = wcre,
  Year                     = {2004},
  Pages                    = {214--223},

  Abstract                 = {Concept location identifies parts of a software system that implement a specific concept that originates from the problem or the solution domain. Concept location is a very common software engineering activity that directly supports software maintenance and evolution tasks such as incremental change and reverse engineering. This paper addresses the problem of concept location using an advanced information retrieval method, Latent Semantic Indexing (LSI). LSI is used to map concepts expressed in natural language by the programmer to the relevant parts of the source code. Results of a case study on NCSA Mosaic are presented and compared with previously published results of other static methods for concept location.}
}

@InProceedings{2007:wcre:marin,
  Title                    = {Documenting Typical Crosscutting Concerns},
  Author                   = {Marin, Marius and Moonen, Leon and van Deursen, Arie},
  Booktitle                = wcre,
  Year                     = {2007},
  Pages                    = {31--40},

  Abstract                 = {Our analysis of crosscutting concerns in real-life software systems (totaling over 500,000 LOC) and in reports from literature indicated a number of properties that allow for their decomposition in primitive building blocks which are atomic crosscutting concerns. We classify these blocks in crosscutting concern sorts, and we use them to describe the cross- cutting structure of many (well-known) designs and common mechanisms in software systems. In this paper, we formalize the notion of crosscutting concern sorts by means of relational queries over (object-oriented) source models. Based on these queries, we present a concern management tool called SoQueT, which can be used to document the occurrences of crosscutting concerns in object-oriented systems. We assess the sorts-based approach by using the tool to cover various crosscutting concerns in two open-source systems: JHotDraw and Java PetStore.}
}

@InProceedings{2005:icsm:marin,
  Title                    = {A Classification of Crosscutting Concerns},
  Author                   = {Marin, Marius and Moonen, Leon and van Deursen, Arie},
  Booktitle                = icsm,
  Year                     = {2005},
  Pages                    = {673--676},

  Abstract                 = {Refactoring software to apply aspect oriented solutions requires a clear understanding of what are the potential crosscutting concerns and which aspect solutions to replace them with. This process can benefit from the recognition of recurring generic concerns and their reusable aspect solutions. In this paper, we propose a classification of crosscutting concerns in sorts based on the analysis of various refactoring efforts. We discuss how sorts help concern understanding and refactoring, how they support the identification of crosscutting concerns, and how they can contribute to the evolution of aspect languages.}
}

@InProceedings{2005:macs:marin,
  Title                    = {An approach to aspect refactoring based on crosscutting concern types},
  Author                   = {Marin, Marius and Moonen, Leon and van Deursen, Arie},
  Booktitle                = macs,
  Year                     = {2005},
  Note                     = {5~pages},

  Abstract                 = {We argue for the importance of organizing generic crosscutting concerns by distinctive properties and describing them as types. A type's properties consist of a general intent, an implementation idiom criteria, and one (desired) aspect language mechanism to address the concerns within the specific type. We argue the usefulness of this approach for aspect refactoring, and in the areas of concern identification and aspect languages development.}
}

@InProceedings{2011:promise:marks,
  Title                    = {Studying the fix-time for bugs in large open source projects},
  Author                   = {Marks, Lionel and Zou, Ying and Hassan, Ahmed E.},
  Booktitle                = promise,
  Year                     = {2011},
  Pages                    = {11:1--11:8},

  Abstract                 = {Background: Bug fixing lies at the core of most software maintenance efforts. Most prior studies examine the effort needed to fix a bug (fix-effort). However, the effort needed to fix a bug may not correlate with the calendar time needed to fix it (fix-time). For example, the fix-time for bugs with low fix-effort may be long if they are considered to be of low priority. Aims: We study the fix-time for bugs in large open source projects. Method: We study the fix-time along three dimensions: (1) the location of the bug (e.g., which component), (2) the reporter of the bug, and (3) the description of the bug. Using these three dimensions and their associated attributes, we examine the fix-time for bugs in two large open source projects: Eclipse and Mozilla, using a random forest classifier. Results: We show that we can correctly classify $\tilde$65\% of the time the fix-time for bugs in these projects. We perform a sensitivity analysis to identify the most important attributes in each dimension. We find that the time of the filing of a bug and its location are the most important attributes in the Mozilla project for determining the fix-time of a bug. On the other hand, the fix-time in the Eclipse project is highly dependant on the severity of the bug. Surprisingly, the priority of the bug is not an important attribute when determining the fix-time for a bug in both projects. Conclusion: Attributes affecting the fix-time vary between projects and vary over time within the same project.},
  Doi                      = {10.1145/2020390.2020401}
}

@InProceedings{2006:icse:martin,
  Title                    = {Understanding software application interfaces via string analysis},
  Author                   = {Evan Martin and Tao Xie},
  Booktitle                = icse,
  Year                     = {2006},
  Pages                    = {901--904},

  Abstract                 = {In software systems, different software applications often interact with each other through specific interfaces by exchanging data in string format. For example, web services interact with each other through XML strings. Database applications interact with a database through strings of SQL statements. Sometimes these interfaces between different software applications are complex and distributed. For example, a table in a database can be accessed by multiple methods in a database application and a single method can access multiple tables. In this paper, we propose an approach to understanding software application interfaces through string analysis. The approach first performs a static analysis of source code to identify interaction points (in the form of interface-method-call sites). We then leverage existing string analysis tools to collect all possible string data that can be sent through these different interaction points. Then we manipulate collected string data by grouping similar data together. For example, we group together all collected SQL statements that access the same table. Then we associate various parts of aggregated data with interaction points in order to show the connections between entities from interacting applications. Our preliminary results show that the approach can help us understand the characteristics of interactions between database applications and databases. We also identify some challenges in this approach for our future work.},
  Doi                      = {10.1145/1134285.1134447}
}

@InProceedings{1994:ptdoosm:martin,
  Title                    = {{OO} design quality metrics: An analysis of dependencies},
  Author                   = {Robert Martin},
  Booktitle                = ptdoosm,
  Year                     = {1994},
  Note                     = {8~pages},

  Abstract                 = {This paper describes a set of metrics that can be used to measure the quality of an object-oriented design in terms of the interdependence between the subsystems of that design. Designs which are highly interdependent tend to be rigid, unreusable and hard to maintain. Yet interdependence is necessary if the subsystems of the design are to collaborate. Thus, some forms of dependency must be desirable, and other forms must be undesirable. This paper proposes a design pattern in which all the dependencies are of the desirable form. Finally, this paper describes a set of metrics that measure the conformance of a design to the desirable pattern.}
}

@InProceedings{2008:isec:maskeri,
  Title                    = {Mining business topics in source code using latent {D}irichlet allocation},
  Author                   = {Maskeri, Girish and Sarkar, Santonu and Heafield, Kenneth},
  Booktitle                = isec,
  Year                     = {2008},
  Pages                    = {113--120},

  Abstract                 = {One of the difficulties in maintaining a large software system is the absence of documented business domain topics and correlation between these domain topics and source code. Without such a correlation, people without any prior application knowledge would find it hard to comprehend the functionality of the system. Latent Dirichlet Allocation (LDA), a statistical model, has emerged as a popular technique for discovering topics in large text document corpus. But its applicability in extracting business domain topics from source code has not been explored so far. This paper investigates LDA in the context of comprehending large software systems and proposes a human assisted approachbased on LDA for extracting domain topics from source code. This method has been applied on a number of open source and proprietary systems. Preliminary results indicate that LDA is able to identify some of the domain topics and isa satisfactory starting point for further manual refinement of topics}
}

@InProceedings{2009:uist:matejka,
  Title                    = {{CommunityCommands}: Command recommendations for software applications},
  Author                   = {Matejka, Justin and Li, Wei and Grossman, Tovi and Fitzmaurice, George},
  Booktitle                = uist,
  Year                     = {2009},
  Pages                    = {193--202},

  Abstract                 = {We explore the use of modern recommender system technology to address the problem of learning software applications. Before describing our new command recommender system, we first define relevant design considerations. We then discuss a 3 month user study we conducted with professional users to evaluate our algorithms which generated customized recommendations for each user. Analysis shows that our item-based collaborative filtering algorithm generates 2.1 times as many good suggestions as existing techniques. In addition we present a prototype user interface to ambiently present command recommendations to users, which has received promising initial user feedback.},
  Doi                      = {10.1145/1622176.1622214}
}

@InProceedings{2009:msr:matter,
  Title                    = {Assigning bug reports using a vocabulary-based expertise model of developers},
  Author                   = {Matter, Dominique and Kuhn, Adrian and Nierstrasz, Oscar},
  Booktitle                = msr,
  Year                     = {2009},
  Pages                    = {131--140},

  Abstract                 = {For popular software systems, the number of daily submitted bug reports is high. Triaging these incoming reports is a time consuming task. Part of the bug triage is the assignment of a report to a developer with the appropriate expertise. In this paper, we present an approach to automatically suggest developers who have the appropriate expertise for handling a bug report. We model developer expertise using the vocabulary found in their source code contributions and compare this vocabulary to the vocabulary of bug reports. We evaluate our approach by comparing the suggested experts to the persons who eventually worked on the bug. Using eight years of Eclipse development as a case study, we achieve 33.6\% top-1 precision and 71.0\% top-10 recall.},
  Doi                      = {10.1109/MSR.2009.5069491}
}

@Article{1999:sqj:mayer,
  Title                    = {A Critical Analysis of Current {OO} Design Metrics},
  Author                   = {Tobias Mayer and Tracy Hall},
  Journal                  = sqj,
  Year                     = {1999},

  Month                    = oct,
  Number                   = {2},
  Pages                    = {97--110},
  Volume                   = {8},

  Abstract                 = {Chidamber and Kemerer (C&K) outlined some initial proposals for language-independent OO design metrics in 1991. This suite is expanded on by C\&K in 1994 and the metrics were tested on systems developed in C++ and Smalltalk. The six metrics making up the C\&K suite can be criticised for a number of reasons. This does not make them bad metrics; on the contrary the C\&K work represents one of the most thorough treatments of the subject at the current time. However, the authors explicitly state, ``there is no reason to believe that the proposed metrics will be found to be comprehensive, and further work could result in additions, changes and possible deletions from this suite.'' This analysis will serve to make other researchers and practitioners aware of some of the problems that may arise from using these measures. As a by-product, the axioms of E. Weyuker (1983) come under scrutiny in terms of their applicability to object-oriented metrics.},
  Doi                      = {10.1023/A:1008900825849}
}

@InProceedings{1994:iccd:mayrhauser,
  Title                    = {Domain Based Testing: Increasing Test Case Reuse},
  Author                   = {von Mayrhauser, Anneliese and Richard Mraz and Jeff Walls and Pete Ocken},
  Booktitle                = iccd,
  Year                     = {1994},
  Pages                    = {484--491},

  Abstract                 = {Domain based testing (DBT) is a test generation method based on two concepts from software reuse, domain analysis and domain modeling. We applied DBT to command-based systems where the domain model represents the syntax and semantics of the command language. The domain model drives the generation of test cases. The test generation process consists of three stages: scripting; command template generation; and parameter value selection. Because it is based on ideas from software reuse, DBT also provides a good structure for test case reuse. Production use of the DBT tool Sleuth to generate system tests for an automated robot tape library confirms that DBT provides a wide variety of test case reuse scenarios.},
  Doi                      = {10.1109/ICCD.1994.331957}
}

@Article{1995:computer:von_mayrhauser,
  Title                    = {Program Comprehension during Software Maintenance and Evolution},
  Author                   = {von Mayrhauser, Anneliese and Vans, A. Marie},
  Journal                  = computer,
  Year                     = {1995},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {44--55},
  Volume                   = {28},

  Abstract                 = {How well programmers comprehend programs is key to effective software maintenance and evolution. But how exactly do programmers understand code? Over the years, several code-comprehension models have been developed to help researchers answer this question. Five types of tasks are commonly associated with software maintenance and evolution: adaptive, perfective, and corrective maintenance; reuse; and code leverage. Each type has its own set of typical activities. Some of them, such as understanding a system or problem, are common to several tasks. Code comprehension models describe the cognitive processes involved in these tasks. Experiments support some, but not all of these models. The authors analyze six comprehension models and their validation experiments to determine the current state of knowledge program comprehension offers. They begin by describing some common elements of cognition models. Programmer knowledge plays a key role in the understanding process. The mental model--an internal, working representation of the software under consideration--includes both static and dynamic elements. Static elements include text-structure knowledge, ``chunks," plans, hypotheses, beacons, and rules of discourse. Dynamic elements include strategies, actions, episodes, and processes. The authors compare the six models in this light and identify a need for more experimental studies with experienced software engineers working on specific maintenance tasks and large-scale code in state-of-the-art computing environments.},
  Doi                      = {10.1109/2.402076}
}

@Article{1976:tse:mccabe,
  Title                    = {A Complexity Measure},
  Author                   = {McCabe, Thomas J.},
  Journal                  = tse,
  Year                     = {1976},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {308--320},
  Volume                   = {2},

  Abstract                 = {This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program complexity. The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the correlation between intuitive complexity and the graph-theoretic complexity. Several properties of the graph-theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program.},
  Doi                      = {10.1109/TSE.1976.233837}
}

@Book{2004:book:mcconnell,
  Title                    = {Code Complete},
  Author                   = {Steve McConnell},
  Publisher                = {Microsoft Press},
  Year                     = {2004},
  Edition                  = {2nd}
}

@InProceedings{1996:icsm:mccrickard,
  Title                    = {Assessing the impact of changes at the architectural level: A case study on graphical debuggers},
  Author                   = {McCrickard, D. Scott and Abowd, Gregory D.},
  Booktitle                = icsm,
  Year                     = {1996},
  Pages                    = {59--67},

  Abstract                 = {Understanding the high level structure of a software system is useful for determining the impact of proposed changes. We investigate techniques for recovering the high level structure, or software architecture, for cases in which no trusted architectural description exists. We then demonstrate how a scenario based approach to architectural evaluation can be used to document the impact of proposed changes on a software system. Architectural recovery and impact analysis are demonstrated using a case study comparing two graphical debuggers, xdbx and ups.},
  Doi                      = {10.1109/ICSM.1996.564989}
}

@InProceedings{1997:www:mccrickard,
  Title                    = {Visualizing Search Results using {SQWID}},
  Author                   = {S. Scott McCrickard and Colleen M. Kehoe},
  Booktitle                = www,
  Year                     = {1997},

  Abstract                 = {Most approaches to displaying search results create a list of results with some fixed order. Missing is the ability to explore common topics within the set of search results. This paper examines techniques to solve this problem and introduces SQWID, a system that uses many of these techniques. The SQWID (Search Query Weighted Information Display) system provides an interactive visualization of the search results, allowing users to see the relevance of the results to different key terms.},
  Url                      = {http://www.ra.ethz.ch/cdstore/www6/Posters/739/sqwid.html}
}

@Article{1982:spe:mcgregor,
  Title                    = {Backtrack search algorithms and the maximal common subgraph problem},
  Author                   = {James J. McGregor},
  Journal                  = spe,
  Year                     = {1982},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {23--34},
  Volume                   = {12},

  Abstract                 = {Backtrack algorithms are applicable to a wide variety of problems. An efficient but readable version of such an algorithm is presented and its use in the problem of finding the maximal common subgraph of two graphs is described. Techniques available in this application area for ordering and pruning the backtrack search are discussed. This algorithm has been used successfully as a component of a program for analysing chemical reactions and enumerating the bond changes which have taken place.},
  Doi                      = {10.1002/spe.4380120103}
}

@InProceedings{1968:nato:mcilroy,
  Title                    = {Mass-produced software components},
  Author                   = {M. D. McIlroy},
  Booktitle                = {Software Engineering: Report on a Conference by the NATO Science Committee},
  Year                     = {1968},
  Pages                    = {138--155},

  Abstract                 = {Software components (routines), to be widely applicable to different machines and users, should be available in families arranged according to precision, robustness, generality and time-space performance. Existing sources of components---manufacturers, software houses, users' groups and algorithm collections---lack the breadth of interest or coherence of purpose to assemble more than one or two members of such families, yet software production in the large would be enormously helped by the availability of spectra of high quality routines, quite as mechanical design is abetted by the existence of families of structural shapes, screws or resistors. The talk will examine the kinds of variability necessary in software components, ways of producing useful inventories, types of components that are ripe for such standardization, and methods of instituting pilot production.},
  Url                      = {http://homepages.cs.ncl.ac.uk/brian.randell/NATO/nato1968.PDF}
}

@InProceedings{2012:icse:mcmillan,
  Title                    = {Recommending source code for use in rapid software prototypes},
  Author                   = {McMillan, Collin and Hariri, Negar and Poshyvanyk, Denys and Cleland-Huang, Jane and Mobasher, Bamshad},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {848--858},

  Abstract                 = {Rapid prototypes are often developed early in the software development process in order to help project stakeholders explore ideas for possible features, and to discover, analyze, and specify requirements for the project. As prototypes are typically thrown-away following the initial analysis phase, it is imperative for them to be created quickly with little cost and effort. Tool support for finding and reusing components from open-source repositories offers a major opportunity to reduce this manual effort. In this paper, we present a system for rapid prototyping that facilitates software reuse by mining feature descriptions and source code from open-source repositories. Our system identifies and recommends features and associated source code modules that are relevant to the software product under development. The modules are selected such that they implement as many of the desired features as possible while exhibiting the lowest possible levels of external coupling. We conducted a user study to evaluate our approach and results indicated that it returned packages that implemented more features and were considered more relevant than the state-of-the-art approach.},
  Doi                      = {10.1109/ICSE.2012.6227134}
}

@InProceedings{2011:icse:mcveigh,
  Title                    = {Evolve: Tool support for architecture evolution},
  Author                   = {McVeigh, Andrew and Kramer, Jeff and Magee, Jeff},
  Booktitle                = icse,
  Year                     = {2011},
  Pages                    = {1040--1042},

  Abstract                 = {Incremental change is intrinsic to both the initial development and subsequent evolution of large complex software systems. Evolve is a graphical design tool that captures this incremental change in the definition of software architecture. It supports a principled and manageable way of dealing with unplanned change and extension. In addition, Evolve supports decentralized evolution in which software is extended and evolved by multiple independent developers. Evolve supports a model-driven approach in that architecture definition is used to directly construct both initial implementations and extensions to these implementations. The tool implements Backbone---an architectural description language (ADL), which has both a textual and a UML2, based graphical representation. The demonstration focuses on the graphical representation.}
}

@Article{2007:ist:medvidovic,
  Title                    = {Moving architectural description from under the technology lamppost},
  Author                   = {Medvidovic, Nenad and Dashofy, Eric M. and Taylor, Richard N.},
  Journal                  = ist,
  Year                     = {2007},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {12--31},
  Volume                   = {49},

  Abstract                 = {In 2000, we published an extensive study of existing software architecture description languages (ADLs), which has served as a useful reference to software architecture researchers and practitioners. Since then, circumstances have changed. The Unified Modeling Language (UML) has gained popularity and wide adoption, and many of the ADLs we studied have been pushed into obscurity. We argue that this progression can be attributed to early ADLs' nearly exclusive focus on technological aspects of architecture, ignoring application domain and business contexts within which software systems and development organizations exist. These three concerns---technology, domain, and business---constitute three ``lampposts'' needed to appropriately ``illuminate'' software architecture and architectural description.},
  Doi                      = {10.1016/j.infsof.2006.08.006}
}

@Article{2002:tosem:medvidovic,
  Title                    = {Modeling software architectures in the {U}nified {M}odeling {L}anguage},
  Author                   = {Medvidovic, Nenad and Rosenblum, David S. and Redmiles, David F. and Robbins, Jason E.},
  Journal                  = tosem,
  Year                     = {2002},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {2--57},
  Volume                   = {11},

  Abstract                 = {The Unified Modeling Language (UML) is a family of design notations that is rapidly becoming a de facto standard software design language. UML provides a variety of useful capabilities to the software designer, including multiple, interrelated design views, a semiformal semantics expressed as a UML meta model, and an associated language for expressing formal logic constraints on design elements. The primary goal of this work is an assessment of UML's expressive power for modeling software architectures in the manner in which a number of existing software architecture description languages (ADLs) model architectures. This paper presents two strategies for supporting architectural concerns within UML. One strategy involves using UML "as is," while the other incorporates useful features of existing ADLs as UML extensions. We discuss the applicability, strengths, and weaknesses of the two strategies. The strategies are applied on three ADLs that, as a whole, represent a broad cross-section of present-day ADL capabilities. One conclusion of our work is that UML currently lacks support for capturing and exploiting certain architectural concerns whose importance has been demonstrated through the research and practice of software architectures. In particular, UML lacks direct support for modeling and exploiting architectural styles, explicit software connectors, and local and global architectural constraints.},
  Doi                      = {10.1145/504087.504088}
}

@Article{2000:tse:medvidovic,
  Title                    = {A classification and comparison framework for software architecture description languages},
  Author                   = {Medvidovic, Nenad and Taylor, Richard N.},
  Journal                  = tse,
  Year                     = {2000},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {70--93},
  Volume                   = {26},

  Abstract                 = {Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us, in the process, to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future},
  Doi                      = {10.1109/32.825767}
}

@InProceedings{2005:ase:mehra,
  Title                    = {A generic approach to supporting diagram differencing and merging for collaborative design},
  Author                   = {Mehra, Akhil and Grundy, John and Hosking, John},
  Booktitle                = ase,
  Year                     = {2005},
  Pages                    = {204--213},

  Abstract                 = {Differentiation tools enable team members to compare two or more text files, e.g. code or documentation, after change. Although a number of general-purpose differentiation tools exist for comparing text documents very few tools exist for comparing diagrams. We describe a new approach for realising visual differentiation in CASE tools via a set of plug-in components. We have added diagram version control, visual differentiation and merging support as component-based plug-ins to the Pounamu meta-CASE tool. The approach is generic across a wide variety of diagram types and has also been deployed with an Eclipse diagramming plug-in. We describe our approach's architecture, key design and implementation issues, illustrate feasibility of our approach via implementation of it as plug-in components and evaluate its effectiveness.},
  Doi                      = {10.1145/1101908.1101940}
}

@InProceedings{2000:icse:mehta,
  Title                    = {Towards a taxonomy of software connectors},
  Author                   = {Mehta, Nikunj R. and Medvidovic, Nenad and Phadke, Sandeep},
  Booktitle                = icse,
  Year                     = {2000},
  Pages                    = {178--187},

  Abstract                 = {Software systems of today are frequently composed from prefabricated, heterogeneous components that provide complex functionality and engage in complex interactions. Existing research on component-based development has mostly focused on component structure, interfaces, and functionality. Recently, software architecture has emerged as an area that also places significant importance on component interactions, embodied in the notion of software connectors. However, the current level of understanding and support for connectors has been insufficient. This has resulted in their inconsistent treatment and a notable lack of understanding of what the fundamental building blocks of software interaction are and how they can be composed into more complex interactions. This paper attempts to address this problem. It presents a comprehensive classification framework and taxonomy of software connectors. The taxonomy is obtained through an extensive analysis of existing component interactions. The taxonomy is used both to understand existing software connectors and to suggest new, unprecedented connectors. We demonstrate the use of the taxonomy on the architecture of a large, existing system.},
  Doi                      = {10.1145/337180.337201}
}

@InProceedings{2010:csmr:mende,
  Title                    = {Effort-Aware Defect Prediction Models},
  Author                   = {Mende, Thilo and Koschke, Rainer},
  Booktitle                = csmr,
  Year                     = {2010},
  Pages                    = {107--116},

  Abstract                 = {Defect Prediction Models aim at identifying error-prone modules of a software system to guide quality assurance activities such as tests or code reviews. Such models have been actively researched for more than a decade, with more than 100 published research papers. However, most of the models proposed so far have assumed that the cost of applying quality assurance activities is the same for each module. In a recent paper, we have shown that this fact can be exploited by a trivial classifier ordering files just by their size: such a classifier performs surprisingly good, at least when effort is ignored during the evaluation. When effort is considered, many classifiers perform not significantly better than a random selection of modules. In this paper, we compare two different strategies to include treatment effort into the prediction process, and evaluate the predictive power of such models. Both models perform significantly better when the evaluation measure takes the effort into account.},
  Doi                      = {10.1109/CSMR.2010.18}
}

@Article{2009:jsmerp:mende,
  Title                    = {An evaluation of code similarity identification for the grow-and-prune model},
  Author                   = {Mende, Thilo and Koschke, Rainer and Beckwermert, Felix},
  Journal                  = jsmerp,
  Year                     = {2009},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {143--169},
  Volume                   = {21},

  Abstract                 = {In case new functionality is required, which is similar to the existing one, developers often copy the code that implements the existing functionality and adjust the copy to the new requirements. The result of the copying is code growth. If developers face maintenance problems, because of the need to make changes multiple times for the original and all its copies, they may decide to merge the original and its copies again; that is, they prune the code. This approach was named the grow-and-prune model by Faust and Verhoef. This paper describes tool support for the grow-and-prune model in the evolution of software by identifying similar functions that may be merged. These functions are identified in two steps. First, token-based clone detection is used to detect pairs of functions sharing code. Second, Levenshtein distance (LD) measures the textual similarity among these functions. Sufficient similarity at function level is then lifted to the architectural level. The approach is evaluated by a case study for the Linux kernel. We give examples of instances of the grow-and-prune model for Linux. Then, we evaluate our technique quantitatively by measuring recall and precision with respect to an oracle. To obtain the oracle, we asked nine different developers to decide whether they believe certain functions are similar and should be merged. The evaluation shows that the recall and precision of our technique are about 75\%. Calculating LD on token values rather than characters is superior. The two metrics strongly correlate but the token-based calculation reduces runtime by a factor of 4.6. Clone detection is an effective filter to reduce the number of calculations of the relatively expensive LD.},
  Doi                      = {10.1002/smr.v21:2}
}

@InProceedings{2011:esec_fse:meng,
  Title                    = {Sydit: creating and applying a program transformation from an example},
  Author                   = {Meng, Na and Kim, Miryung and McKinley, Kathryn S.},
  Booktitle                = esec_fse,
  Year                     = {2011},
  Pages                    = {440--443},

  Abstract                 = {Bug fixes and feature additions to large code bases often require systematic edits-similar, but not identical, coordinated changes to multiple places. This process is tedious and error-prone. Our prior work introduces a systematic editing approach that creates generalized edit scripts from exemplar edits and applies them to user-selected targets. This paper describes how the Sydit plug-in integrates our technology into the Eclipse integrated development environment. A programmer provides an example edit to Sydit that consists of an old and new version of a changed method. Based on this one example, Sydit generates a context-aware, abstract edit script. To make transformations applicable to similar but not identical methods, Sydit encodes control, data, and containment dependences and abstracts position, type, method, and variable names. Then the programmer selects target methods and Sydit customizes the edit script to each target and displays the results for the programmer to review and approve. Sydit thus automates much of the systematic editing process. To fully automate systematic editing, future tool enhancements should include automated selection of targets and testing of Sydit generated edits.},
  Doi                      = {10.1145/2025113.2025185}
}

@InProceedings{2011:pldi:meng,
  Title                    = {Systematic editing: Generating program transformations from an example},
  Author                   = {Meng, Na and Kim, Miryung and {McKinley}, Kathryn S.},
  Booktitle                = pldi,
  Year                     = {2011},
  Pages                    = {329--342},

  Abstract                 = {Software modifications are often systematic---they consist of similar, but not identical, program changes to multiple contexts. Existing tools for systematic program transformation are limited because they require programmers to manually prescribe edits or only suggest a location to edit with a related example. This paper presents the design and implementation of a program transformation tool called SYDIT. Given an example edit, SYDIT generates a context-aware, abstract edit script, and then applies the edit script to new program locations. To correctly encode a relative position of the edits in a new location, the derived edit script includes unchanged statements on which the edits are control and data dependent. Furthermore, to make the edit script applicable to a new context using different identifier names, the derived edit script abstracts variable, method, and type names. The evaluation uses 56 systematic edit pairs from five large software projects as an oracle. SYDIT has high coverage and accuracy. For 82\% of the edits (46/56), SYDIT matches the context and applies an edit, producing code that is 96\% similar to the oracle. Overall, SYDIT mimics human programmers correctly on 70\% (39/56) of the edits. Generation of edit scripts seeks to improve programmer productivity by relieving developers from tedious, error-prone, manual code updates. It also has the potential to guide automated program repair by creating program transformations applicable to similar contexts.}
}

@InProceedings{2012:icse:meng,
  Title                    = {A history-based matching approach to identification of framework evolution},
  Author                   = {Sichen Meng and Xiaoyin Wang and Lu Zheng and Hong Mei},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {353--363},

  Abstract                 = {In practice, it is common that a framework and its client programs evolve simultaneously. Thus, developers of client programs may need to migrate their programs to the new release of the framework when the framework evolves. As framework developers can hardly always guarantee backward compatibility during the evolution of a framework, migration of its client program is often time-consuming and error-prone. To facilitate this migration, researchers have proposed two categories of approaches to identification of framework evolution: operation-based approaches and matching-based approaches. To overcome the main limitations of the two categories of approaches, we propose a novel approach named HiMa, which is based on matching each pair of consecutive revisions recorded in the evolution history of the framework and aggregating revision-level rules to obtain framework-evolution rules. We implemented our HiMa approach as an Eclipse plug-in targeting at frameworks written in Java using SVN as the version-control system. We further performed an experimental study on HiMa together with a state-of-art approach named AURA using six tasks based on three subject Java frameworks. Our experimental results demonstrate that HiMa achieves higher precision and higher recall than AURA in most circumstances and is never inferior to AURA in terms of precision and recall in any circumstances, although HiMa is computationally more costly than AURA.},
  Doi                      = {10.1109/ICSE.2012.6227179}
}

@Article{2002:tse:mens,
  Title                    = {A State-of-the-Art Survey on Software Merging},
  Author                   = {Mens, Tom},
  Journal                  = tse,
  Year                     = {2002},

  Month                    = may,
  Number                   = {5},
  Pages                    = {449--462},
  Volume                   = {28},

  Abstract                 = {Software merging is an essential aspect of the maintenance and evolution of large-scale software systems. This paper provides a comprehensive survey and analysis of available merge approaches. Over the years, a wide variety of different merge techniques has been proposed. While initial techniques were purely based on textual merging, more powerful approaches also take the syntax and semantics of the software into account. There is a tendency towards operation-based merging because of its increased expressiveness. Another tendency is to try to define merge techniques that are as general, accurate, scalable, and customizable as possible, so that they can be used in any phase in the software life-cycle and detect as many conflicts as possible. After comparing the possible merge techniques, we suggest a number of important open problems and future research directions.},
  Doi                      = {10.1109/TSE.2002.1000449}
}

@InProceedings{2001:ffse:mens,
  Title                    = {Transformational software evolution by assertions},
  Author                   = {Mens, Tom},
  Booktitle                = ffse,
  Year                     = {2001},
  Note                     = {8~pages},

  Abstract                 = {This paper explores the use of software transformations as a formal foundation for software evolution. More precisely, we express software transformations in terms of assertions (preconditions, postconditions and invariants) on top of the formalism of graph rewriting. This allows us to tackle scalability issues in a straightforward way. Useful applications include: detecting syntactic merge conflicts, removing redundancy in a transformation sequence, factoring out common subsequences, etc.}
}

@InProceedings{2008:icsm:mens,
  Title                    = {The evolution of {E}clipse},
  Author                   = {Mens, Tom and Fern{\'a}ndez-Ramil, Juan and Degrandsart, Sylvain},
  Booktitle                = icsm,
  Year                     = {2008},
  Pages                    = {386--395},

  Abstract                 = {We present a metrics-based study of the evolution of Eclipse, an open source integrated development environment, based on data from seven major releases, from releases 1.0 to 3.3. We investigated whether three of the laws of software evolution were supported by the data. We found that Eclipse displayed continual change and growth, hence supporting laws 1 and 6. Six size indicators, out of eight, closely followed trend models. Four were linear and two superlinear. We found evidence of increasing complexity (law 2) in only two indicators, out of five. At subproject level, size and complexity are not distributed uniformly, and subproject size can be modelled as a negative exponential function of the rank position. We encountered a range of different size and complexity trends across subprojects. Our approach and results can help in evaluating the future evolution of Eclipse, the evolution of other systems and in performing comparisons.},
  Doi                      = {10.1109/ICSM.2008.4658087}
}

@InProceedings{2006:lmo:mens,
  Title                    = {Transformation dependency analysis: A comparison of two approaches},
  Author                   = {Mens, T. and Kniesel, G. and Runge, O.},
  Booktitle                = lmo,
  Year                     = {2006},
  Pages                    = {167--182},

  Abstract                 = {Transformation dependency analysis is crucial to provide better tool support for current-day software development techniques---two prominent examples are program refactoring and model transformation. Unfortunately, it is unclear how existing tools that provide generic support for these techniques relate to each other, due to their difference in terminology, concepts and formal foundations (graphs versus logic). This article reports on the results of an experimental comparison between two tools: AGG and Condor. Among others, we noticed a performance advantage of several orders of magnitude for the logic-based approach.}
}

@Article{2002:entcs:mens,
  Title                    = {A graph-based metamodel for object-oriented software metrics},
  Author                   = {Tom Mens and Michele Lanza},
  Journal                  = entcs,
  Year                     = {2002},

  Month                    = nov,
  Number                   = {2},
  Pages                    = {57--68},
  Volume                   = {72},

  Abstract                 = {Metrics are essential in object-oriented software engineering for several reasons, among which quality assessment and improvement of development team productivity. While the mathematical nature of metrics calls for clear definitions, frequently there exist many contradicting definitions of the same metric depending on the implementation language. We suggest to express and define metrics using a language-independent metamodel based on graphs. This graph-based approach allows for an unambiguous definition of generic object-oriented metrics and higher-order metrics. We also report on some prototype tools that implement these ideas.},
  Doi                      = {10.1016/S1571-0661(05)80529-8}
}

@Article{2004:tse:mens,
  Title                    = {A survey of software refactoring},
  Author                   = {Mens, Tom and Tourw{\'e}, Tom},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {126--139},
  Volume                   = {30},

  Abstract                 = {We provide an extensive overview of existing research in the field of software refactoring. This research is compared and discussed based on a number of different criteria: the refactoring activities that are supported, the specific techniques and formalisms that are used for supporting these activities, the types of software artifacts that are being refactored, the important issues that need to be taken into account when building refactoring tool support, and the effect of refactoring on the software process. A running example is used to explain and illustrate the main concepts.},
  Doi                      = {10.1109/TSE.2004.1265817}
}

@InProceedings{2008:icsm:menzies,
  Title                    = {Automated Severity Assessment of Software Defect Reports},
  Author                   = {Menzies, Tim and Marcus, Andrian},
  Booktitle                = icsm,
  Year                     = {2008},
  Pages                    = {346--355},

  Abstract                 = {In mission critical systems, such as those developed by NASA, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue. The paper presents a new and automated method named SEVERIS (severity issue assessment), which assists the test engineer in assigning severity levels to defect reports. SEVERIS is based on standard text mining and machine learning techniques applied to existing sets of defect reports. A case study on using SEVERIS with data from NASApsilas Project and Issue Tracking System (PITS) is presented in the paper. The case study results indicate that SEVERIS is a good predictor for issue severity levels, while it is easy to use and efficient.},
  Doi                      = {10.1109/ICSM.2008.4658083}
}

@InProceedings{1994:icsr:merkl,
  Title                    = {Learning the semantic similarity of reusable software components},
  Author                   = {Merkl, D. and Tjoa, A. M. and Kappel, G.},
  Booktitle                = icsr,
  Year                     = {1994},
  Pages                    = {33--41},

  Abstract                 = {Properly structured software libraries are crucial for the success of software reuse. Specifically, the structure of the software library ought to reflect the functional similarity of the stored software components in order to facilitate the retrieval process. We propose the application of artificial neural network technology to achieve such a structured library. In more detail, we utilize an artificial neural network adhering to the unsupervised learning paradigm. The distinctive feature of this very model is to make the semantic relationship between the stored software components geographically explicit. Thus, the actual user of the software library gets a notion of the semantic relationship between the components in terms of their geographical closeness},
  Doi                      = {10.1109/ICSR.1994.365813}
}

@InProceedings{2010:oopsla:merkle,
  Title                    = {Stop the software architecture erosion},
  Author                   = {Merkle, Bernhard},
  Booktitle                = oopslacomp,
  Year                     = {2010},
  Pages                    = {295--297},

  Abstract                 = {During the evolution of a software system, it becomes more and more difficult to understand the originally planned software architecture. Often an architectural degeneration happens because of various reasons during the development phases. In this tutorial we will be looking how to avoid such architectural decay and degeneration and how continuous monitoring can improve the situation and avoid architectural violations. In addition we will look at ``refactoring in the large" and how refactoring can be simulated. We will also look at some popular open source projects like ant, findbugs and eclipse (CDT/JDT) and see if and how far architectural an erosion happens/ed there. The participants will analyze a system and search with tool support for erosion areas and subsequently simulate virtual refactoring for improving the software. At the end participants will improve the system and have a good feeling how far an automated and tool supported approach can lead to better results.},
  Doi                      = {10.1145/1869542.1869621}
}

@InProceedings{2002:compsac:merlo,
  Title                    = {Investigating large software system evolution: The {L}inux kernel},
  Author                   = {Merlo, E. and Dagenais, M. and Bachand, P. and Sormani, J. S. and Gradara, S. and Antoniol, G.},
  Booktitle                = compsac,
  Year                     = {2002},
  Pages                    = {421--426},

  Abstract                 = {Large multi-platform, multi-million lines of codes software systems evolve to cope with new platform or to meet user ever changing needs. While there has been several studies focused on the similarity of code fragments or modules, few studies addressed the need to monitor the overall system evolution. Meanwhile, the decision to evolve or to refactor a large software system needs to be supported by high level information, representing the system overall picture, abstracting from unnecessary details.This paper proposes to extend the concept of similarity of code fragments to quantify similarities at the release/system level. Similarities are captured by four software metricsrepresentative of the commonalities and differences within and among software artifacts.To show the feasibility of characterizing large software system with the new metrics, 365 releases of the Linux kernel were analyzed. The metrics, the experimental results as well as the lessons learned are presented in the paper.}
}

@Book{2000:book:meyer,
  Title                    = {Object-Oriented Software Construction},
  Author                   = {Bertrand Meyer},
  Publisher                = {Prentice Hall},
  Year                     = {2000},
  Edition                  = {2nd}
}

@Article{1999:computer:meyer,
  Title                    = {On to components},
  Author                   = {Bertrand Meyer},
  Journal                  = computer,
  Year                     = {1999},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {139--140},
  Volume                   = {32},

  Abstract                 = {With this issue, the Object Technology department becomes Component and Object Technology. Although component technology has figured prominently in earlier columns, we felt we had to go further. Excitement about components runs high in the computing community. In a recent survey of the Computer readership, components placed at the very top of the topics of interest. Note, by the way, that Computer will devote its July issue's theme to component-based development.},
  Doi                      = {10.1109/2.738312}
}

@Article{2008:tosem:meyers,
  Title                    = {An empirical study of slice-based cohesion and coupling metrics},
  Author                   = {Timothy M. Meyers and David Binkley},
  Journal                  = tosem,
  Year                     = {2008},

  Month                    = dec,
  Number                   = {1},
  Pages                    = {2:1--2:27},
  Volume                   = {17},

  Abstract                 = {Software reengineering is a costly endeavor, due in part to the ambiguity of where to focus reengineering effort. Coupling and Cohesion metrics, particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure progress. The most extensive work on such metrics is with slice-based cohesion metrics. While their use of semantic dependence information should make them an excellent choice for cohesion measurement, their wide spread use has been impeded in part by a lack of empirical study. Recent advances in software tools make, for the first time, a large-scale empirical study of slice-based cohesion and coupling metrics possible. Four results from such a study are presented. First, ``head-to-head" qualitative and quantitative comparisons of the metrics identify which metrics provide similar views of a program and which provide unique views of a program. This study includes statistical analysis showing that slice-based metrics are not proxies for simple size-based metrics such as lines of code. Second, two longitudinal studies show that slice-based metrics quantify the deterioration of a program as it ages. This serves to validate the metrics: the metrics quantify the degradation that exists during development; turning this around, the metrics can be used to measure the progress of a reengineering effort. Third, baseline values for slice-based metrics are provided. These values act as targets for reengineering efforts with modules having values outside the expected range being the most in need of attention. Finally, slice-based coupling is correlated and compared with slice-based cohesion.},
  Doi                      = {10.1145/1314493.1314495}
}

@InProceedings{2002:oopsla:mezini,
  Title                    = {Integrating independent components with on-demand remodularization},
  Author                   = {Mira Mezini and Klaus Ostermann},
  Booktitle                = oopsla,
  Year                     = {2002},
  Pages                    = {52--67},

  Abstract                 = {This paper proposes language concepts that facilitate the separation of an application into independent reusable building blocks and the integration of pre-build generic software components into applications that have been developed by third party vendors. A key element of our approach are on-demand remodularizations, meaning that the abstractions and vocabulary of an existing code base are translated into the vocabulary understood by a set of components that are connected by a common collaboration interface. This general concept allows us to mix-and-match remodularizations and components on demand.}
}

@InProceedings{2001:metrics:misic,
  Title                    = {Cohesion is structural, coherence is functional: Different views, different measures},
  Author                   = {Vojislav B. Mi{\v{s}}i{\'c}},
  Booktitle                = metrics,
  Year                     = {2001},
  Pages                    = {135--144},

  Abstract                 = {Traditionally, the cohesion of a software component is considered to be a characteristic of its internal structure, and most cohesion measures proposed so far measure cohesion through the similarity of its constituent parts. However, cohesion may also be interpreted as an externally observed functional property, without regard for the component's internal structure. One way of measuring functional cohesion would be to measure the similarity of usage patterns of a component's external clients. One such measure is defined in this paper using a generic system model and its associated mechanism for calculating object sizes as the foundation. The new measure is simple to understand, easy to automate, and flexible enough to be used at different levels of abstraction. Moreover, it satisfies the most important properties that a cohesion measure is expected to satisfy. Examples are provided to illustrate the concept and its possible uses in analyzing and re-packaging of the components of a software system.}
}

@InProceedings{2001:icse:michail,
  Title                    = {{C}ode {W}eb: Data mining library reuse patterns},
  Author                   = {Amir Michail},
  Booktitle                = icse,
  Year                     = {2001},
  Pages                    = {827--828},

  Abstract                 = {Developers learn to use a software library not just from its documentation but also from toy examples and existing real-life application code (e.g., by using grep). The CodeWeb tool takes this simple idea further by a deeper analysis of a large collection of applications to see what characteristic usage of the library is like. We demonstrate the tool by showing how the KDE core libraries are used in real-life KDE applications. Moreover, we look at a recently developed feature that helps software developers port an application from an old version of a library to a new one.},
  Doi                      = {10.1109/ICSE.2001.919192}
}

@InProceedings{2000:icse:michail,
  Title                    = {Data mining Library Reuse Patterns Using Generalized Association Rules},
  Author                   = {Amir Michail},
  Booktitle                = icse,
  Year                     = {2000},
  Pages                    = {167--176},

  Abstract                 = {It is shown how data mining can be used to discover library reuse patterns in existing applications. Specifically, we consider the problem of discovering library classes and member functions that are typically reused in combination by application classes. The paper improves upon earlier research using ``association rules" (A. Michail, 1999) by taking into account the inheritance hierarchy using ``generalized association rules". This turns out to be a non-trivial but worthwhile endeavor. By browsing generalized association rules, a developer can discover patterns in library usage in a way that takes into account inheritance relationships. For example, such a rule might tell us that application classes that inherit from a particular library class often instantiate another class or one of its descendents. We illustrate the approach using our tool, CodeWeb, by demonstrating characteristic ways in which applications reuse classes in the KDE application framework.},
  Doi                      = {10.1109/ICSE.2000.870408}
}

@InProceedings{1999:icse:michail,
  Title                    = {Assessing Software Libraries by Browsing Similar Classes, Functions and Relationships},
  Author                   = {Amir Michail and David Notkin},
  Booktitle                = icse,
  Year                     = {1999},
  Pages                    = {463--472},

  Abstract                 = {Comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another. The current state of the art in assessing libraries relies on qualitative methods. To reduce costs and/or assess a large collection of libraries, automation is necessary. Although there are tools that help a developer examine an individual library in terms of architecture, style, etc., we know of no tools that help the developer directly compare several libraries. With existing tools, the user must manually integrate the knowledge learned about each library. Automation to help developers directly compare and contrast libraries requires matching of similar components (such as classes and functions) across libraries. This is different than the traditional component retrieval problem in which components are returned that best match a user's query. Rather, we need to find those components that are similar across the libraries under consideration. In this paper, we show how this kind of matching can be done.},
  Doi                      = {10.1145/302405.302678}
}

@InProceedings{2001:iwpse:mikkonen,
  Title                    = {Practical perspectives on software architectures, high-level design, and evolution},
  Author                   = {Tommi Mikkonen and Peeter Pruuden},
  Booktitle                = iwpse,
  Year                     = {2001},
  Pages                    = {122--125},

  Abstract                 = {Evolving software gets more complex in each increment. As real-life increments tend to be additive rather than upgrades with more fundamental purpose, the underlying code base keeps extending. With such increments, the associated core architecture of the system gets more and more difficult to modify, because an increasing number of functions are attached to it. Therefore, only the first versions of systems can be properly architected, whereas later versions rely on an already existing architecture into which new features are reflected as change requests. Due to this reason, architecture and high-level design in the scope of evolved software systems becomes primarily impact analysis and reengineering, and only secondarily design of new software artifacts, especially when considering large systems that have a long history. Every now and then, however, it may be possible to upgrade very core structures of a system, resulting in a new generation of the system. Understanding of these basic phenomena gives rise to practices that compensate the increase of complexity of software during software evolution.}
}

@Book{1994:book:miles,
  Title                    = {Qualitative Data Analysis: An Expanded Sourcebook},
  Author                   = {Matthew B. Miles and Michael Huberman},
  Publisher                = {Sage Publications},
  Year                     = {1994},
  Edition                  = {2nd}
}

@InProceedings{1994:icse:mili,
  Title                    = {Storing and retrieving software components: A refinement system},
  Author                   = {Mili, A. and Mili, R. and Mittermeir, R.},
  Booktitle                = icse,
  Year                     = {1994},
  Pages                    = {91--100},

  Abstract                 = {Software reuse poses a number of challenges, ranging from managerial to technical - not least of these is the problem of storing and retrieving software components in a time efficient manner. This paper presents the design and implementation of an automated software repository, where software components can be automatically stored and retrieved. This repository is based on a formal representation of programs and their specifications, as well as a refinement ordering of these specifications.},
  Doi                      = {10.1109/ICSE.1994.296769}
}

@Article{1998:annse:mili,
  Title                    = {A survey of software reuse libraries},
  Author                   = {Mili, A. and Mili, R. and Mittermeir, R. T.},
  Journal                  = annse,
  Year                     = {1998},
  Number                   = {1},
  Pages                    = {349--414},
  Volume                   = {5},

  Abstract                 = {The study of storage and retrieval methods of software assets in software libraries gives rise to a number of paradoxes: While this subject has been under investigation for nearly two decades, it still remains an active area of research in software reuse and software engineering; this can be explained by the observation that new technologies (such as the internet, the world wide web, object-oriented programming) keep opening new opportunities for better asset packaging, better library organizations, and larger scale libraries---thereby posing new technical challenges. Also, while many sophisticated solutions have been proposed to this problem, the state of the practice in software reuse is characterized by the use of ad-hoc, low-tech methods; this can be explained by the observation that most existing solutions are either too ineffective to be useful or too intractable to be usable. Finally, while it is difficult to imagine a successful software reuse program without a sophisticated, well-tuned, systematic procedure for software component storage and retrieval, it seems many successful software reuse experiments rely on trivial methods of component storage and retrieval; this can be explained by the observation that, in the current state of the practice, software libraries are not the bottleneck of the software reuse process. This paper presents a survey of methods of storage and retrieval of software assets in software libraries. In addition to a review of existing research efforts, the paper makes two contributions. First, a definition of (presumably) orthogonal attributes of storage and retrieval methods; these attributes are used, in turn, to classify existing methods into six broad classes. Second, a definition of (presumably) orthogonal assessment criteria, which include technical, managerial and human factors; these criteria afford us an exhaustive and uniform basis for assessing and comparing individual methods and classes of methods.},
  Doi                      = {10.1023/A:1018964121953}
}

@Article{1997:tse:mili,
  Title                    = {Storing and retrieving software components: A refinement based system},
  Author                   = {Mili, R. and Mili, A. and Mittermeir, R.},
  Journal                  = tse,
  Year                     = {1997},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {445--460},
  Volume                   = {23},

  Abstract                 = {Software libraries are repositories which contain software components; as such, they represent a precious resource for the software engineer. As software libraries grow in size, it becomes increasingly difficult to maintain adequate precision and recall with informal retrieval algorithms. In this paper, we discuss the design and implementation of a storage and retrieval structure for software components that is based on formal specifications and on the refinement ordering between specifications.},
  Doi                      = {10.1109/32.605762}
}

@Article{1985:spe:miller,
  Title                    = {A file comparison program},
  Author                   = {Webb Miller and Eugene W. Myers},
  Journal                  = spe,
  Year                     = {1985},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {1025--1040},
  Volume                   = {15},

  Abstract                 = {This paper presents a simple method for computing a shortest sequence of insertion and deletion commands that converts one given file to another. The method is particularly efficient when the difference between the two files is small compared to the files' lengths. In experiments performed on typical files, the program often ran four times faster than the UNIX diff command.},
  Doi                      = {10.1002/spe.4380151102}
}

@Article{2012:infosci:miranskyy,
  Title                    = {Using entropy measures for comparison of software traces},
  Author                   = {A. V. Miranskyy and M. Davison and R. M. Reesor and S. S. Murtaza},
  Journal                  = infosci,
  Year                     = {2012},

  Month                    = {25 } # oct,
  Pages                    = {59--72},
  Volume                   = {203},

  Abstract                 = {The analysis of execution paths (also known as software traces) collected from a given software product can help in a number of areas including software testing, software maintenance and program comprehension. The lack of a scalable matching algorithm operating on detailed execution paths motivates the search for an alternative solution. This paper proposes the use of word entropies for the classification of software traces. Using a well-studied defective software as an example, we investigate the application of both Shannon and extended entropies (Landsberg-Vedral, R{\'e}nyi and Tsallis) to the classification of traces related to various software defects. Our study shows that using entropy measures for comparisons gives an efficient and scalable method for comparing traces. The three extended entropies, with parameters chosen to emphasize rare events, all perform similarly and are superior to the Shannon entropy.},
  Doi                      = {10.1016/j.ins.2012.03.017}
}

@Article{2008:softcomp:mitchell,
  Title                    = {On the evaluation of the {B}unch search-based software modularization algorithm},
  Author                   = {Mitchell, Brian and Mancoridis, Spiros},
  Journal                  = softcomp,
  Year                     = {2008},

  Month                    = aug,
  Number                   = {1},
  Pages                    = {77--93},
  Volume                   = {12},

  Abstract                 = {Abstract The first part of this paper describes an automatic reverse engineering process to infer subsystem abstractions that are useful for a variety of software maintenance activities. This process is based on clustering the graph representing the modules and module-level dependencies found in the source code into abstract structures not in the source code called subsystems. The clustering process uses evolutionary algorithms to search through the enormous set of possible graph partitions, and is guided by a fitness function designed to measure the quality of individual graph partitions. The second part of this paper focuses on evaluating the results produced by our clustering technique. Our previous research has shown through both qualitative and quantitative studies that our clustering technique produces good results quickly and consistently. In this part of the paper we study the underlying structure of the search space of several open source systems. We also report on some interesting findings our analysis uncovered by comparing random graphs to graphs representing real software systems.},
  Doi                      = {10.1007/s00500-007-0218-3}
}

@InProceedings{2003:icsm:mitchell,
  Title                    = {A heuristic approach to solving the software clustering problem},
  Author                   = {Mitchell, Brian S.},
  Booktitle                = icsm,
  Year                     = {2003},
  Pages                    = {285--288},

  Abstract                 = {This paper provides an overview of the author's Ph.D. thesis (2002). The primary contribution of this research involved developing techniques to extract architectural information about a system directly from its source code. To accomplish this objective a series of software clustering algorithms were developed. These algorithms use metaheuristic search techniques to partition a directed graph generated from the entities and relations in the source code into subsystems. Determining the optimal solution to this problem was shown to be NP-hard, thus significant emphasis was placed on finding solutions that were regarded as ``good enough" quickly. Several evaluation techniques were developed to gauge solution quality, and all of the software clustering tools created to support this work was made available for download over the Internet.},
  Doi                      = {10.1109/ICSM.2003.1235432}
}

@Article{2006:tse:mitchell,
  Title                    = {On the automatic modularization of software systems using the {B}unch tool},
  Author                   = {Mitchell, Brian S. and Mancoridis, S.},
  Journal                  = tse,
  Year                     = {2006},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {193--208},
  Volume                   = {32},

  Abstract                 = {Since modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering techniques are useful to support the creation of these abstractions by producing architectural-level views of a system's structure directly from its source code. This paper examines the Bunch clustering system which, unlike other software clustering tools, uses search techniques to perform clustering. Bunch produces a subsystem decomposition by partitioning a graph of the entities (e.g., classes) and relations (e.g., function calls) in the source code. Bunch uses a fitness function to evaluate the quality of graph partitions and uses search algorithms to find a satisfactory solution. This paper presents a case study to demonstrate how Bunch can be used to create views of the structure of significant software systems. This paper also outlines research to evaluate the software clustering results produced by Bunch.},
  Doi                      = {10.1109/TSE.2006.31}
}

@InProceedings{2001:icsm:mitchell,
  Title                    = {Comparing the decompositions produced by software clustering algorithms using similarity measurements},
  Author                   = {Mitchell, Brian S. and Mancoridis, Spiros},
  Booktitle                = icsm,
  Year                     = {2001},
  Pages                    = {744--753},

  Abstract                 = {Decomposing source code components and relations into subsystem clusters is an active area of research. Numerous clustering approaches have been proposed in the reverse engineering literature, each one using a different algorithm to identify subsystems. Since different clustering techniques may not produce identical results when applied to the same system, mechanisms that can measure the extent of these differences are needed. Some work to measure the similarity between decompositions has been done, but this work considers the assignment of source code components to clusters as the only criterion for similarity. We argue that better similarity measurements can be designed if the relations between the components are considered. The authors propose two similarity measurements that overcome certain problems in existing measurements. We also provide some suggestions on how to identify and deal with source code components that tend to contribute to poor similarity results. We conclude by presenting experimental results, and by highlighting some of the benefits of our similarity measurements.}
}

@Book{1997:book:mitchell,
  Title                    = {Machine Learning},
  Author                   = {Mitchell, Thomas M.},
  Publisher                = {McGraw-Hill},
  Year                     = {1997}
}

@Article{1982:ai:mitchell,
  Title                    = {Generalization as search},
  Author                   = {Tom M. Mitchell},
  Journal                  = ai,
  Year                     = {1982},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {203--226},
  Volume                   = {18},

  Abstract                 = {The problem of concept learning, or forming a general description of a class of objects given a set of examples and non-examples, is viewed here as a search problem. Existing programs that generalize from examples are characterized in terms of the classes of search strategies that they employ. Several classes of search strategies are then analyzed and compared in terms of their relative capabilities and computational complexities.},
  Doi                      = {10.1016/0004-3702(82)90040-6}
}

@Article{2004:netmath:mitzenmarcher,
  Title                    = {A brief history of generative models for power law and lognormal distribution},
  Author                   = {Mitzenmarcher, Michael},
  Journal                  = netmath,
  Year                     = {2004},
  Number                   = {2},
  Pages                    = {226--251},
  Volume                   = {1},

  Abstract                 = {Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a lognormal distribution. In trying to learn enough about these distributions to settle the question, I found a rich and long history, spanning many fields. Indeed, several recently proposed models from the computer science community have antecedents in work from decades ago. Here, I briefly survey some of this history, focusing on underlying generative models that lead to these distributions. One finding is that lognormal and power law distributions connect quite naturally, and hence, it is not surprising that lognormal distributions have arisen as a possible alternative to power law distributions across many fields.},
  Url                      = {http://projecteuclid.org/euclid.im/1089229510}
}

@InCollection{2008:book:shull:mockus,
  Title                    = {Missing data in software engineering},
  Author                   = {Mockus, Audris},
  Booktitle                = {Guide to Advanced Empirical Software Engineering},
  Publisher                = {Springer},
  Year                     = {2008},
  Chapter                  = {7},
  Editor                   = {Forrest Shull and Janice Singer and Dag I. K. Sj{\o}berg},
  Pages                    = {185--200},

  Abstract                 = {The collection of valid software engineering data involves substantial effort and is not a priority in most software production environments. This often leads to missing or otherwise invalid data. This fact tends to be overlooked by most software engineering researchers and may lead to a biased analysis. This chapter reviews missing data methods and applies them on a software engineering data set to illustrate a variety of practical contexts where such techniques are needed and to highlight the pitfalls of ignoring the missing data problem.},
  Doi                      = {10.1007/978-1-84800-044-5_7}
}

@Article{2002:tosem:mockus,
  Title                    = {Two case studies of open source software development: {A}pache and {M}ozilla},
  Author                   = {Audris Mockus and Roy T. Fielding and James D. Herbsleb},
  Journal                  = tosem,
  Year                     = {2002},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {309--346},
  Volume                   = {11},

  Abstract                 = {According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.},
  Doi                      = {10.1145/567793.567795}
}

@InProceedings{2009:esem:mockus,
  Title                    = {Test coverage and post-verification defects: A multiple case study},
  Author                   = {Mockus, Audris and Nagappan, Nachiappan and Dinh-Trong, Trung T.},
  Booktitle                = esem,
  Year                     = {2009},
  Pages                    = {291--301},

  Abstract                 = {Test coverage is a promising measure of test effectiveness and development organizations are interested in cost-effective levels of coverage that provide sufficient fault removal with contained testing effort. We have conducted a multiple-case study on two dissimilar industrial software projects to investigate if test coverage reflects test effectiveness and to find the relationship between test effort and the level of test coverage. We find that in both projects the increase in test coverage is associated with decrease in field reported problems when adjusted for the number of prerelease changes. A qualitative investigation revealed several potential explanations, including code complexity, developer experience, the type of functionality, and remote development teams. All these factors were related to the level of coverage and quality, with coverage having an effect even after these adjustments. We also find that the test effort increases exponentially with test coverage, but the reduction in field problems increases linearly with test coverage. This suggests that for most projects the optimal levels of coverage are likely to be well short of 100\%.},
  Doi                      = {10.1109/ESEM.2009.5315981}
}

@InProceedings{2004:icse:mohagheghi,
  Title                    = {An Empirical Study of Software Reuse vs. Defect-Density and Stability},
  Author                   = {Mohagheghi, Parastoo and Conradi, Reidar and Killi, Ole M. and Schwarz, Henrik},
  Booktitle                = icse,
  Year                     = {2004},
  Pages                    = {282--292},

  Abstract                 = {The paper describes results of an empirical study, where some hypotheses about the impact of reuse on defect-density and stability, and about the impact of component size on defects and defect-density in the context of reuse are assessed, using historical data (``data mining") on defects, modification rate, and software size of a large-scale telecom system developed by Ericsson. The analysis showed that reused components have lower defect-density than non-reused ones. Reused components have more defects with highest severity than the total distribution, but less defects after delivery, which shows that that these are given higher priority to fix. There are an increasing number of defects with component size for non-reused components, but not for reused components. Reused components were less modified (more stable) than non-reused ones between successive releases, even if reused components must incorporate evolving requirements from several application products. The study furthermore revealed inconsistencies and weaknesses in the existing defect reporting system, by analyzing data that was hardly treated systematically before.},
  Doi                      = {10.1109/ICSE.2004.1317450}
}

@InProceedings{2005:wcre:moise,
  Title                    = {Extracting and Representing Cross-Language Dependencies in Diverse Software Systems},
  Author                   = {D. L. Moise and K. Wong},
  Booktitle                = wcre,
  Year                     = {2005},
  Pages                    = {209--218},

  Abstract                 = {This paper presents an approach for dealing with multi-language software systems. Much of the focus of reverse engineering tools is in analyzing software systems written in one programming language. Nowadays, the abundance of new technologies and languages used to ease application development raises new challenges for reverse engineers. Therefore, this paper focuses on finding cross-language dependencies in such diverse, heterogeneous software systems. Our approach uses source navigator extractors to produce the facts inside each language. Then, we show an example for finding Java Native Interface (JNI) dependencies between facts from Java and C/C++ code. The integrated facts are produced in GXL form, and conform to a unified schema introduced in the paper. This approach is useful from several perspectives. It illustrates how to retrieve the dependencies from software systems written in more than one programming language. Also, the generated facts conform to the GXL format, which is accepted by many reverse engineering tools. The usefulness and scalability of the approach are tested in a case study.}
}

@InProceedings{2002:iwpc:moonen,
  Title                    = {Lightweight Impact Analysis using Island Grammars},
  Author                   = {Leon Moonen},
  Booktitle                = iwpc,
  Year                     = {2002},
  Pages                    = {219--228},

  Abstract                 = {Impact analysis is needed for the planning and estimation of software maintenance projects. Traditional impact analysis techniques tend to be too expensive for this phase, so there is need for more lightweight approaches.In this paper, we present a technique for the generation of lightweight impact analyzers from island grammars. We demonstrate this technique using a real-world case study in which we describe how island grammars can be used to find account numbers in the software portfolio of a large bank. We show how we have implemented this analysis and achieved lightweightness using a reusable generative framework for impact analyzers.}
}

@InProceedings{2001:wcre:moonen,
  Title                    = {Generating Robust Parsers using Island Grammars},
  Author                   = {Leon Moonen},
  Booktitle                = wcre,
  Year                     = {2001},
  Pages                    = {13--24},

  Abstract                 = {Source model extraction---the automated extraction of information from system artifacts---is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts that are typical for the reverse engineering domain (for example, syntactic errors, incomplete source code, language dialects and embedded languages).This paper proposes a solution in the form of island grammars, a special kind of grammars that combine the detailed specification possibilities of grammars with the liberal behavior of lexical approaches. We show how island grammars can be used to generate robust parsers that combine the accuracy of syntactical analysis with the speed, flexibility and tolerance usually only found in lexical analysis. We conclude with a discussion of the development of Mangrove, a generator for source model extractors based on island grammars and describe its application to a number of case studies.}
}

@Article{1997:tosem:moormann_zaremski,
  Title                    = {Specification matching of software components},
  Author                   = {Moormann Zaremski, Amy and Wing, Jeannette M.},
  Journal                  = tosem,
  Year                     = {1997},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {333--369},
  Volume                   = {6},

  Abstract                 = {Specification matching is a way to compare two software components, based on descriptions of the component's behaviors. In the context of software reuse and library retrieval, it can help determine whether one component can be substituted for another or how one can be modified to fit the requirements of the other. In the context of object-oriented programming, it can help determine when one type is a behavioral subtype of another. We use formal specifications to describe the behavior of software components and, hence, to determine whether two components match. We give precise definitions of not just exact match, but, more relevantly, various flavors of relaxed match. These definitions capture the notions of generalization, specialization, and substitutability of software components. Since our formal specifications are pre- and postconditions written as predicates in first-order logic, we rely on theorem proving to determine match and mismatch. We give examples from our implementation of specification matching using the Larch Prover.},
  Doi                      = {10.1145/261640.261641}
}

@Article{1995:tosem:moormann_zaremski,
  Title                    = {Signature matching: A tool for using software libraries},
  Author                   = {Moormann Zaremski, Amy and Wing, Jeannette M.},
  Journal                  = tosem,
  Year                     = {1995},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {146--170},
  Volume                   = {4},

  Abstract                 = {Signature matching is a method for organizing, navigating through, and retrieving from software libraries. We consider two kinds of software library components---functions and modules---and hence two kinds of matching---function matching and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match but also various flavors of relaxed match. We describe various applications of signature matching as a tool for using software libraries, inspired by the use of our implementation of a function signature matcher written in Standard ML.},
  Doi                      = {10.1145/210134.210179}
}

@InProceedings{1993:fse:moormann_zaremski,
  Title                    = {Signature matching: A key to reuse},
  Author                   = {Moormann Zaremski, Amy and Wing, Jeannette M.},
  Booktitle                = fse,
  Year                     = {1993},
  Pages                    = {182--190},

  Abstract                 = {Software reuse is only effective if it is easier to locate (and appropriately modify) a reusable component than to write it from scratch. We present signature matching as a method for achieving this goal by using signature information easily derived from the component. We consider two kinds of software components, functions and modules, and hence two kinds of matching, function mathcing and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match, but also various flavors of relaxed match. We briefly describe an experimental facility written in Standard ML for performing signature matching over a library of ML functions.},
  Doi                      = {10.1145/210134.210179}
}

@Article{2004:tse:morel,
  Title                    = {{SPARTACAS}: Automating Component Reuse and Adaptation},
  Author                   = {Morel, Brandon and Alexander, Perry},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {587--600},
  Volume                   = {30},

  Abstract                 = {A continuing challenge for software designers is to develop efficient and cost-effective software implementations. Many see software reuse as a potential solution; however, the cost of reuse tends to outweigh the potential benefits. The costs of software reuse include establishing and maintaining a library of reusable components, searching for applicable components to be reused in a design, as well as adapting components toward a proper implementation. We introduce SPARTACAS, a framework for automating specification-based component retrieval and adaptation that has been successfully applied to synthesis of software for embedded and digital signal processing systems. Using specifications to abstractly represent implementations allows automated theorem-provers to formally verify logical reusability relationships between specifications. These logical relationships are used to evaluate the feasibility of reusing the implementations of components to implement a problem. Retrieving a component that is a complete match to a problem is rare. It is more common to retrieve a component that partially satisfies the requirements of a problem. Such components have to be adapted. Rather than adapting components at the code level, SPARTACAS adapts the behavior of partial matches by imposing interactions with other components at the architecture level. A subproblem is synthesized that specifies the missing functionality required to complete the problem; the subproblem is used to query the library for components to adapt the partial match. The framework was implemented and evaluated empirically, the results suggest that automated adaptation using architectures successfully promotes software reuse, and hierarchically organizes a solution to a design problem.},
  Doi                      = {10.1109/TSE.2004.53}
}

@Article{1990:tse:moriconi,
  Title                    = {Approximate reasoning about the semantic effects of program changes},
  Author                   = {Moriconi, M. and Winkler, T. C.},
  Journal                  = tse,
  Year                     = {1990},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {980--992},
  Volume                   = {16},

  Abstract                 = {It is pointed out that the incremental cost of a change to a program is often disproportionately high because of inadequate means of determining the semantic effects of the change. A practical logical technique for finding the semantic effects of changes through a direct analysis of the program is presented. The programming language features considered include parametrized modules, procedures, and global variables. The logic described is approximate in that weak (conservative) results sometimes are inferred. Isolating the exact effects of a change is undecidable in general. The basis for an approximation is a structural interpretation of the information-flow relationships among program objects. The approximate inference system is concise, abstract, extensible, and decidable, giving it significant advantages over the main alternative formalizations. The authors' implementation of the logic records the justification for each dependency to facilitate the interpretation of results.},
  Doi                      = {10.1109/32.58785}
}

@Article{2002:tse:morisio,
  Title                    = {Success and failure factors in software reuse},
  Author                   = {M. Morisio and M. Ezran and C. Tully},
  Journal                  = tse,
  Year                     = {2002},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {340--357},
  Volume                   = {28},

  Abstract                 = {This paper aims at identifying some of the key factors in adopting or running a company-wide software reuse program. Key factors are derived from empirical evidence of reuse practices, as emerged from a survey of projects for the introduction of reuse in European companies: 24 such projects performed from 1994 to 1997 were analyzed using structured interviews. The projects were undertaken in both large and small companies, working in a variety of business domains, and using both object-oriented and procedural development approaches. Most of them produce software with high commonality between applications, and have at least reasonably mature processes. Despite that apparent potential for success, around one-third of the projects failed. Three main causes of failure were not introducing reuse-specific processes, not modifying nonreuse processes, and not considering human factors. The root cause was a lack of commitment by top management, or nonawareness of the importance of those factors, often coupled with the belief that using the object-oriented approach or setting up a repository seamlessly is all that is necessary to achieve success in reuse. Conversely, successes were achieved when, given a potential for reuse because of commonality among applications, management committed to introducing reuse processes, modifying nonreuse processes, and addressing human factors.},
  Doi                      = {10.1109/TSE.2002.995420}
}

@Article{2008:entcs:moskal,
  Title                    = {{E}-matching for Fun and Profit},
  Author                   = {Moskal, Micha{\l} and {\L}opusza{\'n}ski, Jakub and Kiniry, Joseph R.},
  Journal                  = entcs,
  Year                     = {2008},

  Month                    = may,
  Number                   = {2},
  Pages                    = {19--35},
  Volume                   = {198},

  Abstract                 = {Efficient handling of quantifiers is crucial for solving software verification problems. E-matching algorithms are used in satisfiability modulo theories solvers that handle quantified formulas through instantiation. Two novel, efficient algorithms for solving the E-matching problem are presented and compared to a well-known algorithm described in the literature.},
  Doi                      = {10.1016/j.entcs.2008.04.078}
}

@Article{1948:ams:mosteller,
  Title                    = {A $k$-sample slippage test for an extreme population},
  Author                   = {Mosteller, Frederick},
  Journal                  = ams,
  Year                     = {1948},
  Number                   = {1},
  Pages                    = {58--65},
  Volume                   = {19},

  Abstract                 = {A test is proposed for deciding whether one of $k$ populations has slipped to the right of the rest, under the null hypothesis that all populations are continuous and identical. The procedure is to pick the sample with the largest observation, and to count the number of observations $r$ in it which exceed all observations of all other samples. If all samples are of the same size $n$, $n$ large, the probability of getting $r$ or more such observations, when the null hypothesis is true, is about $k^{1-r}$. Some remarks are made about kinds of errors in testing hypothesies.},
  Doi                      = {10.1214/aoms/1177730290}
}

@InProceedings{2012:oopsla:muslu,
  Title                    = {Speculative analysis of integrated development environment recommendations},
  Author                   = {K{\i}van{\c{c}} Mu{\c{s}}lu and Yuriy Brun and Reid Holmes and Michael D. Ernst and David Notkin},
  Booktitle                = oopsla,
  Year                     = {2012},

  Abstract                 = {Modern integrated development environments make recommendations and automate common tasks, such as refactorings, auto-completions, and error corrections. However, these tools present little or no information about the consequences of the recommended changes. For example, a rename refactoring may: modify the source code without changing program semantics; modify the source code and (incorrectly) change program semantics; modify the source code and (incorrectly) create compilation errors; show a name collision warning and require developer input; or show an error and not change the source code. Having to compute the consequences of a recommendation---either mentally or by making source code changes---puts an extra burden on the developers. This paper aims to reduce this burden with a technique that informs developers of the consequences of code transformations. Using Eclipse Quick Fix as a domain, we describe a plug-in, Quick Fix Scout, that computes the consequences of Quick Fix recommendations. In our experiments, developers completed compilation-error removal tasks 10\% faster when using Quick Fix Scout than Quick Fix, although the sample size was not large enough to show statistical significance.},
  Doi                      = {10.1145/2384616.2384665}
}

@Article{1992:misq:mukhopadhyay,
  Title                    = {Examining the feasibility of a case-based reasoning model for software effort estimation},
  Author                   = {Mukhopadhyay, Tridas and Steven S. Vicinanza and Michael J. Prietula},
  Journal                  = misq,
  Year                     = {1992},
  Number                   = {2},
  Pages                    = {155--171},
  Volume                   = {16},

  Abstract                 = {Existing algorithmic models fail to produce accurate software development effort estimates. To address this problem, a case-based reasoning model, called Estor, was developed based on the verbal protocols of a human expert solving a set of estimation tasks. The estimates of Estor were compared to those of the expert as well as those of the function point and COCOMO estimations of the projects. The estimates generated by the human expert and Estor were more accurate and consistent than those of the function point and COCOMO methods. In fact, Estor was nearly as accurate and consistent and the expert. These results suggest that a case-based reasoning approach for software effort estimation holds promise and merits additional research.},
  Url                      = {http://misq.org/examining-the-feasibility-of-a-case-based-reasoning-model-for-software-effort-estimation.html?SID=hf1sq5c98v4q0gpmmorjc4pv11}
}

@InProceedings{2010:pse:mulder,
  Title                    = {Identifying cross-cutting concerns using software repository mining},
  Author                   = {Mulder, Frank and Zaidman, Andy},
  Booktitle                = pse,
  Year                     = {2010},
  Pages                    = {23--32},

  Abstract                 = {Cross-cutting concerns are pieces of functionality that have not been captured into a separate module, thereby hindering program comprehension and maintainability. Solving these problems requires first identifying these cross-cutting concerns in pieces of software. Several methods for identification have been proposed but the option of using software repository mining has largely been left unexplored. That technique can uncover relationships between modules that may not be present in the source code and thereby provide a different perspective on the cross-cutting concerns in a software system. We perform software repository mining on the repositories of two software systems for which the cross-cutting concerns are known: JHotDraw and Tomcat. Based on the results of the evaluation, we make some suggestions for future directions in the area of identifying crosscutting concerns using software repository mining.}
}

@InProceedings{2010:foser:murphy,
  Title                    = {Human-centric software engineering},
  Author                   = {Murphy, Gail C.},
  Booktitle                = foser,
  Year                     = {2010},
  Pages                    = {251--254},

  Abstract                 = {Research into how humans interact with computers has a long and rich history. Only a small fraction of this research has considered how humans interact with computers when engineering software. A similarly small amount of research has considered how humans interact with humans when engineering software. For the last forty years, we have largely taken an artifact-centric approach to software engineering research. To meet the challenges of building future software systems, I argue that we need to balance the artifact-centric approach with a human-centric approach, in which the focus is on amplifying the human intelligence required to build great software systems. A human-centric approach involves performing empirical studies to understand how software engineers work with software and with each other, developing new methods for both decomposing and composing models of software to to ease the cognitive load placed on engineers and on creating computationally intelligent tools aimed at focusing the humans on the tasks only the humans can solve.},
  Doi                      = {10.1145/1882362.1882414}
}

@Article{2006:software:murphy,
  Title                    = {How Are {J}ava Software Developers Using the {E}clipse {IDE}?},
  Author                   = {Murphy, Gail C. and Kersten, Mik and Findlater, Leah},
  Journal                  = software,
  Year                     = {2006},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {76--83},
  Volume                   = {23},

  Abstract                 = {Eclipse is a leading development environment that provides a rich set of features supporting Java development. However, little data is available about its usage. Usage data from 41 developers using Java and Eclipse shows that they're using advanced features such as refactoring and are extending the environment using third-party tools. However, they rarely use some of the other features, such as bookmarking places in the code. The article also includes briefly describes the authors' Eclipse-based open-source analysis framework. Open-source projects such as Eclipse should be gathering and analyzing more usage data to ensure the tools they're building evolve to meet user communities' needs.},
  Doi                      = {10.1109/MS.2006.105}
}

@Article{1997:computer:murphy,
  Title                    = {Reengineering with reflexion models: A case study},
  Author                   = {Murphy, Gail C. and Notkin, David},
  Journal                  = computer,
  Year                     = {1997},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {29--36},
  Volume                   = {30},

  Abstract                 = {Reengineering large and complex software systems is often very costly. The article presents a reverse engineering technique and relates how a Microsoft engineer used it to aid an experimental reengineering of Excel---a product that comprises about 1.2 million lines of C code. The reflexion technique is designed to be lightweight and iterative. To use it, the user first defines a high-level structural model, then extracts a map of the source code and uses a set of computation tools to compare the two models. The approach lets software engineers effectively validate their high-level reasoning with information from the source code. The engineer in this case study---a developer with 10-plus years at Microsoft---specified and computed an initial reflexion model of Excel in a day and then spent four weeks iteratively refining it. He estimated that gaining the same degree of familiarity with the Excel source code might have taken up to two years with other available approaches. On the basis of this experience, the authors believe that the reflexion technique has practical applications.},
  Doi                      = {10.1109/2.607045}
}

@Article{1996:tosem:murphy,
  Title                    = {Lightweight lexical source model extraction},
  Author                   = {Gail C. Murphy and David Notkin},
  Journal                  = tosem,
  Year                     = {1996},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {262--292},
  Volume                   = {5},

  Abstract                 = {Software engineers maintaining an existing software system often depend on the mechanized extraction of information from system artifacts. Some useful kinds of information---source models---are well known: call graphs, file dependences, etc. Predicting every kind of source model that a software engineer may need is impossible. We have developed a lightweight approach for generating flexible and tolerant source model extractors from lexical specifications. The approach is lightweight in that the specifications are relatively small and easy to write. It is flexible in that there are few constraints on the kinds of artifacts from which source models are extracted (e.g., we can extract from source code, structured data files, documentation, etc.). It is tolerant in that there are few constraints on the condition of the artifacts. For example, we can extract from source that cannot necessarily be compiled. Our approach extended the kinds of source models that can be easily produced from lexical information while avoiding the constraints and brittleness of most parser-based approaches. We have developed tools to support this approach and applied the tools to the extraction of a number of different source models (file dependences, event interactions, call graphs) from a variety of system artifacts (C, C++, CLOS, Eiffel. TCL, structured data). We discuss our approach and describe its application to extract source models not available using existing systems; for example, we compute the implicitly-invokes relation over Field tools. We compare and contrast our approach to the conventional lexical and syntactic approaches of generating source models.},
  Doi                      = {10.1145/234426.234441}
}

@InProceedings{1995:sen:murphy,
  Title                    = {Software reflexion models: Bridging the gap between source and high-level models},
  Author                   = {Murphy, Gail C. and Notkin, David and Sullivan, Kevin},
  Booktitle                = fse,
  Year                     = {1995},
  Month                    = oct,
  Number                   = {4},
  Pages                    = {18--28},
  Volume                   = {20},

  Abstract                 = {Software engineers often use high-level models (for instance, box and arrow sketches) to reason and communicate about an existing software system. One problem with high-level models is that they are almost always inaccurate with respect to the system's source code. We have developed an approach that helps an engineer use a high-level model of the structure of an existing software system as a lens through which to see a model of that system's source code. In particular, an engineer defines a high-level model and specifies how the model maps to the source. A tool then computes a software reflexion model that shows where the engineer's high-level model agrees with and where it differs from a model of the source. The paper provides a formal characterization of reflexion models, discusses practical aspects of the approach, and relates experiences of applying the approach and tools to a number of different systems. The illustrative example used in the paper describes the application of reflexion models to NetBSD, an implementation of Unix comprised of 250,000 lines of C code. In only a few hours, an engineer computed several reflexion models that provided him with a useful, global overview of the structure of the NetBSD virtual memory subsystem. The approach has also been applied to aid in the understanding and experimental reengineering of the Microsoft Excel spreadsheet product.},
  Doi                      = {10.1145/222132.222136}
}

@Article{2001:tse:murphy,
  Title                    = {Software reflexion models: Bridging the gap between design and implementation},
  Author                   = {Murphy, Gail C. and Notkin, David and Sullivan, Kevin J.},
  Journal                  = tse,
  Year                     = {2001},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {364--380},
  Volume                   = {27},

  Abstract                 = {The artifacts constituting a software system often drift apart over time. We have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting, rather than removing, the drift between design and implementation. More specifically, the technique helps an engineer compare artifacts by summarizing where one artifact (such as a design) is consistent with and inconsistent with another artifact (such as source). The technique can be applied to help a software engineer evolve a structural mental model of a system to the point that it is ``good enough'' to be used for reasoning about a task at hand. The software reflexion model technique has been applied to support a variety of tasks, including design conformance, change assessment, and an experimental reengineering of the million-lines-of-code Microsoft Excel product. We provide a formal characterization of the reflexion model technique, discuss practical aspects of the approach, relate experiences of applying the approach and tools, and place the technique into the context of related work.},
  Doi                      = {10.1109/32.917525}
}

@InProceedings{2009:wrt:murphy-hill,
  Title                    = {A Model of Refactoring Tool Use},
  Author                   = {Emerson Murphy-Hill},
  Booktitle                = wrt,
  Year                     = {2009},
  Note                     = {4~pages},

  Abstract                 = {For the most part, refactoring tools have changed little since the Smalltalk Refactoring Browser. By continuing to mimic the Refactoring Browser's user interface, the community of tool builders may be not find new user interfaces that help programmers do their job more effectively. In this position paper, I put forward a general model of how programmers use Refactoring Browser-like tools. I argue that the model is useful for breaking away from the design of traditional refactoring tools and thinking about the design of new ones.}
}

@InProceedings{2008:icse:murphy-hill,
  Title                    = {Breaking the barriers to successful refactoring: Observations and tools for {Extract Method}},
  Author                   = {Murphy-Hill, Emerson and Black, Andrew P.},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {421--430},

  Abstract                 = {Refactoring is the process of changing the structure of code without changing its behavior. Refactoring can be semi-automated with tools, which should make it easier for programmers to refactor quickly and correctly. However, we have observed that many tools do a poor job of communicating errors triggered by the refactoring process and that programmers using them sometimes refactor slowly, conservatively, and incorrectly. In this paper we characterize problems with current refactoring tools, demonstrate three new tools to assist in refactoring, and report on a user study that compares these new tools against existing tools. The results of the study show that speed, accuracy, and user satisfaction can be significantly increased. From the new tools we induce a set of usability recommendations that we hope will help inspire a new generation of programmer-friendly refactoring tools.}
}

@InProceedings{2012:fse:murphy-hill,
  Title                    = {Improving Software Developers' Fluency by Recommending Development Environment Commands},
  Author                   = {Emerson Murphy-Hill and Rahul Jiresal and Gail C. Murphy},
  Booktitle                = fse,
  Year                     = {2012},
  Pages                    = {42:1--42:11},

  Abstract                 = {Software developers interact with the development environments they use by issuing commands that execute various programming tools, from source code formatters to build tools. However, developers often only use a small subset of the commands offered by modern development environments, reducing their overall development fluency. In this paper, we use several existing command recommender algorithms to suggest new commands to developers based on their existing command usage history, and also introduce several new algorithms. By running these algorithms on data submitted by several thousand Eclipse users, we describe two studies that explore the feasibility of automatically recommending commands to software developers. The results suggest that, while recommendation is more difficult in development environments than in other domains, it is still feasible to automatically recommend commands to developers based on their usage history, and that using patterns of past discovery is a useful way to do so.},
  Doi                      = {10.1145/2393596.2393645}
}

@InProceedings{2013:icse:murphyhill,
  Title                    = {The design of bug fixes},
  Author                   = {Emerson R. Murphy-Hill and
 Thomas Zimmermann and
 Christian Bird and
 Nachiappan Nagappan},
  Booktitle                = icse,
  Year                     = {2013},
  Pages                    = {332--341}
}

@Article{2003:physreve:myers,
  Title                    = {Software systems as complex networks: Structure, function, and evolvability of software collaboration graphs},
  Author                   = {Christopher R. Myers},
  Journal                  = physreve,
  Year                     = {2003},

  Month                    = {20 } # oct,
  Pages                    = {046116:1--046116:15},
  Volume                   = {68},

  Abstract                 = {Software systems emerge from mere keystrokes to form intricate functional networks connecting many collaborating modules, objects, classes, methods, and subroutines. Building on recent advances in the study of complex networks, I have examined software collaboration graphs contained within several open-source software systems, and have found them to reveal scale-free, small-world networks similar to those identified in other technological, sociological, and biological systems. I present several measures of these network topologies, and discuss their relationship to software engineering practices. I also present a simple model of software system evolution based on refactoring processes which captures some of the salient features of the observed systems. Some implications of object-oriented design for questions about network robustness, evolvability, degeneracy, and organization are discussed in the wake of these findings.},
  Doi                      = {10.1103/PhysRevE.68.046116}
}

@Article{1986:algorithmica:myers,
  Title                    = {An $O(ND)$ difference algorithm and its variations},
  Author                   = {Eugene W. Myers},
  Journal                  = algorithmica,
  Year                     = {1986},

  Month                    = nov,
  Number                   = {1--4},
  Pages                    = {251--266},
  Volume                   = {1},

  Abstract                 = {The problems of finding a longest common subsequence of two sequences $A$ and $B$ and a shortest edit script for transforming $A$ into $B$ have long been known to be dual problems. In this paper, they are shown to be equivalent to finding a shortest/longest path in an edit graph. Using this perspective, a simple $O(ND)$ time and space algorithm is developed where $N$ is the sum of the lengths of $A$ and $B$ and $D$ is the size of the minimum edit script for $A$ and $B$. The algorithm performs well when differences are small (sequences are similar) and is consequently fast in typical applications. The algorithm is shown to have $O(N+D^2)$ expected-time performance under a basic stochastic model. A refinement of the algorithm requires only $O(N)$ space, and the use of suffix trees leads to an $O(N \log N+D^2)$ time variation.},
  Doi                      = {10.1007/BF01840446}
}

@Book{2002:book:myers,
  Title                    = {Research Design \& Statistical Analysis},
  Author                   = {Jerome L. Myers and Arnold D. Well},
  Publisher                = {Routledge},
  Year                     = {2002},
  Edition                  = {2nd}
}

@Article{2001:tse:myrtveit,
  Title                    = {Analyzing Data Sets with Missing Data: An Empirical Evaluation of Imputation Methods and Likelihood-Based Methods},
  Author                   = {Ingunn Myrtveit and Erik Stensrud and Ulf H. Olsson},
  Journal                  = tse,
  Year                     = {2001},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {999--1013},
  Volume                   = {27},

  Abstract                 = {Missing data are often encountered in data sets used to construct software effort prediction models. Thus far, the common practice has been to ignore observations with missing data. This may result in biased prediction models. The authors evaluate four missing data techniques (MDTs) in the context of software cost modeling: listwise deletion (LD), mean imputation (MI), similar response pattern imputation (SRPI), and full information maximum likelihood (FIML). We apply the MDTs to an ERP data set, and thereafter construct regression-based prediction models using the resulting data sets. The evaluation suggests that only FIML is appropriate when the data are not missing completely at random (MCAR). Unlike FIML, prediction models constructed on LD, MI and SRPI data sets will be biased unless the data are MCAR. Furthermore, compared to LD, MI and SRPI seem appropriate only if the resulting LD data set is too small to enable the construction of a meaningful regression-based prediction model.},
  Doi                      = {10.1109/32.965340}
}

@InCollection{2010:book:oram:nagappan,
  Title                    = {Evidence-Based Failure Prediction},
  Author                   = {Nagappan, Nachiappan and Ball, Thomas},
  Booktitle                = {Making Software: What Really Works, and Why We Believe It},
  Publisher                = {O'Reilly},
  Year                     = {2010},
  Chapter                  = {23},
  Editor                   = {Andy Oram and Greg Wilson},
  Pages                    = {415--434},

  Abstract                 = {Empirical software engineering (SE) studies collect and analyze data from software artifacts and the associated processes and variables to quantify, characterize, and explore the relationship between different variables to deliver high-quality, secure software on time and within budget. In this chapter we discuss empirical studies related to failure prediction on the Windows operating system family. Windows is a large commercial software system implemented predominantly in C/C++/C# and currently used by several hundreds of millions of users across the world. It contains 40+ million lines of code and is worked on by several hundreds of engineers. Failure prediction forms a crucial part of empirical SE, as it can be used to understand the maintenance effort required for testing and resource allocation. For example: Resource allocation Software quality assurance consumes a considerable effort in any large-scale software development. To raise the effectiveness of this effort, it is necessary to plan in advance for fixing issues in the components that are more likely to fail and thereby need quality assurance most. Decision making Predictions on the number of failures can support other decisions, such as choosing the correct requirements or design, the ability to select the best possible fix for a problem, etc. The associated risk of a change (or likelihood to fail) can help in making decisions about the risk introduced by the change. Failure predictions also help in assessing the overall stability of the system and help make decisions about the ability to release on time. We explore the step-by-step progression of applying various metrics to predict failures. For each set for metrics we discuss the rationale behind metric selection, a description of the metric, and the results of applying the metrics for actual failure prediction in a large commercial software system.}
}

@InProceedings{2005:icse:nagappan,
  Title                    = {Use of relative code churn measures to predict system defect density},
  Author                   = {Nagappan, Nachiappan and Ball, Thomas},
  Booktitle                = icse,
  Year                     = {2005},
  Pages                    = {284--292},

  Abstract                 = {Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn.Using statistical regression models, we show that while absolute measures of code churn are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.},
  Doi                      = {10.1145/1062455.1062514}
}

@InProceedings{2008:icse:nagappan,
  Title                    = {The influence of organizational structure on software quality: An empirical case study},
  Author                   = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {521--530},

  Abstract                 = {Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.},
  Doi                      = {10.1145/1368088.1368160}
}

@InProceedings{2010:issre:nagappan,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nagappan, Nachiappan and Zeller, Andreas and Zimmermann, Thomas and Herzig, Kim and Murphy, Brendan},
  Booktitle                = issre,
  Year                     = {2010},
  Pages                    = {309--318},

  Abstract                 = {In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90\%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.},
  Doi                      = {10.1109/ISSRE.2010.25}
}

@InProceedings{2007:icsm:nagarajan,
  Title                    = {Matching Control Flow of Program Versions},
  Author                   = {Vijay Nagarajan and Rajiv Gupta and Xiangyu Zhang and Matias Madou and De Sutter, Bjorn},
  Booktitle                = icsm,
  Year                     = {2007},
  Pages                    = {84--93},

  Abstract                 = {In many application areas, including piracy detection, software debugging and maintenance, situations arise in which there is a need for comparing two versions of a program that dynamically behave the same even though they statically appear to be different. Recently dynamic matching [18] was proposed by us which uses execution histories to automatically produce mappings between instructions in the two program versions. The mappings then can be used to understand the correspondence between the two versions by a user involved in software piracy detection or a comparison checker involved in debugging of optimized code. However, if a program's control flow is substantially altered, which usually occurs in obfuscation or even manual transformations, mappings at instruction level are not sufficient to enable a good understanding of the correspondence. In this paper, we present a comprehensive dynamic matching algorithm with the focus on call graph and control flow matching. Our technique works in the presence of aggressive control flow transformations (both interprocedural such as function Mining/outlining and intraprocedural such as control flow flattening) and produces mappings of interprocedural and intraprocedural control flow in addition to mapping between instructions. We evaluated our dynamic matching algorithms by attempting to match original program with versions that were subjected to popular obfuscation and control flow altering transformations. Our experimental results show that the control flow mappings produced are highly accurate and complete, for the programs considered.},
  Doi                      = {10.1109/ICSM.2007.4362621}
}

@InProceedings{2012:ictke:nagwani,
  Title                    = {Predicting expert developers for newly reported bugs using frequent terms similarities of bug attributes},
  Author                   = {Naresh Kumar Nagwani and Shrish Verma},
  Booktitle                = ictke,
  Year                     = {2012},
  Pages                    = {113--117},

  Abstract                 = {A software bug repository not only contains the data about software bugs, but also contains the information about the contribution of developers, quality engineers (testers), managers and other team members. It contains the information about the efforts of team members involved in resolving the software bugs. This information can be analyzed to identify some useful knowledge patterns. One such pattern is identifying the developers, who can help in resolving the newly reported software bugs. In this paper a new algorithm is proposed to discover experts for resolving the newly assigned software bugs. The purpose of proposed algorithm is two fold. First is to identify the appropriate developers for newly reported bugs. And second is to find the expertise for newly reported bugs that can help other developers to fix these bugs if required. All the important information in software bug reports is of textual data types like bug summary, description etc. The algorithm is designed using the analysis of this textual information. Frequent terms are generated from this textual information and then term similarity is used to identify appropriate experts (developers) for the newly reported software bug.},
  Doi                      = {10.1109/ICTKE.2012.6152388}
}

@InProceedings{2008:compsac:nakagawa,
  Title                    = {Software Architecture Relevance in Open Source Software Evolution: A Case Study},
  Author                   = {Nakagawa, Elisa Y. and de Sousa, Elaine Parros Machado and de Brito Murata, Kiyoshi and de Faria Andery, Gabriel and Morelli, Leonardo Bitencourt and Maldonado, Jos{\'e} Carlos},
  Booktitle                = compsac,
  Year                     = {2008},
  Pages                    = {1234--1239},

  Abstract                 = {Software architecture has received increasing attention of practitioners and researchers, since it has played a significant role in determining the success and quality of software systems. At the same time, the success of open source software (OSS) has also sparked interest of researchers in the universities and in the software industry. OSS has been largely used and developed and, as a consequence, the OSS quality has been a concern and an interesting subject for researchers. However, in spite of narrow relation between software architecture and software quality, there is lack of more detailed works that investigate how software architecture can influence OSS quality. In this paper, we present a case study reporting how software architecture is directly related to OSS quality. We have hence proposed architecture refactoring activity in order to repair software architectures, aiming at improving mainly maintainability, functionality and usability of these systems.},
  Doi                      = {10.1109/COMPSAC.2008.171}
}

@InProceedings{2011:issta:namin,
  Title                    = {The use of mutation in testing experiments and its sensitivity to external threats},
  Author                   = {Namin, Akbar Siami and Kakarla, Sahitya},
  Booktitle                = issta,
  Year                     = {2011},
  Pages                    = {342--352},

  Doi                      = {10.1145/2001420.2001461}
}

@MastersThesis{2003:msc:nannen,
  Title                    = {The Paradox of Overfitting},
  Author                   = {Volker Nannen},
  School                   = {Rijksuniversiteit Groningen},
  Year                     = {2003},

  Address                  = {Groningen, The Netherlands},
  Month                    = apr,

  Abstract                 = {The aim of this master's thesis is the experimental verification of Minimum Description Length (MDL) as a method to effectively minimize the generalization error in data prediction. An application was developed to map the strength of the theory in a large number of controlled experiments: the Statistical Data Viewer. This application is primarily intended for scientists. Nevertheless, great care was taken to keep the interface simple enough to allow uninitiated students and interested outsiders to playfully explore and discover the complex laws and mathematics of the theory. This first chapter will introduce you to model selection and the theory of Minimum Description Length. Chapter Two deals with the problems of experimental verification. To make you familiar with the Statistical Data Viewer and to give you an idea of the practical problems involved in model selection, it will also lead you through all the steps of a basic experiments. Chapter Three describes some selected experiments on two dimensional regression problems. Chapter Four discusses the results and Chapter Five gives a short summary of all the thesis. It is not necessary to actually use the application in order to understand the experiments that are described in this thesis. These experiments were selected for diversity and are intended to give you an optimal view on the problem. But they cannot cover all aspects of model selection. If you like to understand the problems of model selection even better you might want to conduct some experiments of your own. Appendix A contains a manual on the application. And if you want to go further and add your own functionality to the application, you will find Appendix B useful where some specifications of the application are given. Appendix C describes the implementation of the algorithms that were used for the experiments of this thesis.},
  Url                      = {http://www.ai.rug.nl/nl/afstuderen/msc-theses/msc-thesis-volker-nannen-2003.pdf}
}

@Book{2004:book:nau,
  Title                    = {Automated Planning: Theory \& Practice},
  Author                   = {Nau, Dana and Ghallab, Malik and Traverso, Paolo},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2004},

  Address                  = {San Francisco, CA, USA}
}

@Article{2001:csur:navarro,
  Title                    = {A guided tour to approximate string matching},
  Author                   = {Navarro, Gonzalo},
  Journal                  = csur,
  Year                     = {2001},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {31--88},
  Volume                   = {33},

  Abstract                 = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
  Doi                      = {10.1145/375360.375365}
}

@Article{2008:software:ncube,
  Title                    = {Opportunistic software systems development: Making systems from what's available},
  Author                   = {Ncube, Cornelius and Oberndorf, Patricia and Kark, Anatol W.},
  Journal                  = software,
  Year                     = {2008},

  Month                    = nov # {--} # dec,
  Number                   = {6},
  Pages                    = {38--41},
  Volume                   = {25},

  Abstract                 = {Despite all the difficulties encountered in each incarnation of software reuse, we persist along the path of trying to figure out how we're going to create systems that meet the ever-increasing demand for capability and, with it, complexity and sheer size. Opportunistic software systems development is a reality today and for the foreseeable future, as the five articles in IEEE Software's November/December 2008 special issue on OSSD demonstrate. This special issue aims to gather together insights into the viability, or perhaps the inevitability, of OSSD and to bring forward the most effective practices known today. Another goal is to point the way to what needs to be done to make OSSD more accessible to all practitioners. The editors hope to help the software community realize the importance of this new trend and the many aspects of it that have yet to be conquered.},
  Doi                      = {10.1109/MS.2008.153}
}

@InProceedings{1989:chi:neal,
  Title                    = {A System for Example-Based Programming},
  Author                   = {L. R. Neal},
  Booktitle                = chi,
  Year                     = {1989},
  Pages                    = {63--68},

  Abstract                 = {We present an approach to programming environments that integrates syntax-directed editors with concepts borrowed from software reuse. We call our approach example-based programming, and we define it as programming using examples as visual aids or to fully or partially copy into programs. To implement an example-based programming environment, we augmented a syntax-directed editor with a window for example programs. The example programs, which are easily accessible, can be used as examples of language constructs, thus providing syntactic information through instantiations of templates, or as examples of algorithms or programs. The code in the example window can be viewed, totally or partially copied, or run. We discuss the motivation for example-based programming, describe our system implementing example-based programming in greater depth, and report on the results of an experiment to see how the system is used by programmers.},
  Doi                      = {10.1145/67449.67464}
}

@InProceedings{2005:msr:neamtiu,
  Title                    = {Understanding source code evolution using abstract syntax tree matching},
  Author                   = {Neamtiu, Iulian and Foster, Jeffrey S. and Hicks, Michael},
  Booktitle                = msrw,
  Year                     = {2005},
  Pages                    = {1--5},

  Abstract                 = {Mining software repositories at the source code level can provide a greater understanding of how software evolves. We present a tool for quickly comparing the source code of different versions of a C program. The approach is based on partial abstract syntax tree matching, and can track simple changes to global variables, types and functions. These changes can characterize aspects of software evolution useful for answering higher level questions. In particular, we consider how they could be used to inform the design of a dynamic software updating system. We report results based on measurements of various versions of popular open source programs. including BIND, OpenSSH, Apache, Vsftpd and the Linux kernel.},
  Doi                      = {10.1145/1082983.1083143}
}

@InProceedings{2012:ecoop:negara,
  Title                    = {Is it dangerous to use version control histories to study source code evolution?},
  Author                   = {Negara, Stas and Vakilian, Mohsen and Chen, Nicholas and Johnson, Ralph E. and Dig, Danny},
  Booktitle                = ecoop,
  Year                     = {2012},
  Pages                    = {79--103},
  Series                   = lncs,
  Volume                   = {7313},

  Abstract                 = {Researchers use file-based Version Control System (VCS) as the primary source of code evolution data. VCSs are widely used by developers, thus, researchers get easy access to historical data of many projects. Although it is convenient, research based on VCS data is incomplete and imprecise. Moreover, answering questions that correlate code changes with other activities (e.g., test runs, refactoring) is impossible. Our tool, CodingTracker, non-intrusively records fine-grained and diverse data during code development. CodingTracker collected data from 24 developers: 1,652 hours of development, 23,002 committed files, and 314,085 testcase runs. This allows us to answer: How much code evolution data is not stored in VCS? How much do developers intersperse refactorings and edits in the same commit? How frequently do developers fix failing tests by changing the test itself? How many changes are committed to VCS without being tested? What is the temporal and spacial locality of changes?},
  Doi                      = {10.1007/978-3-642-31057-7_5}
}

@InCollection{1989:book:biggerstaff:neighbors,
  Title                    = {Draco: A method for engineering reusable software systems},
  Author                   = {J. M. Neighbors},
  Booktitle                = {Software Reusability},
  Publisher                = {Addison--Wesley},
  Year                     = {1989},
  Editor                   = {Ted J. Biggerstaff and Alan J. Perlis},
  Pages                    = {295--319},
  Volume                   = {1: Concepts and Models},

  Abstract                 = {Everyone is looking for an order of magnitude increase in the production of software systems; but, historically, such increases have never been achieved. Certainly such an increase will not be the result of simple extensions of current techniques. Many factors have contributed to the current ``software crisis''. \begin{itemize} \item The price/performance ratio of computing hardware has been decreasing about 20\% per year [Morrissey79]. \item The total installed processing capacity is increasing at better than 40\% per year [Morrissey79]. \item As computers become less expensive, they are used in more application areas all of which demand software. \item The cost of software as a percentage cost of a total computing system has been steadily increasing. The cost of hardware as a percentage cost of a total computing system has been steadily decreasing [Boehm81]. \item The productivity of the software creation process has increased only 3\%--8\% per year for the last thirty years [Morrissey 79]. This increase in productivity includes all the developments in software engineering and the development of higher-level languages. \item There is a shortage of qualified personnel to create software [Lentz80]. \item As the size of a software system grows, it becomes increasingly hard to construct. \end{itemize} The ``software crisis'' is not a problem of small systems. Adequate methods exist for a single programmer to produce 10k lines of high-level source code or five programmers to produce 50k lines of high-level source. Perhaps finding people who are familiar with the development techniques is difficult, but the methods appear adequate. Software development becomes a crisis when twenty people attempt to cooperate in the development of a 200k line system. Systems of this size have murky and ambiguous specifications. The social interactions of the developing team members become a major expense of time. The interest in reusable software stems from the realization that one way to increase productivity during the production of a particular system is to produce less software for that system while achieving the same fu[n]ctionality. This can be done by building the system out of reusable software components and amortizing the cost of developing the general software components over the construction costs of many systems. The Draco approach to the construction of software from reusable software components described in this paper neither deals with the important problems of organizational interactions of developing team members nor methods for the complete specification of software systems. Instead we focus only on the constructive aspects of software production (analysis, design, implementation) under the assumption that with such an approach the number of development team members producing a large system could be drastically cut and the specification clarified using a rapid development feedback cycle with the original specifiers. The first Draco prototype was completed in 1979 [Neighbors80, Neighbors84b, Freeman87] and the last major revision of the mechanism was completed in 1983 [Neighbors84a]. Since that time the instrumental use of the mechanism has been stressed to understand its limits and pitfalls [Gonzalez81, Sundfor83a, Sundfor83b, Arango86]. This paper discusses the approach, including what we perceive as necessary future changes to the method to attempt the construction of truly large systems. These changes have not been implemented and experimented with on real systems.}
}

@InProceedings{1994:icsr:neighbors,
  Title                    = {An assessment of reuse technology after ten years},
  Author                   = {Neighbors, James M.},
  Booktitle                = icsr,
  Year                     = {1994},
  Pages                    = {6--13},

  Abstract                 = {More than ten years ago the first major workshop on ``Reusability in Programming'' was held. Since that time some technologies have advanced and come into successful commercial use while others have gone unused. New management and abstraction techniques have aided reuse. Interfacing to huge abstractions, now in common use, has made reuse more difficult. This paper is not a formal survey of reuse technology but instead discusses the evolution of early concepts and the issues they raise. Much of the research of the original workshop participants is just now becoming relevant. In some cases the research of the past points to problems and solutions for the present. As part of this examination the activities in reuse for the next ten years are forecast and a guide of hard questions to ask purveyors of reuse technology is provided.},
  Doi                      = {10.1109/ICSR.1994.365816}
}

@Book{2006:book:newman,
  Title                    = {The Structure and Dynamics of Networks},
  Author                   = {Mark Newman and Albert-L{\'a}szl{\'o} Barab{\'a}si and Duncan J. Watts},
  Publisher                = {Princeton University Press},
  Year                     = {2006}
}

@Article{2006:pnas:newman,
  Title                    = {Modularity and community structure in networks},
  Author                   = {M. E. J. Newman},
  Journal                  = pnas,
  Year                     = {2006},

  Month                    = {6 } # jun,
  Number                   = {23},
  Pages                    = {8577--8582},
  Volume                   = {103},

  Abstract                 = {Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as ``modularity" over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets.},
  Doi                      = {10.1073/pnas.0601602103}
}

@Article{2005:conphys:newman,
  Title                    = {Power laws, {P}areto distributions and {Z}ipf's law},
  Author                   = {Newman, M. E. J.},
  Journal                  = conphys,
  Year                     = {2005},
  Number                   = {5},
  Pages                    = {323--351},
  Volume                   = {46},

  Abstract                 = {When the probability of measuring a particular value of some quantity varies inversely as a power of that value, the quantity is said to follow a power law, also known variously as Zipf's law or the Pareto distribution. Power laws appear widely in physics, biology, earth and planetary sciences, economics and finance, computer science, demography and the social sciences. For instance, the distributions of the sizes of cities, earthquakes, solar flares, moon craters, wars and people's personal fortunes all appear to follow power laws. The origin of power-law behaviour has been a topic of debate in the scientific community for more than a century. Here we review some of the empirical evidence for the existence of power-law forms and the theories proposed to explain them.},
  Doi                      = {10.1080/00107510500052444}
}

@Article{2003:socnet:newman,
  Title                    = {Ego-centered networks and the ripple effect},
  Author                   = {M. E. J. Newman},
  Journal                  = socnet,
  Year                     = {2003},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {83--95},
  Volume                   = {25},

  Abstract                 = {Recent work has demonstrated that many social networks, and indeed many networks of other types also, have broad distributions of vertex degree. Here we show that this has a substantial impact on the shape of egocentered networks, i.e., sets of network vertices that are within a given distance of a specified central vertex, the ego. This in turn affects concepts and methods based on ego-centered networks, such as snowball sampling and the ``ripple effect." In particular, we argue that one's acquaintances, one's immediate neighbors in the acquaintance network, are far from being a random sample of the population, and that this biases the numbers of neighbors two and more steps away. We demonstrate this concept using data drawn from academic collaboration networks, for which, as we show, current simple theories for the typical size of ego-centered networks give numbers that differ greatly from those measured in reality. We present an improved theoretical model which gives significantly better results.},
  Doi                      = {10.1016/S0378-8733(02)00039-4}
}

@Article{2002:prl:newman,
  Title                    = {Assortative Mixing in Networks},
  Author                   = {M. E. J. Newman},
  Journal                  = prl,
  Year                     = {2002},

  Month                    = {28 } # oct,
  Number                   = {20},
  Pages                    = {208701:1--208701:4},
  Volume                   = {89},

  Abstract                 = {A network is said to show assortative mixing if the nodes in the network that have many connections tend to be connected to other nodes with many connections. Here we measure mixing patterns in a variety of networks and find that social networks are mostly assortatively mixed, but that technological and biological networks tend to be disassortative. We propose a model of an assortatively mixed network, which we study both analytically and numerically. Within this model we find that networks percolate more easily if they are assortative and that they are also more robust to vertex removal.},
  Doi                      = {10.1103/PhysRevLett.89.208701}
}

@Article{2000:jsp:newman,
  Title                    = {Models of the small world: A review},
  Author                   = {M. E. J. Newman},
  Journal                  = jsp,
  Year                     = {2000},

  Month                    = {1 } # nov,
  Number                   = {3--4},
  Pages                    = {819--841},
  Volume                   = {101},

  Abstract                 = {It is believed that almost any pair of people in the world can be connected to one another by a short chain of intermediate acquaintances, of typical length about six. This phenomenon, colloquially referred to as the ``six degrees of separation," has been the subject of considerable recent interest within the physics community. This paper provides a short review of the topic.},
  Doi                      = {10.1023/A:1026485807148}
}

@Article{2001:physreve:newman,
  Title                    = {Random graphs with arbitrary degree distributions and their applications},
  Author                   = {M. E. J. Newman and S. H. Strogatz and D. J. Watts},
  Journal                  = physreve,
  Year                     = {2001},

  Month                    = {24 } # jul,
  Number                   = {2},
  Pages                    = {026118:1--026118:17},
  Volume                   = {64},

  Abstract                 = {Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.},
  Doi                      = {10.1103/PhysRevE.64.026118}
}

@InProceedings{2012:icse:nguyen:b,
  Title                    = {{GraPacc}: A graph-based pattern-oriented, context-sensitive code completion tool},
  Author                   = {Anh Tuan Nguyen and Hoan Anh Nguyen and Tung Thanh Nguyen and Tien N. Nguyen},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {1407--1410},

  Abstract                 = {Code completion tool plays an important role in daily development activities. It helps developers by auto-completing tedious and detailed code during an editing session. However, existing code completion tools are limited to recommending only context-free code templates and a single method call of the variable under editing. We introduce GraPacc, an advanced, context-sensitive code completion tool that is based on frequent API usage patterns. It extracts the context-sensitive features from the code under editing, for example, the API elements on focus and the current editing point, and their relations to other code elements. It then ranks the relevant API usage patterns and auto-completes the current code with the proper elements according to the chosen pattern.},
  Doi                      = {10.1109/ICSE.2012.6227236}
}

@InProceedings{2011:ase:nguyen:a,
  Title                    = {A topic-based approach for narrowing the search space of buggy files from a bug report},
  Author                   = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Al-Kofahi, Jafar and Nguyen, Hung Viet and Nguyen, Tien N.},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {263--272},

  Abstract                 = {Locating buggy code is a time-consuming task in software development. Given a new bug report, developers must search through a large number of files in a project to locate buggy code. We propose BugScout, an automated approach to help developers reduce such efforts by narrowing the search space of buggy files when they are assigned to address a bug report. BugScout assumes that the textual contents of a bug report and that of its corresponding source code share some technical aspects of the system which can be used for locating buggy source files given a new bug report. We develop a specialized topic model that represents those technical aspects as topics in the textual contents of bug reports and source files, and correlates bug reports and corresponding buggy files via their shared topics. Our evaluation shows that BugScout can recommend buggy files correctly up to 45\% of the cases with a recommended ranked list of 10 files.},
  Doi                      = {10.1109/ASE.2011.6100062}
}

@InProceedings{2012:icse:nguyen:a,
  Title                    = {Graph-Based Pattern-Oriented, Context-Sensitive Source Code Completion},
  Author                   = {Anh Tuan Nguyen and Tung Thanh Nguyen and Hoan Anh Nguyen and Ahmed Tamrawi and Hung Viet Nguyen and Jafar Al-Kofahi and Tien N. Nguyen},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {69--79},

  Abstract                 = {Code completion helps improve developers' programming productivity. However, the current support for code completion is limited to context-free code templates or a single method call of the variable on focus. Using software libraries for development, developers often repeat API usages for certain tasks. Thus, a code completion tool could make use of API usage patterns. In this paper, we introduce GraPacc, a graph-based, pattern-oriented, context-sensitive code completion approach that is based on a database of such patterns. GraPacc represents and manages the API usage patterns of multiple variables, methods, and control structures via graph-based models. It extracts the context-sensitive features from the code under editing, e.g. the API elements on focus and their relations to other code elements. Those features are used to search and rank the patterns that are most fitted with the current code. When a pattern is selected, the current code will be completed via a novel graph-based code completion algorithm. Empirical evaluation on several real-world systems shows that GraPacc has a high level of accuracy in code completion.},
  Doi                      = {10.1109/ICSE.2012.6227205}
}

@InProceedings{2013:ase:nguyen,
  Title                    = {A study of repetitiveness of code changes in software evolution},
  Author                   = {Nguyen, Hoan Anh and Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N. and Rajan, Hridesh},
  Booktitle                = ase,
  Year                     = {2013},
  Pages                    = {180--190},

  Doi                      = {10.1109/ASE.2013.6693078}
}

@InProceedings{2011:ase:nguyen:b,
  Title                    = {{iDiff}: Interaction-based Program Differencing Tool},
  Author                   = {Hoan Anh Nguyen and Tung Thanh Nguyen and Hung Viet Nguyen and Tien N. Nguyen},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {572--575},

  Abstract                 = {When a software system evolves, its program entities such as classes/methods are also changed. System comprehension, maintenance, and other tasks require the detection of the changed entities between two versions. However, existing differencing tools are file-based and cannot handle well the common cases in which the methods/classes are reordered/moved or even renamed/modified. Moreover, many tools show the program changes at the text line level. In this demo, we present iDiff, a program differencing tool that is able to display the changes to classes/methods between two versions and to track the corresponding classes/methods even they were reordered/moved/renamed and/or modified. The key idea is that during software evolution, an entity could change its location, name, order, and even its internal implementation. However, its interaction with other entities would be more stable. iDiff represents a system at a version as an attributed graph, in which the nodes represent program entities, the edges represent the interactions between the nodes. Entities between two versions are matched via an incremental matching algorithm, which takes into account the similarity of interactions for matching. The differences of two versions of the entire system including its program entities are detected based on the matched entities.},
  Doi                      = {10.1109/ASE.2011.6100128}
}

@Article{2012:tse:nguyen,
  Title                    = {Clone management for evolving software},
  Author                   = {Nguyen, Hoan Anh and Nguyen, Tung Thanh and Pham, Nam H. and Al-Kofahi, Jafar and Nguyen, Tien N.},
  Journal                  = tse,
  Year                     = {2012},

  Month                    = sep # {--} # oct,
  Number                   = {5},
  Pages                    = {1008--1026},
  Volume                   = {38},

  Doi                      = {10.1109/TSE.2011.90}
}

@InProceedings{2010:oopsla:nguyen,
  Title                    = {A graph-based approach to {API} usage adaptation},
  Author                   = {Nguyen, Hoan Anh and Nguyen, Tung Thanh and Wilson, Jr., Gary and Nguyen, Anh Tuan and Kim, Miryung and Nguyen, Tien N.},
  Booktitle                = oopsla,
  Year                     = {2010},
  Pages                    = {302--321},

  Abstract                 = {Reusing existing library components is essential for reducing the cost of software development and maintenance. When library components evolve to accommodate new feature requests, to fix bugs, or to meet new standards, the clients of software libraries often need to make corresponding changes to correctly use the updated libraries. Existing API usage adaptation techniques support simple adaptation such as replacing the target of calls to a deprecated API, however, cannot handle complex adaptations such as creating a new object to be passed to a different API method, or adding an exception handling logic that surrounds the updated API method calls. This paper presents LIBSYNC that guides developers in adapting API usage code by learning complex API usage adaptation patterns from other clients that already migrated to a new library version (and also from the API usages within the library's test code). LIBSYNC uses several graph-based techniques (1) to identify changes to API declarations by comparing two library versions, (2) to extract associated API usage skeletons before and after library migration, and (3) to compare the extracted API usage skeletons to recover API usage adaptation patterns. Using the learned adaptation patterns, LIBSYNC recommends the locations and edit operations for adapting API usages. The evaluation of LIBSYNC on real-world software systems shows that it is highly correct and useful with a precision of 100\% and a recall of 91\%.},
  Doi                      = {10.1145/1869459.1869486}
}

@InProceedings{2010:wcre:nguyen,
  Title                    = {A Case Study of Bias in Bug-Fix Datasets},
  Author                   = {Nguyen, Thanh H. D. and Adams, Bram and Hassan, Ahmed E.},
  Booktitle                = wcre,
  Year                     = {2010},
  Pages                    = {259--268},

  Abstract                 = {Software quality researchers build software quality models by recovering traceability links between bug reports in issue tracking repositories and source code files. However, all too often the data stored in issue tracking repositories is not explicitly tagged or linked to source code. Researchers have to resort to heuristics to tag the data (e.g., to determine if an issue is a bug report or a work item), or to link a piece of code to a particular issue or bug. Recent studies by Bird et al. and by Antoniol et al. suggest that software models based on imperfect datasets with missing links to the code and incorrect tagging of issues, exhibit biases that compromise the validity and generality of the quality models built on top of the datasets. In this study, we verify the effects of such biases for a commercial project that enforces strict development guidelines and rules on the quality of the data in its issue tracking repository. Our results show that even in such a perfect setting, with a near-ideal dataset, biases do exist---leading us to conjecture that biases are more likely a symptom of the underlying software development process instead of being due to the used heuristics.},
  Doi                      = {10.1109/WCRE.2010.37}
}

@Article{2008:jsw:nguyen,
  Title                    = {Managing Software Architectural Evolution at Multiple Levels of Abstraction},
  Author                   = {Tien N. Nguyen},
  Journal                  = jsw,
  Year                     = {2008},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {60--70},
  Volume                   = {3},

  Abstract                 = {Software development is a dynamic process where engineers constantly modify and refine systems. As a consequence, system architecture evolves over time. Software architectural evolution has been managed at different abstraction levels: the meta level, the architectural level, the application level, and the implementation level. However, management supports for architectural evolution are limited to evolution mechanisms in architectural description languages such as subtyping, inheritance, interface, and genericity. This paper presents a model-oriented version and configuration control approach to managing the evolution of architectural entities and relationships among them in configurations at different levels of abstraction. This paper also illustrates our approach in building an architectural configuration management system, MolhadoArch, that is capable of managing configurations and versions of software architecture across multiple levels of abstraction in a uniform and tightly connected manner. In MolhadoArch, consistent configurations are maintained not only among source code but also with the high-level software architecture. MolhadoArch supports the management of both planned and unplanned evolution of software architecture. We have conducted an experimental study to show that MolhadoArch can handle large and real-world systems. By evaluation, we learned that the benefits outweigh the extra space needed to represent architectural entities.},
  Doi                      = {10.4304/jsw.3.3.60-70}
}

@InProceedings{2004:icsm:nguyen,
  Title                    = {Architectural software configuration management in {M}olhado},
  Author                   = {Nguyen, Tien N. and Munson, Ethan V. and Boyland, John T. and Cheng Thao},
  Booktitle                = icsm,
  Year                     = {2004},
  Pages                    = {296--305},

  Abstract                 = {In this research demonstration, we describes Molhado, a research prototype of an architectural software configuration management (SCM) system that captures the evolution of system architecture, logical structures, and implementation source code in a natural and cohesive manner.},
  Doi                      = {10.1109/ICSM.2004.1357815}
}

@InProceedings{2005:scm:nistor,
  Title                    = {{ArchEvol}: Versioning architectural-implementation relationships},
  Author                   = {Nistor, Eugen C. and Erenkrantz, Justin R. and Hendrickson, Scott A. and van der Hoek, Andr{\'e}},
  Booktitle                = scm,
  Year                     = {2005},
  Pages                    = {99--111},

  Abstract                 = {Previous research efforts into creating links between software architecture and its implementations have not explicitly addressed versioning. These earlier efforts have either ignored versioning entirely, created overly constraining couplings between architecture and implementation, or disregarded the need for versioning upon deployment. This situation calls for an explicit approach to versioning the architecture-implementation relationship capable of being used throughout design, implementation, and deployment. We present ArchEvol, a set of xADL 2.0 extensions, ArchStudio and Eclipse plug-ins, and Subversion guidelines for managing the architectural-implementation relationship throughout the entire software life cycle.}
}

@InProceedings{2010:icse:nita,
  Title                    = {Using twinning to adapt programs to alternative {APIs}},
  Author                   = {Marius Nita and David Notkin},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {205--214},

  Abstract                 = {We describe twinning and its applications to adapting programs to alternative APIs. Twinning is a simple technique that allows programmers to specify a class of program changes, in the form of a mapping, without modifying the target program directly. Using twinning, programmers can specify changes that transition a program from using one API to using an alternative API. We describe two related mapping-based source-to-source transformations. The first applies the mapping to a program, producing a copy with the changes applied. The second generates a new API that abstracts the changes specified in the mapping. Using this API, programmers can invoke either the old (replaced) code or the new (replacement) code through a single interface. Managing program variants usually involves heavyweight tasks that can prevent the program from compiling for extended periods of time, as well as simultaneous maintenance of multiple implementations, which can make it easy to forget to add features or to fix bugs symmetrically. Our main contribution is to show that, at least in some common cases, the heavyweight work can be reduced and symmetric maintenance can be at least encouraged, and often enforced.},
  Doi                      = {10.1145/1806799.1806832}
}

@InProceedings{2010:shark:noppen,
  Title                    = {{ETAK}: Tailoring architectural evolution by (re-)using architectural knowledge},
  Author                   = {Noppen, Joost and Tamzalit, Dalila},
  Booktitle                = shark,
  Year                     = {2010},
  Pages                    = {21--28},

  Abstract                 = {When an architect is faced with architectural evolution needs, he can opt to apply an existing evolution pattern. This is useful for well-known evolutions but at times is only partly sufficient. When he faces more specific evolutions, the architect needs to rely on expertise and intuition to extend the evolution beyond the pattern, a risky, error-prone evolution activity. In particular when the architect wants to assess the relevance of potential evolutions, he has no systematic assistance for analysing architectural knowledge. We propose ETAK as a framework for providing such automated assistance. ETAK allows the architect to define architectural traits he has in mind and the specific architectural knowledge he wants to consider. ETAK establishes the relevance of these traits for the new architecture, which can be used to decide whether to include them. We thus propose tailored architectural evolutions, drawing on intuition of the architect and architectural knowledge.}
}

@InProceedings{2009:pse:nurolahzade,
  Title                    = {The role of patch review in software evolution: An analysis of the Mozilla Firefox},
  Author                   = {Nurolahzade, Mehrdad and Nasehi, Seyed Mehdi and Khandkar, Shahedul Huq and Rawal, Shreya},
  Booktitle                = pse,
  Year                     = {2009},
  Pages                    = {9--18},

  Abstract                 = {Patch review is the basic mechanism for validating the design and implementation of patches and maintaining consistency in some commercial and Free/Libre/Open Source Software (FLOSS) projects. We examine the inner-workings of the development process of the successful and mature Mozilla foundation and highlight how different parties involved affect and steer the process. Although reviewers are the primary actors in the patch review process, success in the process can only be achieved if the community supports reviewers adequately. Peer developers play the supporting role by offering insight and ideas that help create more quality patches. Moreover, they reduce the huge patch backlog reviewers have to clear by identifying and eliminating immature patches.}
}

@TechReport{2003:tr:obrien,
  Title                    = {Architecture Reconstruction Case Study},
  Author                   = {Liam O'Brien and Cristoph Stoermer},
  Institution              = {Carnegie Mellon University, Software Engineering Institute},
  Year                     = {2003},
  Month                    = apr,
  Number                   = {CMU/SEI-2003-TN-008},
  Type                     = {Technical Note},

  Abstract                 = {This report outlines an architecture reconstruction carried out at the Software Engineering Institute (SEI) on a software system called VANISH that was developed for prototyping visualizations. The goals of the reconstruction were to understand the existing VANISH system and to use a new architecture reconstruction tool, called ARMIN, for the reconstruction, while ensuring that ARMIN has at least the same capabilities as the Dali Architecture Reconstruction Workbench. During the reconstruction several architectural views were generated through abstraction of low-level information extracted from the system. These views show the components of the system and the interfaces among them. The ARMIN tool provides the ability to visualize, navigate, and manipulate the set of views generated, and yields results technically compatible with the Dali Workbench but with improved presentation and layout.},
  Url                      = {http://www.sei.cmu.edu/reports/03tn008.pdf}
}

@Article{2008:jsmerp:okeeffe,
  Title                    = {Search-based refactoring: An empirical study},
  Author                   = {O'Keeffe, Mark and {\'O} Cinn{\'e}ide, Mel},
  Journal                  = jsmerp,
  Year                     = {2008},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {345--364},
  Volume                   = {20},

  Abstract                 = {Object-oriented systems that undergo repeated addition of functionality commonly suffer a loss of quality in their underlying design. This problem must often be remedied in a costly refactoring phase before further maintenance programming can take place. Recently search-based approaches to automating the task of software refactoring, based on the concept of treating object-oriented design as a combinatorial optimization problem, have been proposed. However, because search-based refactoring is a novel approach it is yet to be established as to which search techniques are most suitable for the task. In this paper we report the results of an empirical comparison of simulated annealing (SA), genetic algorithms (GAs) and multiple ascent hill-climbing (HCM) in search-based refactoring. A prototype automated refactoring tool is employed, capable of making radical changes to the design of an existing program in order that it conforms more closely to a contemporary quality model. Results show HCM to outperform both SA and GA over a set of five input programs.},
  Doi                      = {10.1002/smr.v20:5}
}

@InProceedings{2003:iwpse:oreilly,
  Title                    = {Lightweight Prevention of Architectural Erosion},
  Author                   = {O'Reilly, Ciaran and Morrow, Philip and Bustard, David},
  Booktitle                = iwpse,
  Year                     = {2003},
  Pages                    = {59--64},

  Abstract                 = {Avoiding architectural erosion helps extend the lifetime of an evolving software systemErosion can be reduced by ensuring that (i) developers share a good understanding of a system's architecture; (ii) alignment is preserved between the architectural description and its implementation at all stages of system construction and maintenance; and (iii) architectural changes are treated with the same care and attention as the production of the initial design. Through the metaphor of agile development' this paper presents a lightweight approach to the control of architectural erosion. In particular, it covers the representation of an architectural description and the management of alignment between description and implementation during system evolution. A prototype support tool, ArchAngel, is introduced. This maintains an architectural design description, identifies when changes occur with respect to that description, and reports these changes for evaluation.},
  Doi                      = {10.1109/IWPSE.2003.1231211}
}

@InProceedings{2005:maple:oancea,
  Title                    = {Generalization in {M}aple},
  Author                   = {Cosmin Oancea and Clare So and Stephen M. Watt},
  Booktitle                = maple,
  Year                     = {2005},
  Pages                    = {277--382},

  Abstract                 = {We explore the notion of generalization in the setting of symbolic mathematical computing. By ``generalization" we mean the process of taking a number of instances of mathematical expressions and producing new expressions that may be specialized to all the instances. We first identify a number of ways in which generalization may be useful in the setting of computer algebra, and formalize this generalization as an antiunification problem. We present a single-pass algorithm for antiunification and give some examples.},
  Url                      = {http://www.csd.uwo.ca/~watt/pub/reprints/2005-mc-gen.pdf}
}

@Misc{2010:misc:uml,
  Title                    = {{UML} Superstructure, Version 2.3},

  Author                   = {{Object Modeling Group}},
  HowPublished             = {http://www.omg.org/spec/UML/2.3/Superstructure/PDF},
  Month                    = may,
  Year                     = {2010},

  Url                      = {http://www.omg.org/spec/UML/2.3/Superstructure/PDF}
}

@Article{1993:jss:offutt,
  Title                    = {A software metric system for module coupling},
  Author                   = {A. Offutt and M. Harrold and P. Kolte},
  Journal                  = jss,
  Year                     = {1993},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {295--308},
  Volume                   = {20},

  Abstract                 = {Low module coupling is considered to be a desirable quality for modular programs to have. Previously, coupling has been defined subjectively and not quantified, making it difficult to use in practice. In this article, we extend previous work to reflect newer programming languages and quantify coupling by developing a general software metric system that allows us to automatically measure coupling. We have precisely defined the levels of coupling so that they can be determined algorithmically, incorporated the notion of direction into the coupling levels, and accounted for different types of nonlocal variables present in modern programming languages. With our system, we can measure the coupling between all pairs of modules in a system, measure the coupling of a particular module with all other modules in a system, and measure the coupling of an entire system. We have implemented our metric system so that it measures the coupling between pairs of procedures in arbitrary C programs and have analyzed several well-used systems of various sizes.},
  Doi                      = {10.1016/0164-1212(93)90072-6}
}

@InProceedings{2006:icicic:ohno,
  Title                    = {Measuring Source Code Similarity Using Reference Vectors},
  Author                   = {Ohno, Asako and Murao, Hajime},
  Booktitle                = icicic,
  Year                     = {2006},
  Pages                    = {92--95},
  Volume                   = {2},

  Abstract                 = {This paper discusses on a method of measuring similarities between program source codes. Unlike many of existing similarity measuring method we do not compare a pair of source codes directly but compare them indirectly with using reference source codes. Using reference vectors calculated from each source codes and reference source codes reduced considerable amount of computation time for similarity measurement. To examine our method, we built a system implemented this algorithm and made computational experiments on Java program source codes submitted as assignments for a programming class. From results, we confirmed there are evident similarities between program source codes have close reference vectors each other}
}

@InProceedings{2003:esec_fse:ohst,
  Title                    = {Differences between versions of {UML} diagrams},
  Author                   = {Ohst, Dirk and Welle, Michael and Kelter, Udo},
  Booktitle                = esec_fse,
  Year                     = {2003},
  Pages                    = {227--236},

  Abstract                 = {This paper addresses the problem of how to detect and visualise differences between versions of {UML} documents such as class or object diagrams. Our basic approach for showing the differences between two documents is to use a unified document which contains the common and specific parts of both base documents; the specific parts are highlighted. The main problems are (a) how to abstract from modifications done to the layout and other (document type-specific) details which are considered irrelevant; (b) how to deal with structural changes such as the shifting of an operation from one class to another; (c) how to reduce the amount of highlighted information. Our approach is based on the assumption that software documents are modelled in a fine-grained way, i.e. they are stored as syntax trees in XML files or in a repository system, and that the version management system supports fine-grained data. Our difference computation algorithm detects structural changes and enables their appropriate visualisation. Highlighting can be restricted on the basis of the types of the elements and on the basis of the revision history, e.g. only changes which occurred during a particular editing session are highlighted.},
  Doi                      = {10.1145/940071.940102}
}

@InProceedings{2007:icsm:oliveto,
  Title                    = {Software Artefact Traceability: The Never-Ending Challenge},
  Author                   = {Rocco Oliveto and Giuliano Antoniol and Andrian Marcus and Jane Huffman Hayes},
  Booktitle                = icsm,
  Year                     = {2007},
  Pages                    = {485--488},

  Abstract                 = {Software artefact traceability is widely recognised as an important factor for the effective development and maintenance of a software system. Unfortunately, the lack of automatic or semi-automatic supports makes the task of maintaining links among software artefacts a tedious and time consuming one. For this reason, often traceability information becomes out of date or it is completely absent during software development. In this working session, we discuss problems and challenges related to various aspects of traceability in software systems.}
}

@Article{2005:tse:van_ommering,
  Title                    = {Software Reuse in Product Populations},
  Author                   = {van Ommering, Rob},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {537--550},
  Volume                   = {31},

  Abstract                 = {Consumer products are becoming increasingly software intensive. The software complexity of individual products grows, while the diversity of products increases and the lead time must decrease. Software reuse is the answer to this, not only within a family but also between families of consumer products. We have devised an approach based upon a software component technology to enable reuse. This paper describes that approach, and it zooms in on two important aspects of component-based development. One aspect concerns the prediction of system properties from properties of components, which we illustrate using thread synchronization as example. The other aspect concerns branching of our software in our configuration management systems, where our analysis leads to the discovery that we may be constantly rewriting our own code and to the definition of the turn-over factor to quantify this. We end this paper with a brief validation of our approach.},
  Doi                      = {10.1109/TSE.2005.84}
}

@Article{2012:machlearn:ontanon,
  Title                    = {Similarity measures over refinement graphs},
  Author                   = {Onta{\~n}{\'o}n, Santiago and Plaza, Enric},
  Journal                  = machlearn,
  Year                     = {2012},

  Month                    = apr,
  Number                   = {1},
  Pages                    = {57--92},
  Volume                   = {87},

  Abstract                 = {Similarity also plays a crucial role in support vector machines. Similarity assessment plays a key role in lazy learning methods such as k-nearest neighbor or case-based reasoning. In this paper we will show how refinement graphs, that were originally introduced for inductive learning, can be employed to assess and reason about similarity. We will define and analyze two similarity measures, $S_{\lambda}$ and $S_{\pi}$, based on refinement graphs. The anti-unification-based similarity, $S_{\lambda}$, assesses similarity by finding the anti-unification of two instances, which is a description capturing all the information common to these two instances. The property-based similarity, $S_{\pi}$, is based on a process of disintegrating the instances into a set of properties, and then analyzing these property sets. Moreover these similarity measures are applicable to any representation language for which a refinement graph that satisfies the requirements we identify can be defined. Specifically, we present a refinement graph for feature terms, in which several languages of increasing expressiveness can be defined. The similarity measures are empirically evaluated on relational data sets belonging to languages of different expressiveness.},
  Doi                      = {10.1007/s10994-011-5274-3}
}

@PhdThesis{1992:thesis:opdyke,
  Title                    = {Refactoring Object-Oriented Frameworks},
  Author                   = {William F. Opdyke},
  School                   = {University of Illinois at Urbana-Champaign},
  Year                     = {1992},

  Address                  = {Urbana, Illinois, USA},

  Abstract                 = {This thesis defines a set of program restructuring operations (refactorings) that support the design, evolution and reuse of object-oriented application frameworks. The focus of the thesis is on automating the refactorings in a way that preserves the behavior of a program. The refactorings are defined to be behavior preserving, provided that their preconditions are met. Most of the refactorings are simple to implement and it is almost trivial to show that they are behavior preserving. However, for a few refactorings, one or more of their preconditions are in general undecidable. Fortunately, for some cases it can be determined whether these refactorings can be applied safely. Three of the most complex refactorings are defined in detail: generalizing the inheritance hierarchy, specializing the inheritance hierarchy and using aggregations to model the relationships among classes. These operations are decomposed into more primitive parts, and the power of these operations is discussed from the perspectives of automatability and usefulness in supporting design. Two design constraints needed in refactoring are class invariants and exclusive components. These constraints are needed to ensure that behavior is preserved across some refactorings. This thesis gives some conservative algorithms for determining whether a program satisfies these constraints, and describes how to use this design information to refactor a program.},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.914&rep=rep1&type=pdf}
}

@InProceedings{2005:woda:orso,
  Title                    = {Selective capture and replay of program executions},
  Author                   = {Orso, Alessandro and Kennedy, Bryan},
  Booktitle                = woda,
  Year                     = {2005},
  Pages                    = {1--7},

  Abstract                 = {In this paper, we present a technique for selective capture and replay of program executions. Given an application, the technique allows for (1) selecting a subsystem of interest, (2) capturing at runtime all the interactions between such subsystem and the rest of the application, and (3) replaying the recorded interactions on the subsystem in isolation. The technique can be used in several scenarios. For example, it can be used to generate test cases from users' executions, by capturing and collecting partial executions in the field. For another example. it can be used to perform expensive dynamic analyses off-line. For yet another example, it can be used to extract subsystem or unit tests from system tests. Our technique is designed to be efficient, in that we only capture information that is relevant to the considered execution. To this end, we disregard all data that, although flowing through the boundary of the subsystem of interest, do not affect the execution. In the paper, we also present a preliminary evaluation of the technique performed using SCARPE, a prototype tool that implements our approach.},
  Doi                      = {10.1145/1082983.1083251}
}

@InProceedings{2001:icse:tarr,
  Title                    = {{Hyper/J}: Multi-dimensional separation of concerns for {J}ava},
  Author                   = {Ossher, Harold and Tarr, Peri},
  Booktitle                = icse,
  Year                     = {2001},
  Pages                    = {729--730},

  Abstract                 = {Hyper/J supports a new approach to constructing, integrating and evolving software, called multi-dimensional separation of concerns. Developers can decompose and organize code and other artifacts according to multiple arbitrary criteria (concerns) simultaneously---even after the software has been implemented---and synthesize or integrate the pieces into larger-scale components and systems. Hyper/J facilitates several common development and evolution activities non-invasively, including: adaptation and customization, mix-and-match of features, reconciliation and integration of multiple domain models, reuse, product line management, extraction or replacement of existing parts of software, and on-demand remodularization. Hyper/J works with standard Java software, not requiring special compilers or environments. This demonstration shows it in action in a number of software engineering scenarios at different stages of the software life-cycle.},
  Doi                      = {10.1109/ICSE.2001.919190}
}

@InProceedings{2010:msr:ossher,
  Title                    = {Automated dependency resolution for open source software},
  Author                   = {Ossher, Joel and Bajracharya, Sushil and Lopes, Cristina},
  Booktitle                = msrwc,
  Year                     = {2010},
  Pages                    = {130--140},

  Abstract                 = {Opportunities for software reuse are plentiful, thanks in large part to the widespread adoption of open source processes and the availability of search engines for locating relevant artifacts. One challenge presented by open source software reuse is simply getting a newly downloaded artifact to build/run in the first place. The artifact itself likely reuses other artifacts, and so depends on their being located to function properly. While merely tedious in the individual case, this can cause serious difficulties for those seeking to study open source software. It is simply not feasible to manually resolve dependencies for thousands of projects, and many forms of analysis require declarative completeness. In this paper we present a method for automatically resolving dependencies for open source software. It works by cross-referencing a project's missing type information with a repository of candidate artifacts. We have implemented this method on top of the Sourcerer, an infrastructure for the large-scale indexing and analysis of open source code. The performance of our resolution algorithm was evaluated in two parts. First, for a small number of popular open source projects, we manually examined the artifacts suggested by our system to determine if they were appropriate. Second, we applied the algorithm to the 13,241 projects in the Sourcerer managed repository to evaluate the rate of resolution success. The results demonstrate the feasibility of this approach, as the algorithm located all of the required artifacts needed by 3,904 additional projects, increasing the percentage of declaratively complete projects in Sourcerer from 39\% to 69\%.},
  Doi                      = {10.1109/MSR.2010.5463346}
}

@Article{1992:tosem:ostertag,
  Title                    = {Computing similarity in a reuse library system: An {AI}-based approach},
  Author                   = {Ostertag, Eduardo and James Hendler and Rub{\'e}n Prieto-D{\'\i}az and Christine Braun},
  Journal                  = tosem,
  Year                     = {1992},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {205--228},
  Volume                   = {1},

  Abstract                 = {This paper presents an AI based library system for software reuse, called AIRS, that allows a developer to browse a software library in search of components that best meet some stated requirement. A component is described by a set of (feature, term) pairs. A feature represents a classification criterion, and is defined by a set of related terms. The system allows to represent packages (logical units that group a set of components) which are also described in terms of features. Candidate reuse components and packages are selected from the library based on the degree of similarity between their descriptions and a given target description. Similarity is quantified by a nonnegative magnitude (distance) proportional to the effort required to obtain the target given a candidate. Distances are computed by comparator functions based on the subsumption, closeness, and package relations. We present a formalization of the concepts on which the AIRS system is based. The functionality of a prototype implementation of the AIRS system is illustrated by application to two different software libraries: a set of Ada packages for data structure manipulation, and a set of C components for use in Command, Control, and Information Systems. Finally, we discuss some of the ideas we are currently exploring to automate the construction of AIRS classification libraries.},
  Doi                      = {10.1145/131736.131739}
}

@InProceedings{2002:issta:ostrand,
  Title                    = {The distribution of faults in a large industrial software system},
  Author                   = {Ostrand, Thomas J. and Weyuker, Elaine J.},
  Booktitle                = issta,
  Year                     = {2002},
  Pages                    = {55--64},

  Abstract                 = {A case study is presented using thirteen releases of a large industrial inventory tracking system. Several types of questions are addressed in this study. The first involved examining how faults are distributed over the different files. This included making a distinction between the release during which they were discovered, the lifecycle stage at which they were first detected, and the severity of the fault. The second category of questions we considered involved studying how the size of modules affected their fault density. This included looking at questions like whether or not files with high fault densities at early stages of the lifecycle also had high fault densities during later stages. A third type of question we considered was whether files that contained large numbers of faults during early stages of development, also had large numbers of faults during later stages, and whether faultiness persisted from release to release. Finally, we examined whether newly written files were more fault-prone than ones that were written for earlier releases of the product. The ultimate goal of this study is to help identify characteristics of files that can be used as predictors of fault-proneness, thereby helping organizations determine how best to use their testing resources.}
}

@PhdThesis{2011:phd:perez,
  Title                    = {Refactoring Planning for Design Smell Correction in Object-Oriented Software},
  Author                   = {Javier P{\'e}rez},
  School                   = {University of Valladolid},
  Year                     = {2011},

  Abstract                 = {The evolution of a software system often implies a certain degree of deterioration of the system's structure. This mainly happens because maintenance efforts concentrate more on bug correction and the addition of new functionality, than on the control and improvement of the system's architecture and design. Bad design practices, often due to inexperience, insufficient knowledge or time pressure, are at the origin of design smells. Design smells are problems encountered in the system's structure which do not produce compile or run-time errors, but negatively affect software quality factors. Correcting or, at least, reducing design smells can improve software quality. Refactoring is a key technique in software evolution. It can be used to improve the structure and quality of a software system without changing its observable behaviour. Consequently, refactoring also seems to be the most adequate technique to correct design smells. Most design smell management approaches focus on suggesting which are the best redesign changes to perform, and which are the best structures to remedy a smell in order to recover the original design intent. These suggestions are often given in terms of refactorings. One of the difficulties in applying refactoring operations is that it is rare that the preconditions of the desired refactorings could be fulfilled by the system's source code in its current state. Therefore, the developer has to plan ahead and apply a significant amount of additional changes to solve this problem. This is a recurring issue appearing when a refactoring process pursues a complex objective, such as correcting design smells. This PhD Thesis Dissertation is aimed at improving the automation of the refactoring activity when it is oriented to the correction of design smells. The approach is based on the definition of refactoring strategies and the instantiation of refactoring plans from them. Refactoring strategies are specifications of complex refactoring sequences aimed at a certain goal, such as the correction of design smells, that can be easily automated. Refactoring plans are refactoring sequences, instantiated from refactoring strategies, that can be applied to the system to effectively achieve that goal. Refactoring strategies and refactoring plans allow the necessary preparatory refactorings to be computed, thus helping the developer to circumvent the violation of refactoring preconditions. Automated planning is an artificial intelligence branch that seeks to generate sequences of actions that will achieve a certain goal when they are performed. The study presented in this document demonstrates that automated planning is a suitable technique for instantiating refactoring plans and thus, for supporting the problem of performing complex refactoring processes. We have found that hierarchical task network planning provides, among all the existing planning approaches, the best balance between search-based and procedural-based strategies for the problem of refactoring planning. Therefore, this research introduces the generation of refactoring plans---based on hierarchical task network planning---as an approach to support the automation of the design smells correction activity.},
  Url                      = {http://www.giro.infor.uva.es/Publications/2011/Per11/thesis_perez_english.pdf}
}

@InProceedings{2009:iwpse_evol:perez,
  Title                    = {Perspectives on automated correction of bad smells},
  Author                   = {P{\'e}rez, Javier and Crespo, Yania},
  Booktitle                = iwpse_evol,
  Year                     = {2009},
  Pages                    = {99--108},

  Abstract                 = {Keeping a software system conformant with a desired architecture and consistent with good design principles is a recurring task during the software evolution process. Deviations from good design principles can manifest in the form of bad smells: problems in the system's structure that can negatively affect software quality factors. Many authors have worked in identifying bad smells and in removing them with refactorings: tools have been built to suggest refactorings; successful approaches to detect bad smells have been developed, etc.. We present a comprehensive and historical review on this subject, in order to model the current state of the art and to identify the open challenges, current trends and research opportunities. We also propose a technique based on automated planning, aimed at taking one step forward in the automatic improvement of a system's structure. This proposal will allow computing complex refactoring sequences which can be directed to the achievement of a certain objective, such as the correction of bad smells.},
  Doi                      = {10.1145/1595808.1595827}
}

@InProceedings{2005:ecoop:pacheco,
  Title                    = {Eclat: Automatic generation and classification of test inputs},
  Author                   = {Carlos Pacheco and Michael D. Ernst},
  Booktitle                = ecoop,
  Year                     = {2005},
  Pages                    = {504--527},

  Abstract                 = {This paper describes a technique that selects, from a large set of test inputs, a small subset likely to reveal faults in the software under test. The technique takes a program or software component, plus a set of correct executions---say, from observations of the software running properly, or from an existing test suite that a user wishes to enhance. The technique first infers an operational model of the software's operation. Then, inputs whose operational pattern of execution differs from the model in specific ways are suggestive of faults. These inputs are further reduced by selecting only one input per operational pattern. The result is a small portion of the original inputs, deemed by the technique as most likely to reveal faults. Thus, the technique can also be seen as an error-detection technique. The paper describes two additional techniques that complement test input selection. One is a technique for automatically producing an oracle (a set of assertions) for a test input from the operational model, thus transforming the test input into a test case. The other is a classification-guided test input generation technique that also makes use of operational models and patterns. When generating inputs, it filters out code sequences that are unlikely to contribute to legal inputs, improving the efficiency of its search for fault-revealing inputs. We have implemented these techniques in the Eclat tool, which generates unit tests for Java classes. Eclat's input is a set of classes to test and an example program execution---say, a passing test suite. Eclat's output is a set of JUnit test cases, each containing a potentially fault-revealing input and a set of assertions at least one of which fails. In our experiments, Eclat successfully generated inputs that exposed fault-revealing behavior; we have used Eclat to reveal real errors in programs. The inputs it selects as fault-revealing are an order of magnitude as likely to reveal a fault as all generated inputs.}
}

@InProceedings{2007:icse:pacheco,
  Title                    = {Feedback-Directed Random Test Generation},
  Author                   = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ernst, Michael D. and Ball, Thomas},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {75--84},

  Abstract                 = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
  Doi                      = {10.1109/ICSE.2007.37}
}

@Article{2007:sttt:pahl,
  Title                    = {An ontology for software component matching},
  Author                   = {Claus Pahl},
  Journal                  = sttt,
  Year                     = {2007},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {169--178},
  Volume                   = {9},

  Abstract                 = {Matching is a central activity in the discovery and assembly of reusable software components. We investigate how ontology technologies can be utilised to support software component development. We use description logics, which underlie Semantic Web ontology languages, such as OWL, to develop an ontology for matching requested and provided components. A link between modal logic and description logics will prove invaluable for the provision of reasoning support for component behaviour.},
  Doi                      = {10.1007/s10009-006-0015-9}
}

@InProceedings{2010:aosd:palix,
  Title                    = {Tracking code patterns over multiple software versions with {H}erodotos},
  Author                   = {Palix, Nicolas and Lawall, Julia and Muller, Gilles},
  Booktitle                = aosd,
  Year                     = {2010},
  Pages                    = {169--180},

  Abstract                 = {An important element of understanding a software code base is to identify the repetitive patterns of code it contains and how these evolve over time. Some patterns are useful to the software, and may be modularized. Others are detrimental to the software, such as patterns that represent defects. In this case, it is useful to study the occurrences of such patterns, to identify properties such as when and why they are introduced, how long they persist, and the reasons why they are corrected. To enable studying pattern occurrences over time, we propose a tool, Herodotos, that semi-automatically tracks pattern occurrences over multiple versions of a software project, independent of other changes in the source files. Guided by a user-provided configuration file, Herodotos builds various graphs showing the evolution of the pattern occurrences and computes some statistics. We have evaluated this approach on the history of a representative range of open source projects over the last three years. For each project, we track several kinds of defects that have been found by pattern matching. This tracking is done automatically in 99\% of the occurrences. The results allow us to compare the evolution of the selected projects and defect kinds over time.},
  Doi                      = {10.1145/1739230.1739250}
}

@InProceedings{2012:rsse:palma,
  Title                    = {Recommendation System for Design Patterns in Software Development: An [sic] {DPR} Overview},
  Author                   = {Francis Palma and Hadi Farzin and Yann-Ga{\"e}l Gu{\'e}h{\'e}neuc and Naouel Moha},
  Booktitle                = rsse,
  Year                     = {2012},

  Abstract                 = {Software maintenance can become monotonous and expensive due to ignorance and misapplication of appropriate design patterns during the early phases of design and development. To have a good and reusable system, designers and developers must be aware of large information set and many quality concerns, e.g., design patterns. Systems with correct design pattern may ensure easy maintenance and evolution. However, without assistance, designing and development of software systems following certain design patterns is difficult for engineers. Recommendation systems for software engineering can assist designers and developers with a wide range of activities including suggesting design patterns. With the help of pattern recommenders, designers can come up with a reusable design. We provide a Design Pattern Recommender (DPR) process overview for software design to suggest design patterns, based on a simple Goal-Question-Metric (GQM) approach. Our prototype provides two-fold solution. In the primary-level, DPR only proposes one or more design patterns for a problem context, and in the secondary level, for a initial set of design, DPR refactors models and suggests design patterns. Our preliminary evaluation shows that DPR has a good trade-off between accuracy and procedural complexity, comparing to other state-of-the-art approaches.},
  Doi                      = {10.1109/RSSE.2012.6233399}
}

@InProceedings{2004:fse:pan,
  Title                    = {Relevancy based semantic interoperation of reuse repositories},
  Author                   = {Pan, Ying and Wang, Lei and Zhang, Lu and Xie, Bing and Yang, Fuqing},
  Booktitle                = fse,
  Year                     = {2004},
  Pages                    = {211--220},

  Abstract                 = {Software reuse is a promising solution to the software crisis. Reuse repositories are the basic infrastructure for software reuse. During the past decade, various academic, commercial, governmental, and industrial organizations have developed many Internet-enabled reuse repositories to provide access to software components and related resources. It has necessitated semantic interoperation to allow distributed maintenance and management of these repositories while enabling users to efficiently and conveniently access resources from multiple reuse repositories via a single representation view. In this paper, we have proposed an approach to enhancing the semantic interoperability of reuse repositories, called the improved relevancy matching and ranking (IRMR) method, based on analyzing the correlation of terms in representation methods of the repositories. A prototype system, the Virtual Repository supporting Semantic Interoperation (VRSI), is presented to illustrate the application of this approach to support the semantic interoperation of reuse repositories. Experimental results on real world reuse repositories demonstrated significant improvement in terms of searching effectiveness.},
  Doi                      = {10.1145/1029894.1029924}
}

@Article{1995:cacm:pancake,
  Title                    = {The Promise and the Cost of Object Technology: A Five-Year Forecast},
  Author                   = {Pancake, Cherri M.},
  Journal                  = cacm,
  Year                     = {1995},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {32--49},
  Volume                   = {38},

  Doi                      = {10.1145/226239.226247}
}

@Article{2007:pnas:park,
  Title                    = {Distribution of node characteristics in complex networks},
  Author                   = {Juyong Park and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = pnas,
  Year                     = {2007},

  Month                    = {13 } # nov,
  Number                   = {46},
  Pages                    = {17016--17020},
  Volume                   = {104},

  Abstract                 = {Our enhanced ability to map the structure of various complex networks is increasingly accompanied by the possibility of independently identifying the functional characteristics of each node. Although this led to the observation that nodes with similar characteristics have a tendency to link to each other, in general we lack the tools to quantify the interplay between node properties and the structure of the underlying network. Here we show that when nodes in a network belong to two distinct classes, two independent parameters are needed to capture the detailed interplay between the network structure and node properties. We find that the network structure significantly limits the values of these parameters, requiring a phase diagram to uniquely characterize the configurations available to the system. The phase diagram shows a remarkable independence from the network size, a finding that, together with a proposed heuristic algorithm, allows us to determine its shape even for large networks. To test the usefulness of the developed methods, we apply them to biological and socioeconomic systems, finding that protein functions and mobile phone usage occupy distinct regions of the phase diagram, indicating that the proposed parameters have a strong discriminating power.},
  Doi                      = {10.1073/pnas.0705081104}
}

@InProceedings{2012:msr:park,
  Title                    = {An empirical study of supplementary bug fixes},
  Author                   = {Jihun Park and Miryung Kim and Baishakhi Ray and Doo-Hwan Bae},
  Booktitle                = msrwc,
  Year                     = {2012},
  Pages                    = {40--49},

  Doi                      = {10.1109/MSR.2012.6224298}
}

@InProceedings{1994:icse:parnas,
  Title                    = {Software aging},
  Author                   = {Parnas, David Lorge},
  Booktitle                = icse,
  Year                     = {1994},
  Pages                    = {279--287},

  Abstract                 = {Programs, like people, get old. We can't prevent aging, but we can understand its causes, take steps to limits its effects, temporarily reverse some of the damage it has caused, and prepare for the day when the software is no longer viable. A sign that the Software Engineering profession has matured will be that we lose our preoccupation with the first release and focus on the long term health of our products. Researchers and practitioners must change their perception of the problems of software development. Only then will Software Engineering deserve to be called Engineering.},
  Doi                      = {10.1109/ICSE.1994.296790}
}

@Article{1979:tse:parnas,
  Title                    = {Designing software for ease of extension and contraction},
  Author                   = {David L. Parnas},
  Journal                  = tse,
  Year                     = {1979},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {128--138},
  Volume                   = {5},

  Abstract                 = {Designing software to be extensible and easily contracted is discussed as a special case of design for change. A number of ways that extension and contraction problems manifest themselves in current software are explained. Four steps in the design of software that is more flexible are then discussed. The most critical step is the design of a software structure called the ``uses" relation. Some criteria for design decisions are given and illustrated using a small example. It is shown that the identification of minimal subsets and minimal extensions can lead to software that can be tailored to the needs of a broad variety of users.},
  Doi                      = {10.1109/TSE.1979.234169}
}

@Article{1976:tse:parnas,
  Title                    = {On the design and development of program families},
  Author                   = {D. L. Parnas},
  Journal                  = tse,
  Year                     = {1976},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {1--9},
  Volume                   = {2},

  Abstract                 = {Program families are defined (analogously to hardware families) as sets of programs whose common properties are so extensive that it is advantageous to study the common properties of the programs before analyzing individual members. The assumption that, if one is to develop a set of similar programs over a period of time, one should consider the set as a whole while developing the first three approaches to the development, is discussed. A conventional approach called ``sequential development" is compared to ``stepwise refinement" and ``specification of information hiding modules." A more detailed comparison of the two methods is then made. By means of several examples it is demonstrated that the two methods are based on the same concepts but bring complementary advantages.},
  Doi                      = {10.1109/TSE.1976.233797}
}

@Article{1972:cacm:parnas,
  Title                    = {On the criteria to be used in decomposing systems into modules},
  Author                   = {Parnas, D. L.},
  Journal                  = cacm,
  Year                     = {1972},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {1053--1058},
  Volume                   = {15},

  Abstract                 = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a ``modularization" is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
  Doi                      = {10.1145/361598.361623}
}

@Article{1975:cacm:parnas,
  Title                    = {Use of the concept of transparency in the design of hierarchically structured systems},
  Author                   = {Parnas, D. L. and Siewiorek, D. P.},
  Journal                  = cacm,
  Year                     = {1975},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {401--408},
  Volume                   = {18},

  Abstract                 = {This paper deals with the design of hierarchically structured programming systems. It develops a method for evaluating the cost of requiring programmers to work with an abstraction of a real machine. A number of examples from hardware and software are given as illustrations of the method.},
  Doi                      = {10.1145/360881.360913}
}

@InProceedings{2010:softviz:parnin,
  Title                    = {{CodePad}: Interactive spaces for maintaining concentration in programming environments},
  Author                   = {Parnin, Chris and Gorg, Carsten and Rugaber, Spencer},
  Booktitle                = softviz,
  Year                     = {2010},
  Pages                    = {15--24},

  Abstract                 = {When software developers work with a program's source code, the structure of the source code often requires that they split their attention simultaneously across several documents and artifacts. Disruptions to programmers' concentration caused by overwhelmed capacity can then lead to programming errors and increases in the time to perform a task. We suggest the addition of peripheral interactive spaces to programming environments for supporting developers in maintaining their concentration. We introduce the novel concept of a CodePad, a peripheral, multi-touch enabled display that allows developers to engage with and manipulate multiple programming artifacts. We illustrate visualizations built for a CodePad that support multiple development scenarios and we describe how developers can coordinate the interaction and communication between a CodePad and a programming environment in personal and collaborative tasks. Additionally, we propose a design space for other visualization tools and detail our initial prototype.},
  Doi                      = {10.1145/1879211.1879217}
}

@Article{2004:tse:parsons,
  Title                    = {Cognitive heuristics in software engineering: Applying and extending anchoring and adjustment to artifact reuse},
  Author                   = {Jeffrey Parsons and Chad Saunders},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {873--888},
  Volume                   = {30},

  Abstract                 = {The extensive literature on reuse in software engineering has focused on technical and organizational factors, largely ignoring cognitive characteristics of individual developers. Despite anecdotal evidence that cognitive heuristics play a role in successful artifact reuse, few empirical studies have explored this relationship. This paper proposes how a cognitive heuristic, called anchoring, and the resulting adjustment bias can be adapted and extended to predict issues that might arise when developers reuse code and/or designs. The research proposes that anchoring and adjustment can be manifested in three ways: propagation of errors in reuse artifacts, failure to include requested functionality absent from reuse artifacts, and inclusion of unrequested functionality present in reuse artifacts. Results from two empirical studies are presented. The first study examines reuse of object classes in a programming task, using a combination of practicing programmers and students. The second study uses a database design task with student participants. Results from both studies indicate that anchoring occurs. Specifically, there is strong evidence that developers tend to use the extraneous functionality in the artifacts they are reusing and some evidence of anchoring to errors and omissions in reused artifacts. Implications of these findings for both practice and future research are explored.},
  Doi                      = {10.1109/TSE.2004.94}
}

@InProceedings{1992:icse:patel,
  Title                    = {A measure for composite module cohesion},
  Author                   = {Sukesh Patel and William Chu and Rich Baxter},
  Booktitle                = icse,
  Year                     = {1992},
  Pages                    = {38--48},

  Abstract                 = {An important software design activity is the deomposition of complex systems into conceptually independent modules that cooperate to achieve a desired result. This modularization represents a significant software engineerin activity that continues to receive considerable research attention. This paper illustrates how software may be modularized by automatically determining the cohesiveness of modules in the system. Module cohesion is defined to be a quality attribute that seeks to measure the singleness of purpose of a module. We propose a metric that measures the cohesion of individual subprograms of a software system as related to each other. This metric is illustrated with detailed examples and is supported with empirical evidence supporting the viability of the measure.}
}

@InProceedings{1992:cascon:paul,
  Title                    = {{SCRUPLE}: A reengineer's tool for source code search},
  Author                   = {Santanu Paul},
  Booktitle                = cascon,
  Year                     = {1992},
  Pages                    = {329--346},

  Abstract                 = {For software maintainers and reengineers confronted with the task of locating an interesting section of source code, a slow, painstaking scan of the source code using grep like tools is often the only available option. Similar problems arise in code optimization and program understanding. To alleviate the situation, we have in the past proposed a scheme for defining pattern languages using which one can specify interesting code features [13]. This paper addresses the automatic detection of source code sections that fit patterns described using such languages. We have built prototypes of our runtime system for C and PL/AS (an IBM internal programming language). The key to efficient, automatic search lies in the representations of the source code and the pattern being searched. A domain-independent pattern detection algorithm based on these representations is described. A prototype of the system was also built to search for simple patterns in English text, which demonstrates the generality of the pattern language concept as well as the underlying pattern detection mechanism.}
}

@Article{2011:ijrr:pavlic,
  Title                    = {Generalizing foraging theory for analysis and design},
  Author                   = {Pavlic, Theodore P and Passino, Kevin M},
  Journal                  = ijrr,
  Year                     = {2011},

  Month                    = apr,
  Number                   = {5},
  Pages                    = {505--523},
  Volume                   = {30},

  Abstract                 = {Foraging theory has been the inspiration for several decision-making algorithms for task-processing agents facing random environments. As nature selects for foraging behaviors that maximize lifetime calorie gain or minimize starvation probability, engineering designs are favored that maximize returned value (e.g. profit) or minimize the probability of not reaching performance targets. Prior foraging-inspired designs are direct applications of classical optimal foraging theory (OFT). Here, we describe a generalized optimization framework that encompasses the classical OFT model, a popular competitor, and several new models introduced here that are better suited for some task-processing applications in engineering. These new models merge features of rate maximization, efficiency maximization, and risk-sensitive foraging while not sacrificing the intuitive character of classical OFT. However, the central contributions of this paper are analytical and graphical methods for designing decision-making algorithms guaranteed to be optimal within the framework. Thus, we provide a general modeling framework for solitary agent behavior, several new and classic examples that apply to it, and generic methods for design and analysis of optimal task-processing behaviors that fit within the framework. Our results extend the key mathematical features of optimal foraging theory to a wide range of other optimization objectives in biological, anthropological, and technological contexts.},
  Doi                      = {10.1177/0278364910396551}
}

@Article{1900:ptrsl:pearson,
  Title                    = {{I}. Mathematical Contributions to the Theory of Evolution---{VII}. On the Correlation of Characters not Quantitatively Measurable},
  Author                   = {Karl Pearson},
  Journal                  = ptrsl,
  Year                     = {1900},
  Pages                    = {1--47 {\&} 405},
  Volume                   = {195},

  Abstract                 = {In August, 1899, I presented a memoir to the Royal Society on the inheritance of coat-colour in the horse and of eye-colour in man, which was read November, 1899, and ultimately ordered to be published in the `Phil. Trans.' Before that memoir was printed, Mr. Yule's valuable memoir on Association was read, and, further, Mr. Leslie Bramley-Moore showed me that the theory of my memoir as given in \S 6 of the present memoir led to somewhat divergent results according to the methods of proportioning adopted. We therefore undertook a new investigation of the theory of the whole subject, which is embodied in the present memoir. The data involved in the paper on coat-colour in horses and eye-colour in man have all been recalculated, and that paper is nearly ready for presentation.* [* Since ordered to be printed in the `Phil. Trans.'] But it seemed best to separate the purely theoretical considerations from their application to special cases of inheritance, and accordingly the old memoir now reappears in two sections. The theory discussed in this paper was, further, the basis of a paper on the Law of Reversion with special reference to the Inheritance of Coat-colour in Basset Hounds recently communicated to the Society, and about to appear in the `Proceedings.' \dag [\dag Read January 25, 1900. `Roy. Soc. Proc.,' vol. 65, p. 140.] While I am responsible for the general outlines or the present paper, the rough draft of it was taken up and carried on in leisure momemnts by Mr. Leslie Bramley-Moore, Mr. L. N. G. Filon, M.A., and Miss Alice Lee, D.Sc. Mr. Bramley-Moore discovered the $n$-functions; Mr. Filon proved most of their general properties and the convergency of the series; I alone am responsible for sections 4, 5, and 6. Mr. Leslie Bramley-Moore sent me, without proof, on the eve of his departure for the Cape, the general expansion for $s$ on p. 26. I am responsible for the present proof and its applications. To Dr. Alice Lee we owe most of the illustrations and the table on p. 17. Thus the work is essentially a joint memoir in which we have equal part, and the use of the first personal pronoun is due to the fact that the material had to be put together and thrown into form by one of our number.---K. P.},
  Url                      = {http://www.jstor.org/stable/90764}
}

@Article{1999:tpami:pelillo,
  Title                    = {Matching Hierarchical Structures Using Association Graphs},
  Author                   = {Marcello Pelillo and Kaleem Siddiqi and Steven W. Zucker},
  Journal                  = tpami,
  Year                     = {1999},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {1105--1119},
  Volume                   = {21},

  Abstract                 = {It is well-known that the problem of matching two relational structures can be posed as an equivalent problem of linding a maximal clique in a (derived) ``association graph.'' However, it is not clear how to apply this approach to computer vision problems where the graphs are hierarchically organized, i.e., are trees, since maximal cliques are not constrained to preserve the partial order. Here, we provide a solution to the problem of matching two trees by constructing the association graph using the graph-theoretic concept of connectivity. We prove that, in the new formulation, there is a one-to-one correspondence between maximal cliques and maximal subtree isomorphisms. This allows us to cast the tree matching problem as an indefinite quadratic program using the Motzkin-Straw theorem, and we use ``replicator'' dynamical systems developed in theoretical biology to solve it. Such continuous solutions to discrete problems are attractive because they can motivate analog and biological implementations. The framework is also extended to the matching of attributed trees by using weighted association graphs. We illustrate the power of the approach by matching articulated and deformed shapes described by shock trees.},
  Doi                      = {10.1109/34.809105}
}

@InProceedings{2013:hpcs:pellegrini,
  Title                    = {Hijacker: Efficient static software instrumentation with applications in high performance computing},
  Author                   = {Pellegrini, Alessandro},
  Booktitle                = hpcs,
  Year                     = {2013},
  Pages                    = {650--655},

  Doi                      = {10.1109/HPCSim.2013.6641486}
}

@Article{1999:ijase:penix,
  Title                    = {Efficient Specification-Based Component Retrieval},
  Author                   = {John Penix and Perry Alexander},
  Journal                  = ijase,
  Year                     = {1999},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {139--170},
  Volume                   = {6},

  Abstract                 = {In this paper we present a mechanism for making specification-based component retrieval more efficient by limiting the amount of theorem proving required at query time. This is done by using a classification scheme to reduce the number of specification matching proofs that are required to process a query. Components are classified by assigning features that correspond to necessary conditions implied by the component specifications. We show how this method of feature assignment can be used to approximate reusability relationships between queries and library components. The set of possible classification features are formally defined, permitting automation of the classification process. The classification process itself is made efficient by using a specialized theorem proving tactic to prove feature implication. The retrieval mechanism was implemented and evaluated experimentally using a library of list manipulation components. The results indicate a better response time than existing formal approaches. The approach provides higher levels of consistency and automation than informal methods, with comparable retrieval performance.},
  Doi                      = {10.1023/A:1008766530096}
}

@InProceedings{2005:paste:perkins,
  Title                    = {Automatically Generating Refactorings to Support {API} Evolution},
  Author                   = {Jeff Perkins},
  Booktitle                = paste,
  Year                     = {2005},
  Pages                    = {111--114},

  Abstract                 = {When library APIs change, client code should change in response, in order to avoid erroneous behavior, compilation failures, or warnings. Previous research has introduced techniques for generating such client refactorings. This paper improves on the previous work by proposing a novel, lightweight technique that takes advantage of information that programmers can insert in the code rather than forcing them to use a different tool to re-express it. The key idea is to replace calls to deprecated methods by their bodies, where those bodies consist of the appropriate replacement code. This approach has several benefits. It requires no change in library development practice, since programmers already adjust method bodies and/or write example code, and there are no new tools or languages to learn. It does not require distribution of new artifacts, and a tool to apply it can be lightweight. We evaluated the applicability of our approach on a number of libraries and found it to to be applicable in more than 75\% of the cases.},
  Doi                      = {10.1145/1108792.1108818}
}

@InProceedings{2009:sosp:perkins,
  Title                    = {Automatically Patching Errors in Deployed Software},
  Author                   = {Perkins, Jeff H. and Kim, Sunghun and Larsen, Sam and Amarasinghe, Saman and Bachrach, Jonathan and Carbin, Michael and Pacheco, Carlos and Sherwood, Frank and Sidiroglou, Stelios and Sullivan, Greg and Wong, Weng-Fai and Zibin, Yoav and Ernst, Michael D. and Rinard, Martin},
  Booktitle                = sosp,
  Year                     = {2009},
  Pages                    = {87--102},

  Doi                      = {10.1145/1629575.1629585}
}

@Book{2003:book:perrone,
  Title                    = {J2EE Developer's Handbook},
  Author                   = {Paul J. Perrone and Venkata S. R. R. Chaganti and Tom Schwenk},
  Publisher                = {Sams Publishing},
  Year                     = {2003}
}

@Article{1992:sen:perry,
  Title                    = {Foundations for the study of software architecture},
  Author                   = {Perry, Dewayne E. and Wolf, Alexander L.},
  Journal                  = sen,
  Year                     = {1992},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {40--52},
  Volume                   = {17},

  Abstract                 = {The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architecture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements---that is, the constraints on the elements. The rationale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system requirements. We discuss the components of the model in the context of both architectures and architectural styles and present an extended example to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, summarizing our contributions, and relating our approach to other current work.},
  Doi                      = {10.1145/141874.141884}
}

@Article{2009:mt:peter,
  Title                    = {Refactoring Large Software Systems},
  Author                   = {Sibylle Peter and Sven Ehrke},
  Journal                  = mt,
  Year                     = {2009},

  Month                    = {Winter},
  Number                   = {4},
  Pages                    = {2--17},
  Volume                   = {17},

  Abstract                 = {Refactoring a software system means to refurbish it internally without interfering with its external properties. In his famous book Martin Fowler defines refactoring as follows: Refactoring is the process of changing a software system in such a way that it does not alter the external behavior of the code yet improves its internal structure. [Fowl, p. xvi] Fowler describes how to refactor code in a controlled and efficient manner. Although we frequently use these techniques to refactor large software systems, this article is not about refactoring techniques. It rather treats large scale refactoring of systems that suffer from being stuck in an infeasibility stage, which means that it is no longer possible to implement new features and that every bug fix leads to several new bugs. In this article we describe the different stages of a refactoring project. Before a refactoring project is actually started, an assessment and an in-depth analysis are made which result in a list of findings. Ideally, these findings will be translated into a master plan, which defines WHAT should be refactored. This master plan helps to keep the overall picture of the refactoring project in mind and to divide it into several sub-projects or refactoring parts. Once the master plan is defined we use an iterative approach to tackle the individual refactoring steps. Before describing this approach we will first have a look at the life path of applications in general, followed by the prerequisites for refactoring projects. WebInvest, our current refactoring project, will be used as example throughout the article. WebInvest is an advisory tool for investment relationship managers. It enables them to simulate trading and to analyze a customer's portfolio against investment strategies.},
  Url                      = {http://www.methodsandtools.com/archive/archive.php?id=98}
}

@InProceedings{1988:pldi:pfenning,
  Title                    = {Higher-order abstract syntax},
  Author                   = {Pfenning, Frank and Elliot, Conal},
  Booktitle                = pldi,
  Year                     = {1988},
  Pages                    = {199--208},

  Abstract                 = {We describe motivation, design, use, and implementation of higher-order abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higher-order abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higher-order abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.},
  Doi                      = {10.1145/53990.54010}
}

@Article{1995:annse:pfleeger,
  Title                    = {Experimental design and analysis in software engineering},
  Author                   = {Shari Lawrence Pfleeger},
  Journal                  = annse,
  Year                     = {1995},
  Number                   = {1},
  Pages                    = {219--253},
  Volume                   = {1},

  Abstract                 = {The paper presents key activities necessary for designing and analyzing an experiment in software engineering. After explaining how to choose an appropriate research technique to fit project goals, the paper shows how to state a hypothesis and determine how much control is needed over the variables involved. If control is not possible, then a formal experiment is not possible; a case study may be a better approach. Next, the six stages of an experiment (conception, design, preparation, execution, analysis and dissemination) are described, with design examined in detail. Design considerations such as replication, randomization and local control are discussed, and design techniques such as crossing and nesting are explained. Finally, data analysis is shown to be a function both of the experimental design and the distribution of the data. Throughout, examples are given to show how the techniques are interpreted and used in software engineering.},
  Doi                      = {10.1007/BF02249052}
}

@Article{1994:sen:pfleeger,
  Title                    = {Design and analysis in software engineering---{Part~1}: The language of case studies and formal experiments},
  Author                   = {Shari Lawrence Pfleeger},
  Journal                  = sen,
  Year                     = {1994},

  Month                    = oct,
  Number                   = {4},
  Pages                    = {16--20},
  Volume                   = {19},

  Abstract                 = {Software engineers have many questions to answer. The test team wants to know which technique is best for finding faults in code. Maintainers seek the best tool to support configuration management. Project managers try to determine what types of experience make the best programmers or designers, while designers look for models that are good at predicting reliability. To answer these and other questions, we often rely on the advice and experience of others, which is not always based on careful, scientific research [Fenton et al. 1994]. As a software practitioner, it is important to make key decisions or assessments in an objective and scientific way. So we need to know two things: what assessment techniques are available to us, and which should we use in a given situation? Will Tracz has asked me to write an on-going column in SIGSOFT Notes to address these issues. In this first article, I will explain some of the terminology to be used in subsequent articles. In future articles, I and my invited colleagues will try to point out some of the key items affecting decisions about how to do research and how to evaluate the research of others. In the course of doing this, we will also show how software engineering research sometimes differs from research in other fields. For instance, in medicine, it is usually easy to test a new drug by giving a placebo to the control group, But in software engineering we cannot do that---we cannot ask a group not to use a design technique; the group has to use some technique if it is to complete the design, and we end up losing control rather than having a carefully-controlled comparison. As always, your comments are welcome, preferably through electronic mail; I will try to address in future columns the major points you may raise in your queries.},
  Doi                      = {10.1145/190679.190680}
}

@InProceedings{2010:icse:pham,
  Title                    = {Detecting recurring and similar software vulnerabilities},
  Author                   = {Pham, Nam H. and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Wang, Xinying and Nguyen, Anh Tuan and Nguyen, Tien N.},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {227--230},
  Volume                   = {2},

  Abstract                 = {New software security vulnerabilities are discovered on almost daily basis and it is vital to be able to identify and resolve them as early as possible. Fortunately, many software vulnerabilities are recurring or very similar, thus, one could effectively detect and fix a vulnerability in a system by consulting the similar vulnerabilities and fixes from other systems. In this paper, we propose, SecureSync, an automatic approach to detect and provide suggested resolutions for recurring software vulnerabilities on multiple systems sharing/using similar code or API libraries. The core of SecureSync includes a usage model and a mapping algorithm for matching vulnerable code across different systems, a model for the comparison of vulnerability reports, and a tracing technique from a report to corresponding source code. Our preliminary evaluation with case studies showed the potential usefulness of SecureSync.},
  Doi                      = {10.1145/1810295.1810336}
}

@InProceedings{2004:csmr:ping,
  Title                    = {Refactoring {W}eb sites to the controller-centric architecture},
  Author                   = {Ping, Yu and Kontogiannis, Kostas},
  Booktitle                = csmr,
  Year                     = {2004},
  Pages                    = {204--213},

  Abstract                 = {A Web site is a hyperlinked network environment, which consists of hundreds of interconnected pages, usually without an engineered architecture. This is often a large, complex Web site that is difficult to understand and maintain. Here, we propose an approach that aims to restructure an existing Web site by adapting them to a controller-centric architecture. In particular, this approach is twofold. First, it defines a domain model to represent dependencies between Web pages in order to abstract current structure of the Web site. Second, it designs a system architecture as a reference model for restructuring the Web site to the new structure. These principles will be illustrated through a case study using a reengineering tool that implements the refactoring process for a JSP-based Web site.},
  Doi                      = {10.1109/CSMR.2004.1281421}
}

@InProceedings{2003:cascon:ping,
  Title                    = {Migration of legacy web applications to enterprise {J}ava environments : {Net.Data} to {JSP} transformation},
  Author                   = {Ping, Yu and Lu, Jianguo and Lau, Terence C. and Kontogiannis, Kostas and Tong, Tack and Yi, Bo},
  Booktitle                = cascon,
  Year                     = {2003},
  Pages                    = {223--237},

  Abstract                 = {As Web technologies advance, the porting and adaptation of existing Web applications to take advantage of the advancement has become an issue of increasing importance. Examples of such technology advancement include extensible architectural designs, more efficient caching protocols, and provision for customizable dynamic content delivery. This paper presents an experience report on the migration of legacy IBM Net.Data based applications to new enterprise Java environments. In this respect, a Net.Data application is refactored into JavaBeans (Model), JavaServer Pages (View), and Java Servlet (Controller). To evaluate the effectiveness of the migration methodology, a tool has been developed to support the automatic translation of Net.Data to JavaServer Pages. Using such a tool, a case study is presented to deal with IBM WebSphere Commerce applications.},
  Url                      = {http://dl.acm.org/citation.cfm?id=961358}
}

@InProceedings{2003:um:pirolli,
  Title                    = {{SNIF-ACT}: A model of information foraging on the {W}orld {W}ide {W}eb},
  Author                   = {Pirolli, Peter and Fu, Wai-Tat},
  Booktitle                = um,
  Year                     = {2003},
  Pages                    = {45--54},

  Abstract                 = {SNIF-ACT (Scent-based Navigation and Information Foraging in the ACT architecture) has been developed to simulate users as they perform unfamiliar information-seeking tasks on the World Wide Web (WWW). SNIF-ACT selects actions based on the measure of information scent, which is calculated by a spreading activation mechanism that captures the mutual relevance of the contents of a WWW page to the goal of the user. There are two main predictions of SNIF-ACT: (1) users working on unfamiliar tasks are expected to choose links that have high information scent, (2) users will leave a site when the information scent of the site diminishes below a certain threshold. SNIF-ACT produced good fits to data collected from four users working on two tasks each. The results suggest that the current content-based spreading activation SNIF-ACT model is able to generate useful predictions about complex user-WWW interactions.},
  Doi                      = {10.1007/3-540-44963-9_8}
}

@Article{1983:isr:plackett,
  Title                    = {Karl {P}earson and the chi-squared test},
  Author                   = {Plackett, R. L.},
  Journal                  = isr,
  Year                     = {1983},

  Month                    = apr,
  Number                   = {1},
  Pages                    = {59--72},
  Volume                   = {51},

  Abstract                 = {Pearson's paper of 1900 introduced what subsequently became known as the chi-squared test of goodness of fit. The terminology and allusions of 80 years ago create a barrier for the modern reader, who finds that the interpretation of Pearson's test procedure and the assessment of what he achieved are less than straightforward, notwithstanding the technical advances made since then. An attempt is made here to surmount these difficulties by exploring Pearson's relevant activities during the first decade of his statistical career, and by describing the work by his contemporaries and predecessors which seem to have influenced his approach to the problem. Not all the questions are answered, and others remain for further study.},
  Url                      = {http://www.jstor.org/stable/1402731}
}

@Article{1971:machintel:plotkin,
  Title                    = {A further note on inductive generalization},
  Author                   = {Plotkin, G. D.},
  Journal                  = machintel,
  Year                     = {1971},
  Pages                    = {101--124},
  Volume                   = {6},

  Abstract                 = {In this paper, we develop the algorithm, given in Plotkin (1970), for finding the least generalization of two clauses, into a theory of inductive generalization. The types of hypothesis which can be formed are very simple. They all have the form: (x)Px Qx. We have been guided by ideas from the philosophy of science, following Buchanan (1966). There is no search for infallible methods of generating true hypotheses. Instead we define (in terms of first-order predicate calculus) the notions of data and evidence for the data. Next, some formal criteria are set up for a sentence to be a descriptive hypothesis which is a good explanation of the data, given the evidence. We can then look for the best such hypothesis. Although this problem is insoluble in general, some soluble subcases can be distinguished. We programmed one of these and tried some examples.},
  Url                      = {http://adaptiveilp.googlecode.com/svn/trunk/thesis/bibliography/plotkin71futher%20-%20Futher%20note%20on%20lgg.pdf}
}

@Article{1970:machintel:plotkin,
  Title                    = {A note on inductive generalization},
  Author                   = {Gordon D. Plotkin},
  Journal                  = machintel,
  Year                     = {1970},
  Pages                    = {153--163},
  Volume                   = {5},

  Abstract                 = {In the course of the discussion on Reynolds' (1970) paper in this volume, it became apparent that some of our work was related to his, and we therefore present it here. R. J. Popplestone originated the idea that generalizations and least generalizations of literals existed and would be useful when looking for methods of induction. We refer the reader to his paper in this volume for an account of some of his methods (Popplestone 1970). Generalizations of clauses can also be of interest. Consider the following induction: The result of heating this bit of iron to 419C was that it melted. The result of heating that bit of iron to 419C was that it melted. The result of heating any bit of iron to 419C is that it melts. We can formalize this as: Bitofiron (bit l) Heated (bit 1,419) Melted (bit l) Bitofiron (bit 2) Heated (bit2,419) Melted (bit 2) (x) Bitofiron (x) Heated (x,419) Melted (x) Note that both antecedents and conclusion can be expressed as clauses in the usual first-order language with function symbols. Our aim is to find a rule depending on the form of the antecedents which will generate the conclusion in this and similar cases. It will turn out that the conclusion is the least generalization of its antecedents.},
  Url                      = {http://adaptiveilp.googlecode.com/svn/trunk/thesis/bibliography/plotkin70note%20-%20Note%20on%20lgg.pdf}
}

@Article{1990:tse:podgurski,
  Title                    = {A formal model of program dependences and its implications for software testing, debugging, and maintenance},
  Author                   = {Podgurski, Andy and Clarke, Lori A.},
  Journal                  = tse,
  Year                     = {1990},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {965--979},
  Volume                   = {16},

  Abstract                 = {A formal, general model of program dependences is presented and used to evaluate several dependence-based software testing, debugging, and maintenance techniques. Two generalizations of control and data flow dependence, called weak and strong syntactic dependence, are introduced and related to a concept called semantic dependence. Semantic dependence models the ability of a program statement to affect the execution behavior of other statements. It is shown that weak syntactic dependence is a necessary but not sufficient condition for semantic dependence and that strong syntactic dependence is necessary but not sufficient condition for a restricted form of semantic dependence that is finitely demonstrated. These results are used to support some proposed uses of program dependences, to controvert others, and to suggest new uses.},
  Doi                      = {10.1109/32.58784}
}

@InProceedings{1992:icse:podgurski,
  Title                    = {Behavior sampling: A technique for automated retrieval of reusable components},
  Author                   = {Podgurski, Andy and Pierce, Lynn},
  Booktitle                = icse,
  Year                     = {1992},
  Pages                    = {349--361},

  Abstract                 = {A new method, called behavior sampling, is proposed for automated retrieval of reusable components from software libraries. Unlike other retrieval methods, behavior sampling exploits the property that distinguishes software from other forms of text---its executability. Basic behavior-sampling identifies relevant routines by executing library routines on a searcher-supplied sample of operational inputs and comparing the routines' output to output provided by the searcher. The probabilistic basis for behavior sampling is described, and experimental results are reported that suggest basic behavior-sampling exhibits high precision (percentage of retrieved components that are relevant). Extensions to basic behavior-sampling are proposed to improve its recall (percentage of relevant components retrieved) and to make behavior sampling applicable to the retrieval of abstract data types.},
  Doi                      = {10.1109/ICSE.1992.753512}
}

@Article{2007:tse:poshyvanyk,
  Title                    = {Feature Location Using Probabilistic Ranking of Methods Based on Execution Scenarios and Information Retrieval},
  Author                   = {Poshyvanyk, Denys and Gu{\'e}h{\'e}neuc, Yann-Ga{\"e}l and Marcus, Andrian and Antoniol, Giuliano and Rajlich, V{\'a}clav},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {420--432},
  Volume                   = {33},

  Abstract                 = {This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The solution to the problem is formulated as a combination of the opinions of different experts. The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses latent semantic indexing. The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment. The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently},
  Doi                      = {10.1109/TSE.2007.1016}
}

@InProceedings{2007:icpc:poshyvanyk,
  Title                    = {Combining Formal Concept Analysis with Information Retrieval for Concept Location in Source Code},
  Author                   = {Poshyvanyk, Denys and Marcus, Andrian},
  Booktitle                = icpc,
  Year                     = {2007},
  Pages                    = {37--48},

  Abstract                 = {The paper addresses the problem of concept location in source code by presenting an approach which combines Formal Concept Analysis (FCA) and Latent Semantic Indexing (LSI). In the proposed approach, LSI is used to map the concepts expressed in queries written by the programmer to relevant parts of the source code, presented as a ranked list of search results. Given the ranked list of source code elements, our approach selects most relevant attributes from these documents and organizes the results in a concept lattice, generated via FCA. The approach is evaluated in a case study on concept location in the source code of Eclipse, an industrial size integrated development environment. The results of the case study show that the proposed approach is effective in organizing different concepts and their relationships present in the subset of the search results. The proposed concept location method outperforms the simple ranking of the search results, reducing the programmers' effort.}
}

@InProceedings{2006:icpc:poshyvanyk:b,
  Title                    = {{JIRiSS}: An {E}clipse plug-in for Source Code Exploration},
  Author                   = {Poshyvanyk, Denys and Marcus, Andrian and Dong, Yubo},
  Booktitle                = icpc,
  Year                     = {2006},
  Pages                    = {252--255},

  Abstract                 = {JIRiSS (Information Retrieval based Software Search for Java) is a software exploration tool that uses an indexing engine based on an information retrieval method. JIRiSS is implemented as a plug-in for Eclipse and it allows the user to search Java source code for the implementation of concepts formulated as natural language queries. The results of the query are presented as a ranked list of software methods or classes, ordered by the similarity to the user query. In addition to that, JIRiSS includes other advanced features like automatically generated software vocabulary, advanced query formulation options including spell-checking as well as fragment-based search.}
}

@InProceedings{2006:icpc:poshyvanyk:a,
  Title                    = {Combining Probabilistic Ranking and Latent Semantic Indexing for Feature Identification},
  Author                   = {Poshyvanyk, Denys and Marcus, Andrian and Rajlich, V{\'a}clav and Gu{\'e}h{\'e}neuc, Yann-Ga{\"e}l and Antoniol, Giuliano},
  Booktitle                = icpc,
  Year                     = {2006},
  Pages                    = {137--148},

  Abstract                 = {The paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The main contribution consists in the combination of two existing techniques for feature location in source code. Both techniques provide a set of ranked facts from the software, as result to the feature identification problem. One of the techniques is based on a Scenario Based Probabilistic ranking of events observed while executing a program under given scenarios. The other technique is defined as an information retrieval task, based on the Latent Semantic Indexing of the source code. We show the viability and effectiveness of the combined technique with two case studies. A first case study is a replication of feature identification in Mozilla, which allows us to directly compare the results with previously published data. The other case study is a bug location problem in Mozilla. The results show that the combined technique improves feature identification significantly with respect to each technique used independently.}
}

@InProceedings{2004:wicsa:postma,
  Title                    = {Component replacement in a long-living architecture: The {3RDBA} approach},
  Author                   = {Postma, A. and America, P. and Wijnstra, J. G.},
  Booktitle                = wicsa,
  Year                     = {2004},
  Pages                    = {89--98},

  Abstract                 = {In order to respond to changing requirements and advancesin technology, system and software architecturesmust evolve during their lifetimes. Usually, in this evolution,several key components of the architecture are replaced.Achieving successful architecture evolution at areasonable cost and effort is difficult. It requires manyarchitectural and technological decisions. This paper describesan approach, called 3RDBA, that facilitates replacinga key component in a long-living architecture. Itis based on systematically gathering all informationneeded to make well-founded decisions regarding evolutionof the architecture. The approach consists of an exploration,consolidation and migration cycle. Each cyclecontains four steps: Requirements, Design, Build andAnalyze. 3RDBA enables construction and evaluation ofseveral alternative architecture realizations together witha migration path from the existing architecture towardsthe selected, new architecture. We describe how we havesuccessfully applied this approach to support the evolutionof a medical imaging system architecture.}
}

@Article{2005:cacm:potanin,
  Title                    = {Scale-free geometry in object-oriented programs},
  Author                   = {Alex Potanin and James Noble and Marcus Frean and Robert Biddle},
  Journal                  = cacm,
  Year                     = {2005},

  Month                    = may,
  Number                   = {5},
  Pages                    = {99--103},
  Volume                   = {48},

  Abstract                 = {Though conventional OO design suggests programs should be built from many small objects, like Lego bricks, they are instead built from objects that are scale-free, like fractals, and unlike Lego bricks.},
  Doi                      = {10.1145/1060710.1060716}
}

@Article{1993:ibmsj:poulin,
  Title                    = {The business case for software reuse},
  Author                   = {Jeffrey S. Poulin and Joseph M. Caruso and Debera R. Hancock},
  Journal                  = ibmsj,
  Year                     = {1993},
  Number                   = {4},
  Pages                    = {567--594},
  Volume                   = {32},

  Abstract                 = {To remain competitive, software development organizations must reduce cycle time and cost, while at the same time adding function and improving quality. One potential solution lies in software reuse. Because software reuse is not free, we must weigh the potential benefits against the expenditures of time and resources required to identify and integrate reusable software into products. We first introduce software reuse concepts and examine the cost-benefit trade-offs of software reuse investments. We then provide a set of metrics used by IBM to accurately reflect the effort saved by reuse. We define reuse metrics that distinguish the savings and benefits from those already gained through accepted software engineering techniques. When used with the return-on-investment (ROI) model described in this paper, these metrics can effectively establish a sound business justification for reuse and can help assess the success of organizational reuse programs.},
  Doi                      = {10.1147/sj.324.0567}
}

@Article{2000:computer:prechelt,
  Title                    = {An empirical comparison of seven programming languages},
  Author                   = {Lutz Prechelt},
  Journal                  = computer,
  Year                     = {2000},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {23--29},
  Volume                   = {33},

  Abstract                 = {Often heated, debates regarding different programming languages' effectiveness remain inconclusive because of scarce data and a lack of direct comparisons. The author addresses that challenge, comparatively analyzing 80 implementations of the phone-code program in seven different languages (C, C++, Java, Perl, Python, Rexx and Tcl). Further, for each language, the author analyzes several separate implementations by different programmers. The comparison investigates several aspects of each language, including program length, programming effort, runtime efficiency, memory consumption, and reliability. The author uses comparisons to present insight into program language performance.},
  Doi                      = {10.1109/2.876288}
}

@InProceedings{1999:wicsa:pree,
  Title                    = {Rearchitecting Legacy System: Concepts and Case Study},
  Author                   = {Pree, Wolfgang and Koskimies, Kai},
  Booktitle                = wicsa,
  Year                     = {1999},
  Pages                    = {51--64},

  Abstract                 = {TBD}
}

@InProceedings{2010:icsm:prete,
  Title                    = {Template-based reconstruction of complex refactorings},
  Author                   = {Prete, Kyle and Rachatasumrit, Napol and Sudan, Nikita and Kim, Miryung},
  Booktitle                = icsm,
  Year                     = {2010},
  Pages                    = {1--10},

  Abstract                 = {Knowing which types of refactoring occurred between two program versions can help programmers better understand code changes. Our survey of refactoring identification techniques found that existing techniques cannot easily identify complex refactorings, such as an replace conditional with polymorphism refactoring, which consist of a set of atomic refactorings. This paper presents REF-FINDER that identifies complex refactorings between two program versions using a template-based refactoring reconstruction approach - REF-FINDER expresses each refactoring type in terms of template logic rules and uses a logic programming engine to infer concrete refactoring instances. It currently supports sixty three refactoring types from Fowler's catalog, showing the most comprehensive coverage among existing techniques. The evaluation using code examples from Fowler's catalog and open source project histories shows that REF-FINDER identifies refactorings with an overall precision of 0.79 and recall of 0.95.}
}

@Article{1993:software:prieto-diaz,
  Title                    = {Status report: Software reusability},
  Author                   = {Rub\'{e}n Prieto-D\'{\i}az},
  Journal                  = software,
  Year                     = {1993},

  Month                    = may,
  Number                   = {3},
  Pages                    = {61--66},
  Volume                   = {10},

  Abstract                 = {It is argued that the problem with software engineering is not a lack of reuse, but a lack of widespread, systematic reuse. The reuse research community is focusing on formalizing reuse because it recognizes that substantial quality and productivity payoffs will be achieved only if reuse is conducted systematically and formally. The history of reuse, which is characterized by this struggle to formalize in a setting where pragmatic problems are the norm and fast informal solutions usually take precedence, is reviewed. Several reuse methods are discussed.},
  Doi                      = {10.1109/52.210605}
}

@Article{1996:jetai:prietula,
  Title                    = {Software-effort estimation with a case-based reasoner},
  Author                   = {Michael J. Prietula and Steven S. Vicinanza and Tridas Mukhopadhyay},
  Journal                  = jetai,
  Year                     = {1996},
  Number                   = {3--4},
  Pages                    = {341--363},
  Volume                   = {8},

  Abstract                 = {Software effort estimation is an important but difficult task. Existing algorithmic models often fail to predict effort accurately and consistently. To address this, we developed a computational approach to software effort estimation. cEstor is a case-based reasoning engine developed from an analysis of expert reasoning. cEstor's architecture explicitly separates case-independent productivity adaptation knowledge (rules) from case-specific representations of prior projects encountered (cases). Using new data from actual projects, uncalibrated cEstor generated estimates which compare favorably to those of the referent expert, calibrated Function Points and calibrated COCOMO. The estimates were better than those produced by uncalibrated Basic COCOMO and Intermediate COCOMO. The roles of specific knowledge components in cEstor (cases, adaptation rules, and retrieval heuristics) were also examined. The results indicate that case-independent productivity adaptation rules affect the consistency of estimates and appropriate case selection affects the accuracy of estimates, but the combination of an adaptation rule set and unrestricted case base can yield the best estimates. Retrieval heuristics based on source lines of code and a Function Count heuristic based on summing over differences in parameter values, were found to be equivalent in accuracy and consistency, and both performed better than a heuristic based on Function Count totals.},
  Doi                      = {10.1080/095281396147366}
}

@InProceedings{2011:promise:prifti,
  Title                    = {Detecting bug duplicate reports through local references},
  Author                   = {Prifti, Tomi and Banerjee, Sean and Cukic, Bojan},
  Booktitle                = promise,
  Year                     = {2011},
  Pages                    = {8:1--8:9},

  Abstract                 = {Background: Bug Tracking Repositories, such as Bugzilla, are designed to support fault reporting for developers, testers and users of the system. Allowing anyone to contribute finding and reporting faults has an immediate impact on software quality. However, this benefit comes with at least one side-effect. Users often file reports that describe the same fault. This increases the maintainer's triage time, but important information required to fix the fault is likely contributed by different reports. Aim: The objective of this paper is twofold. First, we want to understand the dynamics of bug report filing for a large, long duration open source project, Firefox. Second, we present a new approach that can reduce the number of duplicate reports. Method: The novel element in the proposed approach is the ability to concentrate the search for duplicates on specific portions of the bug repository. Our system can be deployed as a search tool to help reporters query the repository. Results: When tested as a search tool our system is able to detect up to 53\% of duplicate reports. Conclusion: The performance of Information Retrieval techniques can be significantly improved by guiding the search for duplicates. This approach results in higher detection rates and constant classification runtime.},
  Doi                      = {10.1145/2020390.2020398}
}

@InProceedings{2012:esem:przepiora,
  Title                    = {A hybrid release planning method and its empirical justification},
  Author                   = {Przepiora, Mark and Karimpour, Reza and Ruhe, Guenther},
  Booktitle                = esem,
  Year                     = {2012},
  Pages                    = {115--118},

  Abstract                 = {Background: The use of Constraint Programming (CP) has been proposed by Regnell and Kuchcinski to model and solve the Release Planning Problem. However, they did not empirically demonstrate the advantages and disadvantages of CP over existing release planning methods. Aims: The aims of this paper are (1) to perform a comparative analysis between CP and ReleasePlanner (RP), an existing release planning tool, and (2) to suggest a hybrid approach combining the strengths of each individual method. Method: (1) An empirical evaluation was performed, evaluating the efficiency and effectiveness of the individual methods to justify their hybrid usage. (2) A proof of concept for a hybrid release planning method is introduced, and a real-world dataset including more than 600 features was solved using the hybrid method to provide evidence of its effectiveness. Results: (1) Use of RP was found to be more efficient and effective than CP. However, CP is preferred when advanced planning objectives and constraints exist. (2) The hybrid method (RP\&CP) greatly outperformed the individual approach (CP), increasing computational solution quality by 87\%. Conclusion: We were able to increase the expressiveness and thus applicability of an existing, efficient and effective release planning method. We presented evidence for its computational effectiveness, but more work is needed to make this result significant.},
  Doi                      = {10.1145/2372251.2372271}
}

@InProceedings{2011:csmr:przybylek,
  Title                    = {Impact of aspect-oriented programming on software modularity},
  Author                   = {Adam Przyby{\l}ek},
  Booktitle                = csmr,
  Year                     = {2011},
  Pages                    = {369--372},

  Abstract                 = {Over a decade ago, aspect-oriented programming (AOP) was proposed in the literature to ``modularize the un-modularizable". Nowadays, the aspect-oriented paradigm pervades all areas of software engineering. With its growing popularity, practitioners are beginning to wonder whether they should start looking into it. However, every new paradigm makes claims that managers want to hear. The aim of this PhD dissertation is to find out how much of what has been written about AOP is true and how much is hype.}
}

@InProceedings{2011:fase:przybylek,
  Title                    = {Where the Truth Lies: {AOP} and Its Impact on Software Modularity},
  Author                   = {Adam Przyby{\l}ek},
  Booktitle                = fase,
  Year                     = {2011},
  Pages                    = {447--461},
  Series                   = lncs,
  Volume                   = {6603},

  Abstract                 = {Modularity is the single attribute of software that allows a program to be intellectually manageable [29]. The recipe for modularizing is to define a narrow interface, hide an implementation detail, keep low coupling and high cohesion. Over a decade ago, aspect-oriented programming (AOP) was proposed in the literature to ``modularize the un-modularizable" [24]. Since then, aspect-oriented languages have been providing new abstraction and composition mechanisms to deal with concerns that could not be modularized because of the limited abstractions of the underlying programming language. This paper is a continuation of our earlier work [32] and further investigates AO software with regard to coupling and cohesion. We compare two versions (Java and AspectJ) of ten applications to review AOP within the context of software modularity. It turns out that the claim that ``the software built in AOP is more modular than the software built in OOP" is a myth.}
}

@InProceedings{2010:enase:przybylek,
  Title                    = {An Empirical Assessment of the Impact of Aspect-oriented Programming on Software Modularity},
  Author                   = {Adam Przyby{\l}ek},
  Booktitle                = enase,
  Year                     = {2010},
  Pages                    = {139--148},

  Abstract                 = {The term ``crosscutting concern" describes a piece of system that cannot be cleanly modularized because of the limited abstractions offered by the traditional programming paradigms. Symptoms of implementing crosscutting concerns in the languages like C, C# or Java are ``code scattering" and ``code tangling" that both degrade software modularity. Aspect-oriented programming (AOP) was proposed as a new paradigm to overcome these problems. Although it is known that AOP allows programmers to lexically separate crosscutting concerns, the impact of AOP on software modularity is not yet well investigated. This paper reports a quantitative study comparing Java and AspectJ implementations of the Gang-of-Four design patterns with respect to modularity.}
}

@Article{2006:patrecog:qiu,
  Title                    = {Graph matching and clustering using spectral partitions},
  Author                   = {Qiu, Huaijun and Hancock, Edwin R.},
  Journal                  = patrecog,
  Year                     = {2006},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {22--34},
  Volume                   = {39},

  Abstract                 = {Although inexact graph-matching is a problem of potentially exponential complexity, the problem may be simplified by decomposing the graphs to be matched into smaller subgraphs. If this is done, then the process may cast into a hierarchical framework and hence rendered suitable for parallel computation. In this paper we describe a spectral method which can be used to partition graphs into non-overlapping subgraphs. In particular, we demonstrate how the Fiedler-vector of the Laplacian matrix can be used to decompose graphs into non-overlapping neighbourhoods that can be used for the purposes of both matching and clustering.},
  Doi                      = {10.1016/j.patcog.2005.06.014}
}

@MastersThesis{2005:ms:quah,
  Title                    = {Case study on re-architecting of established enterprise software product: Major challenges encountered and {SDM} prescriptions from lessons learned},
  Author                   = {Quah, Kok-Seng},
  School                   = {Massachusetts Institute of Technology},
  Year                     = {2005},

  Address                  = {Cambridge, Massachusetts, USA},
  Type                     = {SM thesis},

  Abstract                 = {The paper studies a real word project of an enterprise software product re-architecting at a mid-sized telecommunication company. It begins with a description of the company and the software product, as well as an elaboration of the project under study. Using written surveys and follow-up interviews as the primary data gathering tools, the paper collects and tabulates first-hand experience and opinions from key project participants. Based on the survey results, the paper proposes an integrative implementation framework, based primarily on literature reviews in offshore outsourcing, systems and project management (SPM) and product design and development (PDD), for a detailed analysis of key challenges encountered by the project under study. The paper also investigates if specific key challenges could have been managed or influenced by the application of specific methods and tools within the proposed framework.},
  Doi                      = {1721.1/32470}
}

@InProceedings{2008:avi:quinn,
  Title                    = {An investigation of dynamic landmarking functions},
  Author                   = {Quinn, Philip and Cockburn, Andy and Indratmo and Gutwin, Carl},
  Booktitle                = aviwc,
  Year                     = {2008},
  Pages                    = {322--325},

  Abstract                 = {It is easy for users to lose awareness of their location and orientation when navigating large information spaces. Providing landmarks is one common technique that helps users remain oriented, alleviating the mental workload and reducing the number of redundant interactions. But how many landmarks should be displayed? We conducted an empirical evaluation of several relationships between the number of potential landmarked items in the display and the number of landmarks rendered at any one time, with results strongly favouring a logarithmic relationship.}
}

@Article{2012:compintel:rachkovskij,
  Title                    = {Similarity-Based Retrieval With Structure-Sensitive Sparse Binary Distributed Representations},
  Author                   = {Rachkovskij, Dmitri A. and Slipchenko, Serge V.},
  Journal                  = compintel,
  Year                     = {2012},

  Month                    = feb,
  Number                   = {1},
  Pages                    = {106--129},
  Volume                   = {28},

  Abstract                 = {We present an approach to similarity-based retrieval from knowledge bases that takes into account both the structure and semantics of knowledge base fragments. Those fragments, or analogues, are represented as sparse binary vectors that allow a computationally efficient estimation of structural and semantic similarity by the vector dot product. We present the representation scheme and experimental results for the knowledge base that was previously used for testing of leading analogical retrieval models MAC/FAC and ARCS. The experiments show that the proposed single-stage approach provides results compatible with or better than the results of two-stage models MAC/FAC and ARCS in terms of recall and precision. We argue that the proposed representation scheme is useful for large-scale knowledge bases and free-structured database applications.},
  Doi                      = {10.1111/j.1467-8640.2011.00423.x}
}

@Article{2012:ese:rahman,
  Title                    = {Clones: What is that smell?},
  Author                   = {Rahman, Foyzur and Bird, Christian and Devanbu, Premkumar},
  Journal                  = ese,
  Year                     = {2012},

  Month                    = aug,
  Number                   = {4--5},
  Pages                    = {503--530},
  Volume                   = {17},

  Doi                      = {10.1007/s10664-011-9195-3}
}

@InProceedings{2009:icpc:rajlich,
  Title                    = {Intensions are a Key to Program Comprehension},
  Author                   = {Rajlich, V{\'a}clav},
  Booktitle                = icpc,
  Year                     = {2009},
  Pages                    = {1--9},

  Abstract                 = {The classical comprehension theories study relations between extensions, intensions, and names. Originally developed in linguistics and mathematics, these theories are applicable to program comprehension as well. While extensions are present in the program, the intensions are usually missing, and evolution and maintenance programmers have to recover them as the program cannot be successfully comprehended and changed without them.}
}

@InProceedings{2002:iwpc:rajlich,
  Title                    = {The role of concepts in program comprehension},
  Author                   = {Rajlich, V{\'a}clav and Wilde, Norman},
  Booktitle                = iwpc,
  Year                     = {2002},
  Pages                    = {271--280},

  Abstract                 = {The paper presents an overview of the role of concepts in program comprehension. It discusses concept location, in which the implementation of a specific concept is located in the code. This process is very common and precedes a large proportion of code changes. The paper also discusses the process of learning about the domain from the code, which is aprerequisite of code reengineering. The paper notes the similarities and overlaps between program comprehension and human learning.}
}

@InProceedings{1999:ssr:ran,
  Title                    = {Software isn't built from {L}ego blocks},
  Author                   = {Ran, Alexander},
  Booktitle                = ssr,
  Year                     = {1999},
  Pages                    = {164--169},

  Abstract                 = {Component-based reuse is seen by many software engineers and managers as a promising approach to reduce high costs of building complex sohare. Lego blocks often serve as a metaphor for component-based reuse. Different kinds of blocks from standard block sets can be used for constructing endless variety of structures. Why cannot software be constructed as easily using ready-made, well-designed, carefully implemented, thoroughly tested, and clearly documented components? As far as experience goes reusing non-trivial software components even in a homogenous environment is very hard. I believe the main reason for this is that complex software is not built from simple components, like Lego blocks. Complex components depend on other components providing specific functionality; they need to observe system-wide policies for security, flow control, overload control, fault detection and handling; they must rely on infrastructure for communication, coordination, state maintenance, execution tracing, etc. For a project to develop reusable components the context of use must be well understood. The context of use for software components is determined by software architecture. Therefor[e] component-based reuse is only possible as a consequence of architecture-based reuse. If we intend to reuse architectures we need to improve our understanding regarding what it is, how it is created, and how it can be reused. This understanding must be shared by software engineers, product and project managers forming a conceptual framework for understanding complex software---a culture in a community.},
  Doi                      = {10.1145/303008.303079}
}

@InProceedings{2011:ase:rasool,
  Title                    = {Flexible design pattern detection based on feature types},
  Author                   = {Ghulam Rasool and Patrick M{\"a}der},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {243--252},

  Abstract                 = {Accurately recovered design patterns support development related tasks like program comprehension and reengineering. Researchers proposed a variety of recognition approaches already. Though, much progress was made, there is still a lack of accuracy and flexibility in recognition. A major problem is the large variety of variants for implementing the same pattern. Furthermore, the integration of multiple search techniques is required to provide more accurate and effective pattern detection. In this paper, we propose variable pattern definitions composed of reusable feature types. Each feature type is assigned to one of multiple search techniques that is best fitting for its detection. A prototype implementation was applied to three open source applications. For each system a baseline was determined and used for comparison with the results of previous techniques. We reached very good results with an improved pattern catalog, but also demonstrated the necessity for customizations on new inspected systems. These results demonstrate the importance of customizable pattern definitions and multiple search techniques in order to overcome accuracy and flexibility issues of previous approaches.},
  Doi                      = {10.1109/ASE.2011.6100060}
}

@InCollection{2010:book:sobh:rasool,
  Title                    = {Software Artifacts Extraction for Program Comprehension},
  Author                   = {Rasool, Ghulam and Philippow, Ilka},
  Booktitle                = {Innovations in Computing Sciences and Software Engineering},
  Publisher                = {Springer},
  Year                     = {2010},
  Editor                   = {Sobh, Tarek and Elleithy, Khaled},
  Pages                    = {443--447},

  Abstract                 = {The maintenance of legacy software applications is a complex, expensive, quiet challenging, time consuming and daunting task due to program comprehension difficulties. The first step for software maintenance is to understand the existing software and to extract the high level abstractions from the source code. A number of methods, techniques and tools are applied to understand the legacy code. Each technique supports the particular legacy applications with automated/semi-automated tool support keeping in view the requirements of the maintainer. Most of the techniques support the modern languages but lacks support for older technologies. This paper presents a lightweight methodology for extraction of different artifacts from legacy COBOL and other applications.},
  Doi                      = {10.1007/978-90-481-9112-3_75}
}

@InProceedings{2007:esem:ratzinger,
  Title                    = {Mining Software Evolution to Predict Refactoring},
  Author                   = {Ratzinger, Jacek and Sigmund, Thomas and Vorburger, Peter and Gall, Harald},
  Booktitle                = esem,
  Year                     = {2007},
  Pages                    = {354--363},

  Abstract                 = {Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, propositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects.}
}

@Article{2003:physreve:ravasz,
  Title                    = {Hierarchical organization in complex networks},
  Author                   = {Erzs{\'e}bet Ravasz and Albert-L{\'a}szl{\'o} Barab{\'a}si},
  Journal                  = physreve,
  Year                     = {2003},

  Month                    = {14 } # feb,
  Pages                    = {026112:1--026112:7},
  Volume                   = {67},

  Abstract                 = {Many real networks in nature and society share two generic properties: they are scale-free and they display a high degree of clustering. We show that these two features are the consequence of a hierarchical organization, implying that small groups of nodes organize in a hierarchical manner into increasingly large groups, while maintaining a scale-free topology. In hierarchical networks, the degree of clustering characterizing the different groups follows a strict scaling law, which can be used to identify the presence of a hierarchical organization in real networks. We find that several real networks, such as the Worldwideweb, actor network, the Internet at the domain level, and the semantic web obey this scaling law, indicating that hierarchy is a fundamental characteristic of many complex systems.},
  Doi                      = {10.1103/PhysRevE.67.026112}
}

@Article{2010:patreclet:raveaux,
  Title                    = {A graph matching method and a graph matching distance based on subgraph assignments},
  Author                   = {Raveaux, Romain and Burie, Jean-Christophe and Ogier, Jean-Marc},
  Journal                  = patreclet,
  Year                     = {2010},

  Month                    = apr,
  Number                   = {5},
  Pages                    = {394--406},
  Volume                   = {31},

  Abstract                 = {During the last decade, the use of graph-based object representation has drastically increased. As a matter of fact, object representation by means of graphs has a number of advantages over feature vectors. As a consequence, methods to compare graphs have become of first interest. In this paper, a graph matching method and a distance between attributed graphs are defined. Both approaches are based on subgraphs. In this context, subgraphs can be seen as structural features extracted from a given graph, their nature enables them to represent local information of a root node. Given two graphs $G_1$, $G_2$, the univalent mapping can be expressed as the minimum-weight subgraph matching between $G_1$ and $G_2$ with respect to a cost function. This metric between subgraphs is directly derived from well-known graph distances. In experiments on four different data sets, the distance induced by our graph matching was applied to measure the accuracy of the graph matching. Finally, we demonstrate a substantial speed-up compared to conventional methods while keeping a relevant precision.},
  Doi                      = {10.1016/j.patrec.2009.10.011}
}

@Article{2003:cacm:ravichandran,
  Title                    = {Software reuse strategies and component markets},
  Author                   = {Thiagarajan Ravichandran and Marcus A. Rothenberger},
  Journal                  = cacm,
  Year                     = {2003},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {109--114},
  Volume                   = {46},

  Abstract                 = {Black-box reuse with component markets could be the silver bullet solution that makes software reuse a reality, and advances software development to a robust industrial process---but only if market makers address the growing pains plaguing this immature industry.},
  Doi                      = {10.1145/859670.859678}
}

@Article{1999:ktp:raymond,
  Title                    = {The Cathedral and the Bazaar},
  Author                   = {Eric Raymond},
  Journal                  = ktp,
  Year                     = {1999},

  Month                    = {Fall},
  Number                   = {3},
  Pages                    = {23--49},
  Volume                   = {12},

  Abstract                 = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of some theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral" model, representing most of the commercial world, versus the ``bazaar" model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that ``Given enough eyeballs, all bugs are shallow," suggesting productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
  Doi                      = {10.1007/s12130-999-1026-0}
}

@InProceedings{1993:chi:redmiles,
  Title                    = {Reducing the Variability of Programmers' Performance through Explained Examples},
  Author                   = {David F. Redmiles},
  Booktitle                = chi,
  Year                     = {1993},
  Pages                    = {67--73},

  Abstract                 = {A software tool called EXPLAINER has been developed for helping programmers perform new tasks by exploring previously worked-out examples. EXPLAINER is based on cognitive principles of learning from examples and problem solving by analogy. The interface is based on the principle of making examples accessible through multiple presentation views and multiple representation perspectives. Empirical evaluation has shown that programmers using EXPLAINER exhibit less variability in their performance compared to programmers using a commercially available, searchable on-line manual. These results are related to other studies of programmers and to current methodologies in software engineering.}
}

@PhdThesis{1992:phd:redmiles,
  Title                    = {From programming tasks to solutions: Bridging the gap through the explanation of examples},
  Author                   = {David Francis Redmiles},
  School                   = {University of Colorado at Boulder},
  Year                     = {1992},

  Address                  = {Boulder, Colorado, USA},

  Abstract                 = {Evidence, experience, and observation indicate that examples provide a powerful aid for problem solvers. In the domain of software engineering, examples not only provide objects to be reused but also a context in which users can explore issues related to the current task. This dissertation describes a software tool called EXPLAINER, which supports programmers' use of examples in the domain of graphics programming, assisting them with examples and explanations from various views and representation perspectives. EXPLAINER provides a conceptual and working framework for the study of programmers' uses of examples in problem solving and serves as a test bed for representations based upon multiple perspectives. The EXPLAINER approach is evaluated and compared with other available approaches, such as on-line manuals. The evaluation showed that subjects using EXPLAINER exhibited a more controlled and directed problem-solving process compared to subjects using a commercially available, searchable on-line manual. Representation of examples from multiple perspectives is seen as a critical aspect of catalog-based design environments.},
  Url                      = {http://ezproxy.lib.ucalgary.ca:2048/login?url=http://search.proquest.com/docview/303959284?accountid=9838}
}

@Article{2013:sosym:reimann,
  Title                    = {On the reuse and recommendation of model refactoring specifications},
  Author                   = {Jan Reimann and Mirko Seifert and Uwe A{\ss}mann},
  Journal                  = sosym,
  Year                     = {2013},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {579--596},
  Volume                   = {12},

  Abstract                 = {Refactorings can be used to improve the structure of software artefacts while preserving the semantics of the encapsulated information. Various types of refactorings have been proposed and implemented for programming languages (e.g., Java or C\#). With the advent of Model-Driven Software Development (MDSD), a wealth of modelling languages rises and the need for restructuring models similar to programs has emerged. Since parts of these modelling languages are often very similar, we consider it beneficial to reuse the core transformation steps of refactorings across languages. In this sense, reusing the abstract transformation steps and the abstract participating elements suggests itself. Previous work in this field indicates that refactorings can be specified generically to foster their reuse. However, existing approaches can handle certain types of modelling languages only and solely reuse refactorings once per language. In this paper, a novel approach based on role models to specify generic refactorings is presented. Role models are suitable for this problem since they support declaration of roles which have to be played in a certain context. Assigned to generic refactoring, contexts are different refactorings and roles are the participating elements. We discuss how this resolves the limitations of previous works, as well as how specific refactorings can be defined as extensions to generic ones. The approach was implemented in our tool Refactory based on the Eclipse Modeling Framework (EMF) and evaluated using multiple modelling languages and refactorings. In addition, this paper investigates on the recommendation of refactoring specifications. This is motivated by the fact that language designers have many possibilities to enable refactorings in their modelling languages with regard to the language structures.To overcome this problem and to support language designers in deciding which refactorings to enable, we propose a solution and a prototypical implementation.},
  Doi                      = {10.1007/s10270-012-0243-2}
}

@InProceedings{2002:ossd:reis,
  Title                    = {An Overview of the Software Engineering Process and Tools in the Mozilla Project},
  Author                   = {Christian Robottom Reis and Renata Pontin de Mattos Fortes},
  Booktitle                = ossd,
  Year                     = {2002},
  Pages                    = {155--175},

  Abstract                 = {The Mozilla Project is an Open Source Software project which is dedicated to development of the Mozilla Web browser and application framework. Possessing one of the largest and most complex communities of developers among Open Source projects, it presents interesting requirements for a software process and the tools to support it. Over the past four years, process and tools have been refined to a point where they are both stable and effective in serving the project's needs. This paper describes the software engineering aspect of a large Open Source project. It also covers the software engineering tools used in the Mozilla Project, since the Mozilla process and tools are intimately related. These tools include Bugzilla, a Web application designed for bug tracking, bug triage, code review and correction; Tinderbox, an automated build and regression testing system; Bonsai, a tool which performs queries to the CVS code repository; and LXR, a hypertext-based source code browser.}
}

@InProceedings{2009:icse:reiss,
  Title                    = {Semantics-based code search},
  Author                   = {Steven P. Reiss},
  Booktitle                = icse,
  Year                     = {2009},
  Pages                    = {243--253},

  Abstract                 = {Our goal is to use the vast repositories of available open source code to generate specific functions or classes that meet a user's specifications. The key words here are specifications and generate. We let users specify what they are looking for as precisely as possible using keywords, class or method signatures, test cases, contracts, and security constraints. Our system then uses an open set of program transformations to map retrieved code into what the user asked for. This approach is implemented in a prototype system for Java with a web interface.},
  Doi                      = {10.1109/ICSE.2009.5070525}
}

@InProceedings{2009:suite:reiss,
  Title                    = {Specifying what to search for},
  Author                   = {Reiss, Steven P.},
  Booktitle                = suite,
  Year                     = {2009},
  Pages                    = {41--44},

  Abstract                 = {In this position paper we look at the problem of letting the programmer specify what they want to search for. We discuss current approaches and their problems. We propose a semantics-based approach and describe the steps we have taken and the many open questions remaining.}
}

@InProceedings{2005:vissoft:reiss,
  Title                    = {The paradox of software visualization},
  Author                   = {Steven P. Reiss},
  Booktitle                = vissoft,
  Year                     = {2005},
  Pages                    = {19:1--19:5},

  Abstract                 = {Software visualization seems like such a logical and helpful concept with obvious benefits and advantages. But after decades of research and work it has yet to be successful in any mainstream development environment. What is the reason for this paradox? Will software visualization ever be actually widely used? In this paper we argue that most past and current work in the field (our own included) is out of touch with the reality of software development and that new approaches and new ideas are needed.}
}

@Article{1996:csur:reiss,
  Title                    = {Software tools and environments},
  Author                   = {Reiss, Steven P.},
  Journal                  = csur,
  Year                     = {1996},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {281--284},
  Volume                   = {28},

  Abstract                 = {Any system that assists the programmer with some aspect of programming can be considered a programming tool. Similarly, a system that assists in some phase of the software development process can be considered a software tool. A programming environment is a suite of programming tools designed to simplify programming and thereby enhance programmer productivity. A software engineering environment extends this to software tools and the whole software development process.},
  Doi                      = {10.1145/234313.234423}
}

@InProceedings{2012:icse:reiss,
  Title                    = {{Code Bubbles}: A practical working-set programming environment},
  Author                   = {Reiss, Steven P. and Bott, Jared N. and LaViola, Jr., Joseph J.},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {1411--1414},

  Abstract                 = {Our original work on the Code Bubbles environment demonstrated that a working-set based framework for software development showed promise. We have spent the past several years extending the underlying concepts into a fully-functional system. In our demonstration, we will show the current Code Bubbles environment for Java, how it works, how it can be used, and why we prefer it over more traditional programming environments. We will also show how we have extended the framework to enhance software development tasks such as complex debugging, testing, and collaboration. This paper describes the features we will demonstrate.},
  Url                      = {http://dl.acm.org/citation.cfm?id=2337223.2337432}
}

@InProceedings{2004:oopsla:ren,
  Title                    = {Chianti: A tool for change impact analysis of {J}ava programs},
  Author                   = {Ren, Xiaoxia and Shah, Fenil and Tip, Frank and Ryder, Barbara G. and Chesley, Ophelia},
  Booktitle                = oopsla,
  Year                     = {2004},
  Pages                    = {432--448},

  Abstract                 = {This paper reports on the design and implementation of Chianti, a change impact analysis tool for Java that is implemented in the context of the Eclipse environment. Chianti analyzes two versions of an application and decomposes their difference into a set of atomic changes. Change impact is then reported in terms of affected (regression or unit) tests whose execution behavior may have been modified by the applied changes. For each affected test, Chianti also determines a set of affecting changes that were responsible for the test's modified behavior. This latter step of isolating the changes that induce the failure of one specific test from those changes that only affect other tests can be used as a debugging technique in situations where a test fails unexpectedly after a long editing session. We evaluated Chianti on a year (2002) of CVS data from M. Ernst's Daikon system, and found that, on average, 52\% of Daikon's unit tests are affected. Furthermore, each affected unit test, on average, is affected by only 3.95\% of the atomic changes. These findings suggest that our change impact analysis is a promising technique for assisting developers with program understanding and debugging.}
}

@InProceedings{1997:esec_fse:reps,
  Title                    = {The use of program profiling for software maintenance with applications to the {Year 2000 Problem}},
  Author                   = {Reps, Thomas and Ball, Thomas and Das, Manuvir and Larus, James},
  Booktitle                = esec_fse,
  Year                     = {1997},
  Pages                    = {432--449},

  Abstract                 = {This paper describes new techniques to help with testing and debugging, using information obtained from path profiling. A path profiler instruments a program so that the number of times each different loop-free path executes is accumulated during an execution run. With such an instrumented program, each run of the program generates a path spectrum for the execution---a distribution of the paths that were executed during that run. A path spectrum is a finite, easily obtainable characterization of a program's execution on a dataset, and provides a behavior signature for a run of the program. Our techniques are based on the idea of comparing path spectra from different runs of the program. When different runs produce different spectra, the spectral differences can be used to identify paths in the program along which control diverges in the two runs. By choosing input datasets to hold all factors constant except one, the divergence can be attributed to this factor. The point of divergence itself may not be the cause of the underlying problem, but provides a starting place for a programmer to begin his exploration. One application of this technique is in the ``Year 2000 Problem'' (i.e., the problem of fixing computer systems that use only 2-digit year fields in date-valued data). In this context, path-spectrum comparison provides a heuristic for identifying paths in a program that are good candidates for being date-dependent computations. The application of path-spectrum comparison to a number of other software-maintenance issues is also discussed.},
  Doi                      = {10.1145/267895.267925}
}

@InProceedings{1984:sde:reps,
  Title                    = {The Synthesizer Generator},
  Author                   = {Reps, Thomas and Teitelbaum, Tim},
  Booktitle                = sde,
  Year                     = {1984},
  Pages                    = {42--48},

  Doi                      = {10.1145/390010.808247}
}

@InProceedings{1993:wcre:reubenstein,
  Title                    = {Separating Parsing and Analysis in Reverse Engineering Tools},
  Author                   = {Howard B. Reubenstein and
 Richard L. Piazza and
 Susan N. Roberts},
  Booktitle                = wcre,
  Year                     = {1993},
  Pages                    = {117--125}
}

@InProceedings{2005:iwpc:revelle,
  Title                    = {Understanding Concerns in Software: Insights Gained from Two Case Studies},
  Author                   = {Meghan Revelle and Tiffany Broadbent and David Coppit},
  Booktitle                = iwpc,
  Year                     = {2005},
  Pages                    = {23--32},

  Abstract                 = {Much of the complexity of software arises from the interactions between disparate concerns. Even in well-designed software, some concerns can not always be encapsulated in a module. Research on separation of concerns seeks to address this problem, but we lack an understanding of how programmers conceptualize the notion of a concern and then identify that concern in code. In this work, we have conducted two exploratory case studies to better understand these issues. The case studies involved programmers identifying concerns and associated code in existing, unfamiliar software: GNU's sort.c and the game Minesweeper. Based on our experiences with these two case studies, we have identified several types of concerns and have detailed a number of factors that impact programmer identification of concerns. Based on these insights, we have created two sets of guidelines: one to help programmers identify relevant concerns and another to help programmers identify code relating to concerns.}
}

@Article{1970:machintel:reynolds,
  Title                    = {Transformational systems and the algebraic structure of atomic formulas},
  Author                   = {Reynolds, John C.},
  Journal                  = machintel,
  Year                     = {1970},
  Number                   = {1},
  Pages                    = {135--151},
  Volume                   = {5},

  Abstract                 = {If the set of atomic formulas is augmented by adding a `universal formula' and a `null formula', then the equivalence classes of this set under alphabetic variation form a complete non-modular lattice, with `instance' as the partial ordering, `greatest common instance' as the meet operation, and `least common generalization' as the join operation. The greatest common instance of two formulas can be obtained from Robinson's Unification Algorithm. An algorithm is given for computing the least common generalization of two formulas, the covering relation of the lattice is determined, bounds are obtained on the length of chains from one formula to another, and it is shown that any formula is the least common generalization of its set of ground instances. A transformational system is a finite set of clauses containing only units and transformations, which are clauses containing exactly one positive and one negative literal. It is shown that every unsatisfiable transformational system has a refutation where every resolution has at least one resolvend which is an initial clause. An algorithm is given for computing a common generalization of all atomic formulas which can be derived from a transformational system, and it is shown that there is no decision procedure for transformational systems.}
}

@InProceedings{2013:wse:ricca,
  Title                    = {Web testware evolution},
  Author                   = {Ricca, Filimo and Leotta, Maurizio and Stocco, Andrea and Clerissi, Diego and Tonella, Paolo},
  Booktitle                = wse,
  Year                     = {2013},
  Pages                    = {39--44},

  Doi                      = {10.1109/WSE.2013.6642415}
}

@Article{1953:tams:rice,
  Title                    = {Classes of Recursively Enumerable Sets and Their Decision Problems},
  Author                   = {Rice, H. G.},
  Journal                  = tams,
  Year                     = {1953},
  Pages                    = {358--366},
  Volume                   = {74},

  Doi                      = {10.1090/S0002-9947-1953-0053041-6}
}

@TechReport{1987:tr:rich,
  Title                    = {The {P}rogrammer's {A}pprentice Project: A Research Overview},
  Author                   = {Charles Rich and Richard C. Waters},
  Institution              = {Artificial Intelligence Laboratory, Massachusetts Institute of Technology},
  Year                     = {1987},
  Month                    = nov,
  Number                   = {AIM-1004},

  Abstract                 = {The goal of the Programmer's Apprentice project is to develop a theory of how expert programmers analyze, synthesize, modify, explain, specify, verify, and document programs. This research goal overlaps both artificial intelligence and software engineering. From the viewpoint of artificial intelligence, we have chosen programming as a domain in which to study fundamental issues of knowledge representation and reasoning. From the viewpoint of software engineering, we seek to automate the programming process by applying techniques from artificial intelligence.},
  Doi                      = {1721.1/6054}
}

@InCollection{2010:book:aggarwal:riesen,
  Title                    = {Exact and Inexact Graph Matching: Methodology and Applications},
  Author                   = {Kaspar Riesen and Xiaoyi Jiang and Horst Bunke},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {7},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {217--247},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {Graphs provide us with a powerful and flexible representation formalism which can be employed in various fields of intelligent information processing. The process of evaluating the similarity of graphs is referred to as graph matching. Two approaches to this task exist, viz. exact and inexact graph matching. The former approach aims at finding a strict correspondence between two graphs to be matched, while the latter is able to cope with errors and measures the difference of two graphs in a broader sense. The present chapter reviews some fundamental concepts of both paradigms and shows two recent applications of graph matching in the fields of information retrieval and pattern recognition.},
  Doi                      = {10.1007/978-1-4419-6045-0_7}
}

@TechReport{2006:tr:rigby,
  Title                    = {A preliminary examination of code review processes in open source projects},
  Author                   = {Peter C. Rigby and Daniel M. German},
  Institution              = {University of Victoria},
  Year                     = {2006},
  Month                    = jan,
  Number                   = {DCS-305-IR},

  Abstract                 = {This paper represents a first attempt to understand the code review processes used by open source projects. Although there have been many studies of open source projects [1, 2, 4], these studies have focused on the entire development process and community of the project or projects. We are not aware of any paper that has examined the review process of open source projects in depth or compared the review processes used among projects. We examined the stated and observed code review processes used by 11 open source projects; the four most interesting projects are discussed. Additionally, we examined the mature, well-known Apache server project in depth. We extracted the developer and commit mailing lists into a database in order to reconstruct and understand the review and patch processes of the Apache project. The paper is broken into six sections. The remainder of this section introduces our research questions. The second section introduces our research methodology and data extraction techniques. The third section discusses the actors in the process, introduces a general patch process, and contrasts the formal and observed processes used by GCC, Linux, Mozilla, and Apache. The fourth section discusses some observed review patterns. The fifth section quantitatively answers our research questions using the Apache data (although we discuss our findings, due to time and other considerations, we leave the development of hypotheses to future work). In the final section we present our conclusions and future work.}
}

@InProceedings{2008:icse:rigby,
  Title                    = {Open Source Software Peer Review Practices: A Case Study of the {A}pache Server},
  Author                   = {Peter C. Rigby and Daniel M. German and Margaret-Anne Storey},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {541--550},

  Abstract                 = {Peer review is seen as an important quality assurance mechanism in both industrial development and the open source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, peer reviews are less well understood. We examine the two peer review techniques used by the successful, mature Apache server project: review-then-commit and commit-then-review. Using archival records of email discussion and version control repositories, we construct a series of metrics that produces measures similar to those used in traditional inspection experiments. Specifically, we measure the frequency of review, the level of participation in reviews, the size of the artifact under review, the calendar time to perform a review, and the number of reviews that find defects. We provide a comparison of the two Apache review techniques as well as a comparison of Apache review to inspection in an industrial project. We conclude that Apache reviews can be described as (1) early, frequent reviews (2) of small, independent, complete contributions (3) conducted asynchronously by a potentially large, but actually small, group of self-selected experts (4) leading to an efficient and effective peer review technique.}
}

@Book{1979:book:van_rijsbergen,
  Title                    = {Information Retrieval},
  Author                   = {van Rijsbergen, C. J.},
  Publisher                = {Butterworth--Heinemann},
  Year                     = {1979},
  Edition                  = {2nd}
}

@InProceedings{1999:oopsla:rinat,
  Title                    = {Correspondence polymorphism for object-oriented languages},
  Author                   = {Rinat, Ran and Magidor, Menachem and Smith, Scott F.},
  Booktitle                = oopsla,
  Year                     = {1999},
  Pages                    = {167--186},

  Abstract                 = {In this paper we propose a new form of polymorphism for object-oriented languages, called correspondence polymorphism. It lies in a different dimension than either parametric or subtype polymorphism. In correspondence polymorphism, some methods are declared to correspond to other methods, via a correspondence relation. With this relation, it is possible to reuse non-generic code in various type contexts---not necessarily subtyping or matching contexts---without having to plan ahead for this reuse. Correspondence polymorphism has advantages over other expressive object type systems in that programmer-declared types still may be simple, first-order types that are easily understood. We define a simple language LCP that reflects these new ideas, illustrating its behavior with multiple examples. We present formal type rules and an operational semantics for LCP, and establish soundness of the type system with respect to reduction.},
  Doi                      = {10.1145/320384.320399}
}

@InCollection{1984:book:arbib:rissland,
  Title                    = {Examples and Learning Systems},
  Author                   = {Edwina L. Rissland},
  Booktitle                = {Adaptive Control of Ill-Defined Systems},
  Publisher                = {Plenum},
  Year                     = {1984},
  Editor                   = {Oliver G. Selfridge and Edwina L. Rissland and Michael A. Arbib},

  Abstract                 = {TBD}
}

@Misc{2003:misc:des_rivieres,
  Title                    = {Enable {E}clipse to be used as a rich client platform},

  Author                   = {des Rivi{\`e}res, Jim},
  HowPublished             = {https://\discretionary{}{}{}bugs.eclipse.org/\discretionary{}{}{}bugs/\discretionary{}{}{}show\_bug.cgi?id=36967},
  Month                    = apr,
  Note                     = {Eclipse Bug~36967},
  Year                     = {2003},

  Url                      = {https://bugs.eclipse.org/bugs/show_bug.cgi?id=36967}
}

@InProceedings{2009:suite:robbes,
  Title                    = {On the evaluation of recommender systems with recorded interactions},
  Author                   = {Robbes, Romain},
  Booktitle                = suite,
  Year                     = {2009},
  Pages                    = {45--48},

  Abstract                 = {Recommender systems are Integrated Development Environment (IDE) extensions which assist developers in the task of coding. However, since they assist specific aspects of the general activity of programming, their impact is hard to assess. In previous work, we used with success an evaluation strategy using automated benchmarks to automatically and precisely evaluate several recommender systems, based on recording and replaying developer interactions. In this paper, we highlight the challenges we expect to encounter while applying this approach to other recommender systems.}
}

@Article{2010:ijase:robbes,
  Title                    = {Improving code completion with program history},
  Author                   = {Robbes, Romain and Lanza, Michele},
  Journal                  = ijase,
  Year                     = {2010},

  Month                    = {1 } # jun,
  Number                   = {2},
  Pages                    = {181--212},
  Volume                   = {17},

  Abstract                 = {Code completion is a widely used productivity tool. It takes away the burden of remembering and typing the exact names of methods or classes: As a developer starts typing a name, it provides a progressively refined list of candidates matching the name. However, the candidate list usually comes in alphabetic order, i.e., the environment is only second-guessing the name based on pattern matching, relying on human intervention to pick the correct one. Finding the correct candidate can thus be cumbersome or slower than typing the full name.},
  Doi                      = {10.1007/s10515-010-0064-x}
}

@InProceedings{2008:icse:robbes,
  Title                    = {{SpyWare}: A change-aware development toolset},
  Author                   = {Robbes, Romain and Lanza, Michele},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {847--850},

  Abstract                 = {Our research is driven by the motivation that change must be put in the center, if one wants to understand the complex processes of software evolution. We built a toolset named SpyWare which, using a monitoring plug-in for integrated development environments (IDEs), tracks the changes that a developer performs on a program as they happen. SpyWare stores these first-class changes in a change repository and offers a plethora of productivity-enhancing IDE extensions to exploit the recorded information.}
}

@InProceedings{2008:models:robbes,
  Title                    = {Example-Based Program Transformation},
  Author                   = {Robbes, Romain and Lanza, Michele},
  Booktitle                = models,
  Year                     = {2008},
  Pages                    = {174--188},
  Series                   = lncs,
  Volume                   = {5301},

  Abstract                 = {Software changes. During their life cycle, software systems experience a wide spectrum of changes, from minor modifications to major architectural shifts. Small-scale changes are usually performed with text editing and refactorings, while large-scale transformations require dedicated program transformation languages. For medium-scale transformations, both approaches have disadvantages. Manual modifications may require a myriad of similar yet not identical edits, leading to errors and omissions, while program transformation languages have a steep learning curve, and thus only pay off for large-scale transformations. We present a system supporting example-based program transformation. To define a transformation, a programmer performs an example change manually, feeds it into our system, and generalizes it to other application contexts. With time, a developer can build a palette of reusable medium-sized code transformations. We provide a detailed description of our approach and illustrate it with examples.}
}

@Article{2007:entcs:robbes,
  Title                    = {A Change-based Approach to Software Evolution},
  Author                   = {Robbes, Romain and Lanza, Michele},
  Journal                  = entcs,
  Year                     = {2007},

  Month                    = jan,
  Pages                    = {93--109},
  Volume                   = {166},

  Abstract                 = {Software evolution research is limited by the amount of information available to researchers: Current version control tools do not store all the information generated by developers. They do not record every intermediate version of the system issued, but only snapshots taken when a developer commits source code into the repository. Additionally, most software evolution analysis tools are not a part of the day-to-day programming activities, because analysis tools are resource intensive and not integrated in development environments. We propose to model development information as change operations that we retrieve directly from the programming environment the developers are using, while they are effecting changes to the system. This accurate and incremental information opens new ways for both developers and researchers to explore and evolve complex systems.},
  Doi                      = {10.1016/j.entcs.2006.06.015}
}

@InProceedings{2007:icpc:robbes,
  Title                    = {Characterizing and Understanding Development Sessions},
  Author                   = {Robbes, Romain and Lanza, Michele},
  Booktitle                = icpc,
  Year                     = {2007},
  Pages                    = {155--166},

  Abstract                 = {The understanding of development sessions, the phases during which a developer actively modifies a software system, is a valuable asset for program comprehension, since the sessions directly impact the current state and future evolution of a software system. Such information is usually lost by state-of-the-art versioning systems, because of the checkin/checkout model they rely on: a developer must explicitly commit his changes to the repository. Since this happens in arbitrary and sometimes long intervals, recovering the changes between two commits is difficult and inaccurate, and recovering the order of the changes is impossible. We have implemented an evolution monitoring prototype which records every semantic change performed on a system, and is able to completely reconstruct development sessions. In this paper we use this fine-grained information to understand and characterize the development sessions as they were carried out on two object-oriented systems.}
}

@InProceedings{2007:fase:robbes,
  Title                    = {An approach to software evolution based on semantic change},
  Author                   = {Robbes, Romain and Lanza, Michele and Lungu, Mircea},
  Booktitle                = fase,
  Year                     = {2007},
  Pages                    = {27--41},

  Abstract                 = {The analysis of the evolution of software systems is a useful source of information for a variety of activities, such as reverse engineering, maintenance, and predicting the future evolution of these systems. Current software evolution research is mainly based on the information contained in versioning systems such as CVS and SubVersion. But the evolutionary information contained therein is incomplete and of low quality, hence limiting the scope of evolution research. It is incomplete because the historical information is only recorded at the explicit request of the developers (a commit in the classical checkin/checkout model). It is of low quality because the file-based nature of versioning systems leads to a view of software as being a set of files. In this paper we present a novel approach to software evolution analysis which is based on the recording of all semantic changes performed on a system, such as refactorings. We describe our approach in detail, and demonstrate how it can be used to perform fine-grained software evolution analysis.}
}

@InProceedings{2011:icse:robbes,
  Title                    = {A study of ripple effects in software ecosystems},
  Author                   = {Robbes, Romain and Lungu, Mircea},
  Booktitle                = icse,
  Year                     = {2011},
  Pages                    = {904--907},

  Abstract                 = {When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation---known as a ripple effect---is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes. Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on early results of such an empirical study of API changes that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating aroung the Squeak and Pharo software ecosystems: six years of evolution, nearly 3,000 contributors, and close to 2,500 distinct systems.}
}

@InProceedings{2008:wcre:robbes,
  Title                    = {Logical Coupling Based on Fine-Grained Change Information},
  Author                   = {Robbes, Romain and Pollet, Damien and Lanza, Michele},
  Booktitle                = wcre,
  Year                     = {2008},
  Pages                    = {42--46},

  Abstract                 = {Logical coupling reveals implicit dependencies between program entities, by measuring how often they changed together during development. Current approaches use coarse-grained change information extracted from the version control history of the software system under study. Entities that are registered as having changed during a commit transaction have their coupling increased by the same amount, regardless of how and how much they actually changed. We present several new logical coupling measures taking into account fine-grained semantic changes. We evaluate their respective accuracy compared to the classical logical coupling measure on two case studies; in particular, we evaluate how well the new measures can estimate logical coupling with less data. Results show that our approach based on fine-grained information greatly ameliorates the state-of-the-art of logical coupling detection.}
}

@Article{2013:tse:robillard,
  Title                    = {Automated {API} Property Inference Techniques},
  Author                   = {M. Robillard and E. Bodden and D. Kawrykow and M. Mezini and T. Ratchford},
  Journal                  = tse,
  Year                     = {2013},

  Month                    = may,
  Number                   = {5},
  Pages                    = {613--637},
  Volume                   = {39},

  Doi                      = {10.1109/TSE.2012.63}
}

@Article{2011:ese:robillard,
  Title                    = {A Field Study of {API} Learning Obstacles},
  Author                   = {Martin Robillard and Robert DeLine},
  Journal                  = ese,
  Year                     = {2011},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {703--732},
  Volume                   = {16},

  Abstract                 = {Large APIs can be hard to learn, and this can lead to decreased programmer productivity. But what makes APIs hard to learn? We conducted a mixed approach, multi-phased study of the obstacles faced by Microsoft developers learning a wide variety of new APIs. The study involved a combination of surveys and in-person interviews, and collected the opinions and experiences of over 440 professional developers. We found that some of the most severe obstacles faced by developers learning new APIs pertained to the documentation and other learning resources. We report on the obstacles developers face when learning new APIs, with a special focus on obstacles related to API documentation. Our qualitative analysis elicited five important factors to consider when designing API documentation: documentation of intent; code examples; matching APIs with scenarios; penetrability of the API; and format and presentation. We analyzed how these factors can be interpreted to prioritize API documentation development efforts.},
  Doi                      = {10.1007/s10664-010-9150-8}
}

@Article{2008:tosem:robillard,
  Title                    = {Topology analysis of software dependencies},
  Author                   = {Martin P. Robillard},
  Journal                  = tosem,
  Year                     = {2008},

  Month                    = aug,
  Number                   = {4},
  Pages                    = {18:1--18:36},
  Volume                   = {17},

  Abstract                 = {Before performing a modification task, a developer usually has to investigate the source code of a system to understand how to carry out the task. Discovering the code relevant to a change task is costly because it is a human activity whose success depends on a large number of unpredictable factors, such as intuition and luck. Although studies have shown that effective developers tend to explore a program by following structural dependencies, no methodology is available to guide their navigation through the thousands of dependency paths found in a nontrivial program. We describe a technique to automatically propose and rank program elements that are potentially interesting to a developer investigating source code. Our technique is based on an analysis of the topology of structural dependencies in a program. It takes as input a set of program elements of interest to a developer and produces a fuzzy set describing other elements of potential interest. Empirical evaluation of our technique indicates that it can help developers quickly select program elements worthy of investigation while avoiding less interesting ones.},
  Doi                      = {10.1145/13487689.13487691}
}

@InProceedings{2005:esec_fse:robillard,
  Title                    = {Automatic generation of suggestions for program investigation},
  Author                   = {Robillard, Martin P.},
  Booktitle                = esec_fse,
  Year                     = {2005},
  Pages                    = {11--20},

  Abstract                 = {Before performing a modification task, a developer usually has to investigate the source code of a system to understand how to carry out the task. Discovering the code relevant to a change task is costly because it is an inherently human activity whose success depends on a large number of unpredictable factors, such as intuition and luck. Although studies have shown that effective developers tend to explore a program by following structural dependencies, no methodology is available to guide their navigation through the typically hundreds of dependency paths found in a non-trivial program. In this paper, we propose a technique to automatically propose and rank program elements that are potentially interesting to a developer investigating source code. Our technique is based on an analysis of the topology of structural dependencies in a program. It takes as input a set of program elements of interest to a developer and produces a fuzzy set describing other elements of potential interest. Empirical evaluation of our technique indicates that it can help developers quickly select program elements worthy of investigation while avoiding less interesting ones.},
  Doi                      = {10.1145/1081706.1081711}
}

@Article{2004:tse:robillard,
  Title                    = {How Effective Developers Investigate Source Code: An Exploratory Study},
  Author                   = {Martin P. Robillard and Wesley Coelho and Gail C. Murphy},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {889--903},
  Volume                   = {30},

  Abstract                 = {Prior to performing a software change task, developers must discover and understand the subset of the system relevant to the task. Since the behavior exhibited by individual developers when investigating a software system is influenced by intuition, experience, and skill, there is often significant variability in developer effectiveness. To understand the factors that contribute to effective program investigation behavior, we conducted a study of five developers performing a change task on a medium-size open source system. We isolated the factors related to effective program investigation behavior by performing a detailed qualitative analysis of the program investigation behavior of successful and unsuccessful developers. We report on these factors as a set of detailed observations, such as evidence of the phenomenon of inattention blindness by developers skimming source code. In general, our results support the intuitive notion that a methodical and structured approach to program investigation is the most effective.},
  Doi                      = {10.1109/TSE.2004.101}
}

@Article{2010:jsmerp:robillard,
  Title                    = {Recommending change clusters to support software investigation: an empirical study},
  Author                   = {Robillard, Martin P. and Dagenais, Barth{\'e}l{\'e}my},
  Journal                  = jsmerp,
  Year                     = {2010},

  Month                    = apr,
  Number                   = {3},
  Pages                    = {143--164},
  Volume                   = {22},

  Abstract                 = {During software maintenance tasks, developers often spend a valuable amount of effort investigating source code. This effort can be reduced if tools are available to help developers navigate the source code effectively. We studied to what extent developers can benefit from information contained in clusters of change sets to guide their investigation of a software system. We defined change clusters as groups of change sets that have a certain amount of elements in common. Our analysis of 4200 change sets for seven different systems and covering a cumulative time span of over 17 years of development showed that less than one in five tasks overlapped with change clusters. Furthermore, a detailed qualitative analysis of the results revealed that only 13\% of the clusters associated with applicable change tasks were likely to be useful. We conclude that change clusters can only support a minority of change tasks, and should only be recommended if it is possible to do so at minimal cost to the developers.},
  Doi                      = {10.1002/smr.v22:3}
}

@Article{2007:tosem:robillard,
  Title                    = {Representing concerns in source code},
  Author                   = {Robillard, Martin P. and Murphy, Gail C.},
  Journal                  = tosem,
  Year                     = {2007},

  Month                    = feb,
  Number                   = {1},
  Pages                    = {3:1--3:38},
  Volume                   = {16},

  Abstract                 = {A software modification task often addresses several concerns. A concern is anything a stakeholder may want to consider as a conceptual unit, including features, nonfunctional requirements, and design idioms. In many cases, the source code implementing a concern is not encapsulated in a single programming language module, and is instead scattered and tangled throughout a system. Inadequate separation of concerns increases the difficulty of evolving software in a correct and cost-effective manner. Tomake it easier to modify concerns that are not well modularized, we propose an approach in which the implementation of concerns is documented in artifacts, called concern graphs. Concern graphs are abstract models that describe which parts of the source code are relevant to different concerns. We present a formal model for concern graphs and the tool support we developed to enable software developers to create and use concern graphs during software evolution tasks. We report on five empirical studies, providing evidence that concern graphs support views and operations that facilitate the task of modifying the code implementing scattered concerns, are cost-effective to create and use, and robust enough to be used with different versions of a software system.},
  Doi                      = {10.1145/1189748.1189751}
}

@InProceedings{2002:icse:robillard,
  Title                    = {Concern graphs: Finding and describing concerns using structural program dependencies},
  Author                   = {Robillard, Martin P. and Murphy, Gail C.},
  Booktitle                = icse,
  Year                     = {2002},
  Pages                    = {406--416},

  Abstract                 = {Many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. Existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. In this paper, we introduce the Concern Graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The abstraction used in a Concern Graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the Feature Exploration and Analysis tool (FEAT) that allows a developer to manipulate a concern representation extracted from a Java system, and to analyze the relationships of that concern to the code base. We have used this tool to find and describe concerns related to software change tasks. We have performed case studies to evaluate the feasibility, usability, and scalability of the approach. Our results indicate that Concern Graphs can be used to document a concern for change, that developers unfamiliar with Concern Graphs can use them effectively, and that the underlying technology scales to industrial-sized programs.},
  Doi                      = {10.1145/581339.581390}
}

@InProceedings{2005:etx:robillard,
  Title                    = {{ConcernMapper}: Simple view-based separation of scattered concerns},
  Author                   = {Robillard, Martin P. and Weigand-Warr, Fr{\'e}d{\'e}ric},
  Booktitle                = etx,
  Year                     = {2005},
  Pages                    = {65--69},

  Abstract                 = {We introduce ConcernMapper, an Eclipse plug-in for experimenting with techniques for advanced separation of concerns. ConcernMapper supports development and maintenance tasks involving scattered concerns by allowing developers to organize and view the code of a project in terms of high-level abstractions called concerns. ConcernMapper is also designed as an extensible platform intended to provide a simple way to store and query concern models created through a variety of approaches. This paper describes the user interface and internal architecture of ConcernMapper, and demonstrates how to write extensions for it.}
}

@InProceedings{2006:icoss:robles,
  Title                    = {Contributor Turnover in Libre Software Projects},
  Author                   = {Robles, Gregorio and Gonzalez-Barahona, Jesus},
  Booktitle                = icoss,
  Year                     = {2006},
  Pages                    = {273--286},
  Publisher                = {Springer},
  Series                   = ifip,
  Volume                   = {203},

  Abstract                 = {A common problem that management faces in software companies is the high instability of their staff. In libre (free, open source) software projects, the permanence of developers is also an open issue, with the potential of causing problems amplified by the self-organizing nature that most of them exhibit. Hence, human resources in libre software projects are even more difficult to manage: developers are in most cases not bound by a contract and, in addition, there is not a real management structure concerned about this problem. This raises some interesting questions with respect to the composition of development teams in libre software projects, and how they evolve over time. There are projects lead by their original founders (some sort of ``code gods"), while others are driven by several different developer groups over time (i.e. the project ``regenerates" itself). In this paper, we propose a quantitative methodology, based on the analysis of the activity in the source code management repositories, to study how these processes (developers leaving, developers joining) affect libre software projects. The basis of it is the analysis of the composition of the core group, the group of developers most active in a project, for several time lapses. We will apply this methodology to several large, well-known libre software projects, and show how it can be used to characterize them. In addition, we will discuss the lessons that can be learned, and the validity of our proposal.}
}

@InProceedings{2002:ssspr:robles-kelly,
  Title                    = {String Edit Distance, Random Walks and Graph Matching},
  Author                   = {Robles-Kelly, Antonio and Hancock, Edwin R.},
  Booktitle                = ssspr,
  Year                     = {2002},
  Pages                    = {104--112},
  Series                   = lncs,
  Volume                   = {2396},

  Abstract                 = {This paper shows how the eigenstructure of the adjacency matrix can be used for the purposes of robust graph-matching. We commence from the observation that the leading eigenvector of a transition probability matrix is the steady state of the associated Markov chain. When the transition matrix is the normalised adjacency matrix of a graph, then the leading eigenvector gives the sequence of nodes of the steady state random walk on the graph. We use this property to convert the nodes in a graph into a string where the node-order is given by the sequence of nodes visited in the random walk. We match graphs represented in this way, by finding the sequence of string edit operations which minimise edit distance.},
  Doi                      = {10.1007/3-540-70659-3_10}
}

@InProceedings{2008:rsse:romero-mariona,
  Title                    = {{SRRS}: A recommendation system for security requirements},
  Author                   = {Romero-Mariona, Jose and Ziv, Hadar and Richardson, Debra J.},
  Booktitle                = rsse,
  Year                     = {2008},

  Abstract                 = {Despite the availability of approaches to specifying security requirements, we have identified a lack of comparative studies of those approaches and, subsequently, lack of guidance and useful tools to determine the most appropriate approach for a specific project. In this paper we propose SRRS (Security Requirements Recommendation System), which takes user input about the most desirable characteristics for their project and recommends the most appropriate approach. We provide an example of applying our system, and show how the SRRS process works in general.},
  Doi                      = {10.1145/1454247.1454266}
}

@Article{2004:tosem:roshandel,
  Title                    = {Mae: A system model and environment for managing architectural evolution},
  Author                   = {Roshandel, Roshanak and van der Hoek, Andr\'{e} and Mikic-Rakic, Marija and Medvidovic, Nenad},
  Journal                  = tosem,
  Year                     = {2004},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {240--276},
  Volume                   = {13},

  Abstract                 = {As with any other artifact produced as part of the software life cycle, software architectures evolve and this evolution must be managed. One approach to doing so would be to apply any of a host of existing configuration management systems, which have long been used successfully at the level of source code. Unfortunately, such an approach leads to many problems that prevent effective management of architectural evolution. To overcome these problems, we have developed an alternative approach centered on the use of an integrated architectural and configuration management system model. Because the system model combines architectural and configuration management concepts in a single representation, it has the distinct benefit that all architectural changes can be precisely captured and clearly related to each other---both at the fine-grained level of individual architectural elements and at the coarse-grained level of architectural configurations. To support the use of the system model, we have developed Mae, an architectural evolution environment through which users can specify architectures in a traditional manner, manage the evolution of the architectures using a check-out/check-in mechanism that tracks all changes, select a specific architectural configuration, and analyze the consistency of a selected configuration. We demonstrate the benefits of our approach by showing how the system model and its accompanying environment were used in the context of several representative projects.},
  Doi                      = {10.1145/1018210.1018213}
}

@Article{1996:tochi:rosson,
  Title                    = {The reuse of uses in {S}malltalk programming},
  Author                   = {Mary Beth Rosson and John M. Carroll},
  Journal                  = tochi,
  Year                     = {1996},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {219--253},
  Volume                   = {3},

  Abstract                 = {Software reuse, a long-standing and refractory issue in software technology, has been specifically emphasized as an advantage of the object-oriented programming paradigm. We report an empirical study of expert Smalltalk programmers reusing user interface classes in small graphical applications. Our primary goal was to develop a qualitative characterization of expert reuse strategies that could be used to identify requirements for teaching and supporting reuse programming. A secondary interest was to demonstrate to these experts the Reuse View Matcher---a prototype reuse tool---and to collect some initial observations of this tool in use during reuse programming. We observed extensive ``reuse of uses" in the programmers' work: they relied heavily on code in expample applications that provided an implicit specification for reuse of the target class. We called this implicit specification a ``usage context." The programmers searched for relevant usage contexts early. They repeatedly evaluated the contextualized information to develop solution plans, and they borrowed and adapted it when the sample context suited their immediate reuse goals. The process of code development was highly dynamic and incremental; analysis and implementation were tightly interleaved, frequently driven by testing and debugging. These results are considered in terms of the tradeoffs that inhere in the reuse of uses and the teaching and tool support that might improve the efficiency and accuracy of this approach to reuse.},
  Doi                      = {10.1145/234526.234530}
}

@InProceedings{1993:ecoop:rosson,
  Title                    = {Active programming strategies in reuse},
  Author                   = {Mary Beth Rosson and John M. Carroll},
  Booktitle                = ecoop,
  Year                     = {1993},
  Pages                    = {4--20},
  Series                   = lncs,
  Volume                   = {707},

  Abstract                 = {In order to capitalize on the potential for software reuse in object-oriented programming, we must better understand the processes involved in software reuse. Our work addresses this need, analyzing four experienced Smalltalk programmers as they enhanced applications by reusing new classes. These were active programmers: rather than suspending programming activity to reflect on how to use the new components, they began work immediately, recruiting code from example usage contexts and relying heavily on the system debugger to guide them in applying the borrowed context. We discuss the implications of these findings for reuse documentation, programming instruction and tools to support reuse.},
  Doi                      = {10.1007/3-540-47910-4_2}
}

@Article{2003:tse:rothenberger,
  Title                    = {Strategies for software reuse: A principal component analysis of reuse practices},
  Author                   = {Marcus A. Rothenberger and Kevin J. Dooley and Uday R. Kulkarni and Nader Nada},
  Journal                  = tse,
  Year                     = {2003},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {825--837},
  Volume                   = {29},

  Abstract                 = {This research investigates the premise that the likelihood of success of software reuse efforts may vary with the reuse strategy employed and, hence, potential reuse adopters must be able to understand reuse strategy alternatives and their implications. We use survey data collected from 71 software development groups to empirically develop a set of six dimensions that describe the practices employed in reuse programs. The study investigates the patterns in which these practices co-occur in the real world, demonstrating that the dimensions cluster into five distinct reuse strategies, each with a different potential for reuse success. The findings provide a means to classify reuse settings and assess their potential for success.},
  Doi                      = {10.1109/TSE.2003.1232287}
}

@Article{2004:tosem:rothermel,
  Title                    = {On test suite composition and cost-effective regression testing},
  Author                   = {Rothermel, Gregg and Elbaum, Sebastian and Malishevsky, Alexey G. and Kallakuri, Praveen and Qiu, Xuemei},
  Journal                  = tosem,
  Year                     = {2004},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {277--331},
  Volume                   = {13},

  Abstract                 = {Regression testing is an expensive testing process used to revalidate software as it evolves. Various methodologies for improving regression testing processes have been explored, but the cost-effectiveness of these methodologies has been shown to vary with characteristics of regression test suites. One such characteristic involves the way in which test inputs are composed into test cases within a test suite. This article reports the results of controlled experiments examining the effects of two factors in test suite composition---test suite granularity and test input grouping---on the costs and benefits of several regression-testing-related methodologies: retest-all, regression test selection, test suite reduction, and test case prioritization. These experiments consider the application of several specific techniques, from each of these methodologies, across ten releases each of two substantial software systems, using seven levels of test suite granularity and two types of test input grouping. The effects of granularity, technique, and grouping on the cost and fault-detection effectiveness of regression testing under the given methodologies are analyzed. This analysis shows that test suite granularity significantly affects several cost-benefit factors for the methodologies considered, while test input grouping has limited effects. Further, the results expose essential tradeoffs affecting the relationship between test suite design and regression testing cost-effectiveness, with several implications for practice.},
  Doi                      = {10.1145/1027092.1027093}
}

@InProceedings{1995:interact:rouet,
  Title                    = {Documentation as part of design: Exploratory field studies},
  Author                   = {Rouet, Jean-Fran{\c{c}}ois and Deleuze-Dordron, Catherine and Bisseret, Andr{\'e}},
  Booktitle                = interact,
  Year                     = {1995},
  Pages                    = {213--216},

  Abstract                 = {TBD}
}

@Article{2009:scp:roy,
  Title                    = {Comparison and evaluation of code clone detection techniques and tools: A qualitative approach},
  Author                   = {Roy, Chanchal K. and Cordy, James R. and Koschke, Rainer},
  Journal                  = scp,
  Year                     = {2009},

  Month                    = {1 } # may,
  Number                   = {7},
  Pages                    = {470--495},
  Volume                   = {74},

  Abstract                 = {Over the last decade many techniques and tools for software clone detection have been proposed. In this paper, we provide a qualitative comparison and evaluation of the current state-of-the-art in clone detection techniques and tools, and organize the large amount of information into a coherent conceptual framework. We begin with background concepts, a generic clone detection process and an overall taxonomy of current techniques and tools. We then classify, compare and evaluate the techniques and tools in two different dimensions. First, we classify and compare approaches based on a number of facets, each of which has a set of (possibly overlapping) attributes. Second, we qualitatively evaluate the classified techniques and tools with respect to a taxonomy of editing scenarios designed to model the creation of Type-1, Type-2, Type-3 and Type-4 clones. Finally, we provide examples of how one might use the results of this study to choose the most appropriate clone detection tool or technique in the context of a particular set of goals and constraints. The primary contributions of this paper are: (1) a schema for classifying clone detection techniques and tools and a classification of current clone detectors based on this schema, and (2) a taxonomy of editing scenarios that produce different clone types and a qualitative evaluation of current clone detectors based on this taxonomy.},
  Doi                      = {10.1016/j.scico.2009.02.007}
}

@Article{2002:prl:rozenfeld,
  Title                    = {Scale-Free Networks on Lattices},
  Author                   = {Alejandro F. Rozenfeld and Reuven Cohen and Daniel ben-Avraham and Shlomo Havlin},
  Journal                  = prl,
  Year                     = {2002},

  Month                    = {1 } # nov,
  Number                   = {21},
  Pages                    = {218701:1--218701:4},
  Volume                   = {89},

  Abstract                 = {We suggest a method for embedding scale-free networks, with degree distribution $P(k) \sim k^{-\lambda}$, in regular Euclidean lattices accounting for geographical properties. The embedding is driven by a natural constraint of minimization of the total length of the links in the system. We find that all networks with $\lambda>2$ can be successfully embedded up to a (Euclidean) distance $\xi$ which can be made as large as desired upon the changing of an external parameter. Clusters of successive chemical shells are found to be compact (the fractal dimension is $d_f=d$), while the dimension of the shortest path between any two sites is smaller than 1: $d_{\text{min}}=(\lambda-2)/(\lambda-1-1/d)$, contrary to all other known examples of fractals and disordered lattices.},
  Doi                      = {10.1103/PhysRevLett.89.218701}
}

@InProceedings{2007:icse:runeson,
  Title                    = {Detection of Duplicate Defect Reports Using Natural Language Processing},
  Author                   = {Runeson, Per and Alexandersson, Magnus and Nyholm, Oskar},
  Booktitle                = icse,
  Year                     = {2007},
  Pages                    = {499--510},

  Abstract                 = {Defect reports are generated from various testing and development activities in software engineering. Sometimes two reports are submitted that describe the same problem, leading to duplicate reports. These reports are mostly written in structured natural language, and as such, it is hard to compare two reports for similarity with formal methods. In order to identify duplicates, we investigate using Natural Language Processing (NLP) techniques to support the identification. A prototype tool is developed and evaluated in a case study analyzing defect reports at Sony Ericsson Mobile Communications. The evaluation shows that about 2/3 of the duplicates can possibly be found using the NLP techniques. Different variants of the techniques provide only minor result differences, indicating a robust technology. User testing shows that the overall attitude towards the technique is positive and that it has a growth potential.},
  Doi                      = {10.1109/ICSE.2007.32}
}

@InProceedings{2007:wsc:sanchez,
  Title                    = {Fundamentals of simulation modeling},
  Author                   = {S{\'{a}}nchez, Paul J.},
  Booktitle                = wsc,
  Year                     = {2007},
  Pages                    = {54--62},

  Abstract                 = {We start with basic terminology and concepts of modeling, and decompose the art of modeling as a process. This overview of the process helps clarify when we should or should not use simulation models. We discuss some common missteps made by many inexperienced modelers, and propose a concrete approach for avoiding those mistakes. After a quick review random number and random variate generation, we view the simulation model as a black-box which transforms inputs to outputs. This helps frame the need for designed experiments to help us gain better understanding of the system being modeled.},
  Doi                      = {10.1109/WSC.2007.4419588}
}

@Article{1979:jcd:sackett,
  Title                    = {Bias in Analytic Research},
  Author                   = {David L. Sackett},
  Journal                  = jcd,
  Year                     = {1979},
  Number                   = {1--2},
  Pages                    = {51--63},
  Volume                   = {32},

  Abstract                 = {Case-control studies are highly attractive. They can be executed quickly and at low cost, even when the disorders of interest are rare. Furthermore, the execution of pilot case-control studies is becoming automated; strategies have been devised for the `computer scanning' of large files of hospital admission diagnoses and prior drug exposures, with more detailed analyses carried out in the same data set on an ad hoc basis [l]. As evidence of their growing popularity, when one original article was randomly selected from each issue of The New England Journal of Medicine, The Lancet, and the Journal of the American Medical Association for the years, 1956, 1966 and 1976, the proportion reporting case-control analytic studies increased fourfold over these two decades (2--8\%) whereas the proportion reporting cohort analytic studies fell by half (30--15\%); incidentally, a general trend toward fewer study subjects but more study authors was also noted [2].},
  Doi                      = {10.1016/0021-9681(79)90012-2}
}

@Article{1996:sen:sadler,
  Title                    = {Evaluating software engineering methods and tools---{Part~4}: The influence of human factors},
  Author                   = {Chris Sadler and Barbara Ann Kitchenham},
  Journal                  = sen,
  Year                     = {1996},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {11--13},
  Volume                   = {21},

  Abstract                 = {In previous articles, we have described the range of methods available if you want to evaluate a software engineering method/tool and the criteria you need to consider to select a method appropriate to your individual circumstances. In future articles we will describe some guidelines to help you perform quantitative case studies and feature analysis. However, in this article we would like to review some of the human factors issues that can affect an evaluation exercise.},
  Doi                      = {10.1145/235969.235972}
}

@InProceedings{2005:iwpse:sadou,
  Title                    = {A unified Approach for Software Architecture Evolution at different abstraction levels},
  Author                   = {Sadou, Nassima and Tamzalit, Dalila and Oussalah, Mourad},
  Booktitle                = iwpse,
  Year                     = {2005},
  Pages                    = {65--70},

  Abstract                 = {This paper presents a model for software architecture evolution, called SAEV (software architecture evolution model). A software architecture is defined through its architectural elements (components, connectors, configurations ..). We associate to these architectural elements three abstraction levels namely from the most abstract one: the meta level, the architectural level and the application one. SAEV offers a whole of concepts, which are evolution operations, evolution rules, evolution strategies and invariants, to describe and manage uniformly the evolution of architectures at the architectural level as well as at the application level. This is done independently of any description or implementation language. In addition, SAEV offers a uniform mechanism to carry out a given evolution at these different levels.}
}

@InProceedings{2005:ase:saff,
  Title                    = {Automatic test factoring for {J}ava},
  Author                   = {Saff, David and Artzi, Shay and Perkins, Jeff H. and Ernst, Michael D.},
  Booktitle                = ase,
  Year                     = {2005},
  Pages                    = {114--123},

  Abstract                 = {Test factoring creates fast, focused unit tests from slow system-wide tests; each new unit test exercises only a subset of the functionality exercised by the system test. Augmenting a test suite with factored unit tests should catch errors earlier in a test run.One way to factor a test is to introduce mock objects. If a test exercises a component T, which interacts with another component E (the ``environment"), the implementation of E can be replaced by a mock. The mock checks that T's calls to E are as expected, and it simulates E's behavior in response. We introduce an automatic technique for test factoring. Given a system test for T and E, and a record of T's and E's behavior when the system test is run, test factoring generates unit tests for T in which E is mocked. The factored tests can isolate bugs in T from bugs in E and, if E is slow or expensive, improve test performance or cost.Our implementation of automatic dynamic test factoring for the Java language reduces the running time of a system test suite by up to an order of magnitude.},
  Doi                      = {10.1145/1101908.1101927}
}

@InProceedings{2006:msr:sager,
  Title                    = {Detecting similar {J}ava classes using tree algorithms},
  Author                   = {Sager, Tobias and Bernstein, Abraham and Pinzger, Martin and Kiefer, Christoph},
  Booktitle                = msrw,
  Year                     = {2006},
  Pages                    = {65--71},

  Abstract                 = {Similarity analysis of source code is helpful during development to provide, for instance, better support for code reuse. Consider a development environment that analyzes code while typing and that suggests similar code examples or existing implementations from a source code repository. Mining software repositories by means of similarity measures enables and enforces reusing existing code and reduces the developing effort needed by creating a shared knowledge base of code fragments. In information retrieval similarity measures are often used to find documents similar to a given query document. This paper extends this idea to source code repositories. It introduces our approach to detect similar Java classes in software projects using tree similarity algorithms. We show how our approach allows to find similar Java classes based on an evaluation of three tree-based similarity measures in the context of five user-defined test cases as well as a preliminary software evolution analysis of a medium-sized Java project. Initial results of our technique indicate that it (1) is indeed useful to identify similar Java classes, (2)successfully identifies the ex ante and ex post versions of refactored classes, and (3) provides some interesting insights into within-version and between-version dependencies of classes within a Java project.},
  Doi                      = {10.1145/1137983.1138000}
}

@InProceedings{2006:oopsla:sahavechaphan,
  Title                    = {{XSnippet}: Mining For sample code},
  Author                   = {Naiyana Sahavechaphan and Kajal T. Claypool},
  Booktitle                = oopsla,
  Year                     = {2006},
  Pages                    = {413--430},

  Abstract                 = {It is common practice for software developers to use examples to guide development efforts. This largely unwritten, yet standard, practice of "develop by example" is often supported by examples bundled with library or framework packages, provided in textbooks, and made available for download on both official and unofficial web sites. However, the vast number of examples that are embedded in the billions of lines of already developed library and framework code are largely untapped. We have developed XSnippet, a context-sensitive code assistant framework that allows developers to query a sample repository for code snippets that are relevant to the programming task at hand. In particular, our work makes three primary contributions. First, a range of queries is provided to allow developers to switch between a context-independent retrieval of code snippets to various degrees of context-sensitive retrieval for object instantiation queries. Second, a novel graph-based code mining algorithm is provided to support the range of queries and enable mining within and across method boundaries. Third, an innovative context-sensitive ranking heuristic is provided that has been experimentally proven to provide better ranking for best-fit code snippets than context-independent heuristics such as shortest path and frequency. Our experimental evaluation has shown that XSnippet has significant potential to assist developers, and provides better coverage of tasks and better rankings for best-fit snippets than other code assistant systems.}
}

@Book{2013:book:saldana,
  Title                    = {The Coding Manual for Qualitative Researchers},
  Author                   = {Johnny Saldana},
  Publisher                = {Sage Publications},
  Year                     = {2013},
  Edition                  = {2nd}
}

@Book{1997:book:sametinger,
  Title                    = {Software Engineering with Reusable Components},
  Author                   = {Johannes Sametinger},
  Publisher                = {Springer},
  Year                     = {1997},
  Month                    = mar,

  Abstract                 = {Software is rarely built completely from scratch. To a great extent, existing software documents (source code, design documents, etc.) are copied and adapted to fit new requirements. Yet we are far from the goal of making reuse the standard approach to software development. Software reuse is the process of creating software systems from existing software rather than building them from scratch. Software reuse is still an emerging discipline. It appears in many different forms from ad-hoc reuse to systematic reuse, and from white-box reuse to black-box reuse. Many different products for reuse range from ideas and algorithms to any documents that are created during the software life cycle. Source code is most commonly reused; thus many people misconceive software reuse as the reuse of source code alone. Recently source code and design reuse have become popular with (object-oriented) class libraries, application frameworks, and design patterns. Software components provide a vehicle for planned and systematic reuse. The software community does not yet agree on what a software component is exactly. Nowadays, the term component is used as a synonym for object most of the time, but it also stands for module or function. Recently the term component-based or component-oriented software development has become popular. In this context components are defined as objects plus something. What something is exactly, or has to be for effective software development, remains yet to be seen. However, systems and models are emerging to support that notion. Systematic software reuse and the reuse of components influence almost the whole software engineering process (independent of what a component is). Software process models were developed to provide guidance in the creation of high-quality software systems by teams at predictable costs. The original models were based on the (mis)conception that systems are built from scratch according to stable requirements. Software process models have been adapted since based on experience, and several changes and improvements have been suggested since the classic waterfall model. With increasing reuse of software, new models for software engineering are emerging. New models are based on systematic reuse of well-defined components that have been developed in various projects.},
  Url                      = {http://www.swe.uni-linz.ac.at/publications/pdf/TR-SE-97.04.pdf}
}

@Article{1998:jru:samuelson,
  Title                    = {Status quo bias in decision making},
  Author                   = {William Samuelson and Richard Zeckhauser},
  Journal                  = jru,
  Year                     = {1998},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {7--59},
  Volume                   = {1},

  Abstract                 = {Most real decisions, unlike those of economics texts, have a status quo alternative---that is, doing nothing or maintaining one's current or previous decision. A series of decision-making experiments shows that individuals disproportionately stick with the status quo. Data on the selections of health plans and retirement programs by faculty members reveal that the status quo bias is substantial in important real decisions. Economics, psychology, and decision theory provide possible explanations for this bias. Applications are discussed ranging from marketing techniques, to industrial organization, to the advance of science.},
  Doi                      = {10.1007/BF00055564}
}

@InProceedings{2005:group:sandusky,
  Title                    = {Negotiation and the coordination of information and activity in distributed software problem management},
  Author                   = {Sandusky, Robert J. and Gasser, Les},
  Booktitle                = group,
  Year                     = {2005},
  Pages                    = {187--196},

  Abstract                 = {Publicly accessible bug report repositories maintained by free / open source development communities provide vast stores of data about distributed software problem management (SWPM). Qualitative analysis of individual bug reports, texts that record community responses to reported software problems, shows how this distributed community uses its SWPM process to manage software quality. We focus on the role of one basic social process, negotiation, in SWPM. We report on the varieties and frequencies of negotiation practices and demonstrate how instances of negotiation in different contexts affect the organization of information, the allocation of community resources, and the disposition of software problems.}
}

@InProceedings{2005:oopsla:sangal:a,
  Title                    = {Using dependency models to manage complex software architecture},
  Author                   = {Sangal, Neeraj and Jordan, Ev and Sinha, Vineet and Jackson, Daniel},
  Booktitle                = oopsla,
  Year                     = {2005},
  Pages                    = {167--176},

  Abstract                 = {An approach to managing the architecture of large software systems is presented. Dependencies are extracted from the code by a conventional static analysis, and shown in a tabular form known as the Dependency Structure Matrix (DSM). A variety of algorithms are available to help organize the matrix in a form that reflects the architecture and highlights patterns and problematic dependencies. A hierarchical structure obtained in part by such algorithms, and in part by input from the user, then becomes the basis for ``design rules'' that capture the architect's intent about which dependencies are acceptable. The design rules are applied repeatedly as the system evolves, to identify violations, and keep the code and its architecture in conformance with one another. The analysis has been implemented in a tool called LDM which has been applied in several commercial projects; in this paper, a case study application to Haystack, an information retrieval system, is described.},
  Doi                      = {10.1145/1094811.1094824}
}

@InProceedings{2005:oopsla:sangal:b,
  Title                    = {Using dependency models to manage software architecture},
  Author                   = {Sangal, Neeraj and Jordan, Ev and Sinha, Vineet and Jackson, Daniel},
  Booktitle                = oopslacomp,
  Year                     = {2005},
  Pages                    = {164--165},

  Abstract                 = {This demonstration will present a new approach, based on the Dependency Structure Matrix (DSM), which uses inter-module dependencies to specify and manage the architecture of software systems. The system is decomposed into a hierarchy of subsystems with the dependencies between the subsystems presented in the form of an adjacency matrix. The matrix representation is concise, intuitive and appears to overcome scaling problems that are commonly associated with directed graph representations. It also permits succinct definition of design rules to specify allowable dependencies.A tool, Lattix LDM, will be used to demonstrate this approach by loading actual open source Java applications to create DSMs that can represent systems with thousands of classes. We will show how algorithms can be applied to organize the matrix in a form that reflects the architecture and highlights problematic dependencies.We will demonstrate how design rules can be used to specify and enforce architectural patterns such as layering and componentization. We will examine the evolution of architecture by creating dependency models for successive generations of Ant, a popular Java utility. Finally, we will explore the application of this approach to the re-engineering of Haystack, an information retrieval system.},
  Doi                      = {10.1145/1094855.1094915}
}

@Article{2007:tse:sarkar,
  Title                    = {{API}-Based and Information-Theoretic Metrics for Measuring the Quality of Software Modularization},
  Author                   = {Sarkar, Santonu and Rama, Girish Maskeri and Kak, Avinash C.},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {14--32},
  Volume                   = {33},

  Abstract                 = {We present in this paper a new set of metrics that measure the quality of modularization of a non-object-oriented software system. We have proposed a set of design principles to capture the notion of modularity and defined metrics centered around these principles. These metrics characterize the software from a variety of perspectives: structural, architectural, and notions such as the similarity of purpose and commonality of goals. (By structural, we are referring to intermodule coupling-based notions, and by architectural, we mean the horizontal layering of modules in large software systems.) We employ the notion of API (application programming interface) as the basis for our structural metrics. The rest of the metrics we present are in support of those that are based on API. Some of the important support metrics include those that characterize each module on the basis of the similarity of purpose of the services offered by the module. These metrics are based on information-theoretic principles. We tested our metrics on some popular open-source systems and some large legacy-code business applications. To validate the metrics, we compared the results obtained on human-modularized versions of the software (as created by the developers of the software) with those obtained on randomized versions of the code. For randomized versions, the assignment of the individual functions to modules was randomized.},
  Doi                      = {10.1109/TSE.2007.256942}
}

@InProceedings{2003:icse:sarma,
  Title                    = {Palant{\'\i}r: Raising awareness among configuration management workspaces},
  Author                   = {Sarma, Anita and Noroozi, Zahra and van der Hoek, Andr{\'e}},
  Booktitle                = icse,
  Year                     = {2003},
  Pages                    = {444--454},

  Abstract                 = {Current configuration management systems promote workspaces that isolate developers from each other. This isolation is both good and bad It is good, because developers make their changes without any interference from changes made concurrently by other developers. It is bad, because not knowing which artifacts are changing in parallel regularly leads to problems when changes are promoted from workspaces into a central configuration management repository. Overcoming the bad isolation, while retaining the good isolation, is a matter of raising awareness among developers, an issue traditionally ignored by the discipline of configuration management. To fill this void, we have developed Palantir, a novel workspace awareness tool that complements existing configuration management systems by providing developers with insight into other workspaces. In particular, the tool informs a developer of which other developers change which other artifacts, calculates a simple measure of severity of those changes, and graphically displays the information in a configurable and generally non-obtrusive manner. To illustrate the use of Palantir, we demonstrate how it integrates with two representative configuration management systems.},
  Doi                      = {10.1109/ICSE.2003.1201222}
}

@InProceedings{2008:icse:schafer,
  Title                    = {Mining framework usage changes from instantiation code},
  Author                   = {Thorsten Sch{\"a}fer and Jan Jonas and Mira Mezini},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {471--480},

  Abstract                 = {Framework evolution may break existing users, which need to be migrated to the new framework version. This is a tedious and error-prone process that benefits from automation. Existing approaches compare two versions of the framework code in order to find changes caused by refactorings. However, other kinds of changes exist, which are relevant for the migration. In this paper, we propose to mine framework usage change rules from already ported instantiations, the latter being applications build on top of the framework, or test cases maintained by the framework developers. Our evaluation shows that our approach finds usage changes not only caused by refactorings, but also by conceptual changes within the framework. Further, it copes well with some issues that plague tools focusing on finding refactorings such as deprecated program elements or multiple changes applied to a single program element.},
  Doi                      = {10.1145/1368088.1368153}
}

@Article{2007:csrev:schaeffer,
  Title                    = {Graph clustering},
  Author                   = {Satu Elisa Schaeffer},
  Journal                  = csrev,
  Year                     = {2007},

  Month                    = aug,
  Number                   = {1},
  Pages                    = {27--64},
  Volume                   = {1},

  Abstract                 = {In this survey we overview the definitions and methods for graph clustering, that is, finding sets of ``related'' vertices in graphs. We review the many definitions for what is a cluster in a graph and measures of cluster quality. Then we present global algorithms for producing a clustering for the entire vertex set of an input graph, after which we discuss the task of identifying a cluster for a specific seed vertex by local computation. Some ideas on the application areas of graph clustering algorithms are given. We also address the problematics of evaluating clusterings and benchmarking cluster algorithms.},
  Doi                      = {10.1016/j.cosrev.2007.05.001}
}

@Article{2006:tse:schafer,
  Title                    = {The {SEXTANT} software exploration tool},
  Author                   = {Thorsten Schafer and Michael Eichberg and Michael Haupt and Mira Mezini},
  Journal                  = tse,
  Year                     = {2006},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {753--768},
  Volume                   = {32},

  Abstract                 = {In this paper, we discuss a set of functional requirements for software exploration tools and provide initial evidence that various combinations of these features are needed to effectively assist developers in understanding software. We observe that current tools for software exploration only partly support these features. This has motivated the development of SEXTANT, a software exploration tool tightly integrated into the Eclipse IDE that has been developed to fill this gap. By means of case studies, we demonstrate how the requirements fulfilled by SEXTANT are conducive to an understanding needed to perform a maintenance task.},
  Doi                      = {10.1109/TSE.2006.94}
}

@InProceedings{1998:hase:schenker,
  Title                    = {The application of fuzzy enhanced case-based reasoning for identifying fault-prone modules},
  Author                   = {Schenker, D. F. and T. M. Khoshgoftaar},
  Booktitle                = hase,
  Year                     = {1998},
  Pages                    = {90--97},

  Abstract                 = {As highly reliable software is becoming an essential ingredient in many systems, the process of assuring reliability can be a time-consuming, costly process. One way to improve the efficiency of the quality assurance process is to target reliability enhancement activities to those modules that are likely to have the most problems. Within the field of software engineering, much research has been performed to allow developers to identify fault-prone modules within a project. Software quality classification models can select the modules that are the most likely to contain faults so that reliability enhancement activities can be performed to lower the occurrences of software faults and errors. This paper introduces fuzzy logic combined with case-based reasoning (CBR) to determine fault-prone modules given a set of software metrics. Combining these two techniques promises more robust, flexible and accurate models. In this paper, we describe this approach, apply it in a real-world case study and discuss the results. The case study applied this approach to software quality modeling using data from a military command, control and communications (C$^3$) system. The fuzzy CBR model had an overall classification accuracy of more than 85\%. This paper also discusses possible improvements and enhancements to the initial model that can be explored in the future.}
}

@Article{1996:cscwjcc:schmidt,
  Title                    = {Coordination mechanisms: Towards a conceptual foundation of {CSCW} systems design},
  Author                   = {Schmidt, Kjeld and Simone, Carla},
  Journal                  = cscwjcc,
  Year                     = {1996},
  Number                   = {2--3},
  Pages                    = {155--200},
  Volume                   = {5},

  Abstract                 = {The paper outlines an approach to CSCW systems design based on the concept of `coordination mechanisms.' The concept of coordination mechanisms has been developed as a generalization of phenomena described in empirical invetigations of the use of artifacts for the purpose of coordinating cooperative activities in different work domains. On the basis of the evidence of this corpus of empirical studies, the paper outlines a theory of the use of artifacts for coordination purposes in cooperative work settings, derives a set of general requirements for computational coordination mechanisms, and sketches the architecture of Ariadne, a CSCW infrastructure for constructing and running such malleable and linkable computational coordination mechanisms.},
  Doi                      = {10.1007/BF00133655}
}

@InProceedings{2006:icpc:schofield,
  Title                    = {Digging the Development Dust for Refactorings},
  Author                   = {Schofield, Curtis and Tansey, Brendan and Xing, Zhenchang and Stroulia, Eleni},
  Booktitle                = icpc,
  Year                     = {2006},
  Pages                    = {23--34},

  Abstract                 = {Software repositories are rich sources of information about the software development process. Mining the information stored in them has been shown to provide interesting insights into the history of the software development and evolution. Several different types of information have been extracted and analyzed from different points of view. However, these types of information have not been sufficiently cross-examined to understand how they might complement each other. In this paper, we present a systematic analysis of four aspects of the software repository of an open source project - source-code metrics, identifiers, return-on-investment estimates, and design differencing - to collect evidence about refactorings that may have happened during the project development. In the context of this case study, we comparatively examine how informative each piece of information is towards understanding the refactoring history of the project and how costly it is to obtain.}
}

@InProceedings{2006:isese:schroeter,
  Title                    = {If Your Bug Database Could Talk...},
  Author                   = {Schr{\"o}ter, Adrian and Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  Booktitle                = isese,
  Year                     = {2006},
  Pages                    = {18--20},
  Volume                   = {2},

  Abstract                 = {We have mined the Eclipse bug and version databases to map failures to Eclipse components. The resulting data set lists the defect density of all Eclipse components. As we demonstrate in three simple experiments, the bug data set can be easily used to relate code, process, and developers to defects. The data set is publicly available for download.}
}

@InProceedings{2010:csmr:schrettner,
  Title                    = {Development of a Methodology, Software-Suite and Service for Supporting Software Architecture Reconstruction},
  Author                   = {Schrettner, Lajos and Hegedus, Peter and Ferenc, Rudolf and Fulop, Lajos Jeno and Bakota, Tibor},
  Booktitle                = csmr,
  Year                     = {2010},
  Pages                    = {190--193},

  Abstract                 = {Having an up-to-date knowledge of the architecture of a software system is of primary importance, since it affects every aspect of software development. It aids under-standing the system, helps defining high level conditions and constraints for making decisions, supports dependency analysis, logical grouping of components, evaluation of high level design, etc. During the evolution of a software, the documentation of its architecture may not be maintained because of the strict deadlines, resulting in an increasing gap between the architectural design and implementation. The national grant project named GOP-1.1.1-07/1-2008-0077 sponsored by the New Hungarian Development Plan, supports the development of appropriate tools for automatic architecture reconstruction and reverse engineering of software systems. The project will result in a complex solution for automatic architecture reconstruction of software systems by offering both a flexible and highly customizable set of services and a state-of-the-art boxed product. On one hand, architecture reconstruction in the scope of the project deals with visualization of the components and their relations. On the other hand, tracking the changes of the architectural elements during software evolution will also be supported. The tools of the project are being developed by FrontEndART Ltd. while the theoretical and technological background is provided by the Department of Software Engineering at University of Szeged.}
}

@Book{2001:book:schwaber,
  Title                    = {Agile Software Development with {S}crum},
  Author                   = {Schwaber, Ken and Beedle, Mike},
  Publisher                = {Prentice Hall},
  Year                     = {2001}
}

@InProceedings{1989:iwssd:schwanke,
  Title                    = {Discovering, visualizing, and controlling software structure},
  Author                   = {Schwanke, R. W. and Altucher, R. Z. and Platoff, M. A.},
  Booktitle                = iwssd,
  Year                     = {1989},
  Pages                    = {147--154},

  Abstract                 = {Although many good principles, methods, and notations for large system design have been developed in the last two decades, they have seen little application to software system maintenance. The reasons seem to be the large cost and the diminishing value of writing specifications after the code already exists. This paper describes research that is attempting to bridge the gap between software design and software maintenance. The MAINTAINER'S ASSISTANT project at Siemens Research is researching ways to make the software maintainer more productive, in part by helping him contend with the enormous amounts of information in the project database. A sub-project, ARCH, is developing a new approach to controlling the structure of a large software system during maintenance. The approach assumes that a large body of code may already exist before the structure is formally specified. The ARCH methodology includes \begin{itemize} \item a practical notation for specifying software structure, a graph editor to help the architect visualize and construct the specification, \item a tool that extracts the actual structure of a system and compares it to the specification, and \item an automatic classification tool that helps an architect analyze existing code to discover an appropriate structure specification. \end{itemize} This paper describes the specification method used in ARCH, the condensation method that aids visualization, and the classification tool, which uses conceptual clustering to help the architect discover the structure of the system. The clustering is based on detailed cross-reference information, which we hypothesize provides useful ``light semantic'' profiles of the parts of a system. The paper includes preliminary results from some experiments in analyzing a real system, and describes the full-scale experiment we plan to carry out next.},
  Doi                      = {10.1145/75200.75223}
}

@InProceedings{2011:esa:schweitzer,
  Title                    = {Isomorphism of (mis)labeled graphs},
  Author                   = {Schweitzer, Pascal},
  Booktitle                = esa,
  Year                     = {2011},
  Pages                    = {370--381},
  Series                   = lncs,
  Volume                   = {6942},

  Abstract                 = {For similarity measures of labeled and unlabeled graphs, we study the complexity of the graph isomorphism problem for pairs of input graphs which are close with respect to the measure. More precisely, we show that for every fixed integer $k$ we can decide in quadratic time whether a labeled graph $G$ can be obtained from another labeled graph $H$ by relabeling at most $k$ vertices. We extend the algorithm solving this problem to an algorithm determining the number $\ell$ of vertices that must be deleted and the number $k$ of vertices that must be relabeled in order to make the graphs equivalent. The algorithm is fixed-parameter tractable in $k + \ell$. Contrasting these tractability results, we also show that for those similarity measures that change only by finite amount $d$ whenever one edge is relocated, the problem of deciding isomorphism of input pairs of bounded distance $d$ is equivalent to solving graph isomorphism in general.},
  Doi                      = {10.1007/978-3-642-23719-5\_32}
}

@Article{2002:csur:sebastiani,
  Title                    = {Machine learning in automated text categorization},
  Author                   = {Fabrizio Sebastiani},
  Journal                  = csur,
  Year                     = {2002},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {1--47},
  Volume                   = {34},

  Abstract                 = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.},
  Doi                      = {10.1145/505282.505283}
}

@Article{2005:tse:selby,
  Title                    = {Enabling reuse-based software development of large-scale systems},
  Author                   = {Richard W. Selby},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {495--510},
  Volume                   = {31},

  Abstract                 = {Software reuse enables developers to leverage past accomplishments and facilitates significant improvements in software productivity and quality. Software reuse catalyzes improvements in productivity by avoiding redevelopment and improvements in quality by incorporating components whose reliability has already been established. This study addresses a pivotal research issue that underlies software reuse---what factors characterize successful software reuse in large-scale systems. The research approach is to investigate, analyze, and evaluate software reuse empirically by mining software repositories from a NASA software development environment that actively reuses software. This software environment successfully follows principles of reuse-based software development in order to achieve an average reuse of 32 percent per project, which is the average amount of software either reused or modified from previous systems. We examine the repositories for 25 software systems ranging from 3,000 to 112,000 source lines from this software environment. We analyze four classes of software modules: modules reused without revision, modules reused with slight revision ($<$25 percent revision), modules reused with major revision ($\ge$25 percent revision), and newly developed modules. We apply nonparametric statistical models to compare numerous development variables across the 2,954 software modules in the systems. We identify two categories of factors that characterize successful reuse-based software development of large-scale systems: module design factors and module implementation factors. We also evaluate the fault rates of the reused, modified, and newly developed modules. The module design factors that characterize module reuse without revision were (after normalization by size in source lines): few calls to other system modules, many calls to utility functions, few input-output parameters, few reads and writes, and many comments. The module implementation factors that characterize module reuse without revision were small size in source lines and (after normalization by size in source lines): low development effort and many assignment statements. The modules reused without revision had the fewest faults, fewest faults per source line, and lowest fault correction effort. The modules reused with major revision had the highest fault correction effort and highest fault isolation effort as wed as the most changes, most changes per source line, and highest change correction effort. In conclusion, we outline future research directions that build on these software reuse ideas and strategies.},
  Doi                      = {10.1109/TSE.2005.69}
}

@Article{1997:tse:sen,
  Title                    = {The role of opportunism in the software design reuse process},
  Author                   = {Arun Sen},
  Journal                  = tse,
  Year                     = {1997},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {418--436},
  Volume                   = {23},

  Abstract                 = {Software design involves translating a set of task requirements into a structured description of a computer program that will perform the task. A software designer can use design schema, collaborative design knowledge, or can reuse design artifacts. Very little has been done to include reuse of design artifacts in the software development life cycle, despite tremendous promises of reuse. As a result, this technique has not seen widespread use, possibly due to a lack of cognitive understanding of the reuse process. This research explores the role of a specific cognitive aspect, opportunism, in demand-side software reuse. We propose a cognitive model based on opportunism that describes the software design process with reuse. Protocol analysis verifies that the software design with reuse is indeed opportunistic and reveals that some software designers employ certain tasks of the reuse process frequently. Based on these findings, we propose a reuse support system that incorporates blackboard technology and existing reuse library management system.},
  Doi                      = {10.1109/32.605760}
}

@InProceedings{2012:csmr:shah,
  Title                    = {Making Smart Moves to Untangle Programs},
  Author                   = {Shah, Syed M. Ali and Dietrich, Jens and McCartin, Catherine},
  Booktitle                = csmr,
  Year                     = {2012},
  Pages                    = {359--364},

  Doi                      = {10.1109/CSMR.2012.44}
}

@InProceedings{2012:sc:shah,
  Title                    = {On the automated modularisation of {J}ava programs using service locators},
  Author                   = {Shah, Syed Muhammad Ali and Dietrich, Jens and McCartin, Catherine},
  Booktitle                = sc,
  Year                     = {2012},
  Pages                    = {132--147},
  Series                   = lncs,
  Volume                   = {7306},

  Abstract                 = {Service locator is a popular design pattern that facilitates building modular and reconfigurable systems. We investigate how existing monolithic systems can be automatically refactored using this pattern into more modular architectures, and measure the benefits of doing so. We present an Eclipse plugin we have developed for this purpose.},
  Doi                      = {10.1007/978-3-642-30564-1_9}
}

@InProceedings{2010:avi:shannon,
  Title                    = {Deep {D}iffs: Visually exploring the history of a document},
  Author                   = {Shannon, Ross and Quigley, Aaron and Nixon, Paddy},
  Booktitle                = aviic,
  Year                     = {2010},
  Pages                    = {361--364},

  Abstract                 = {Software tools are used to compare multiple versions of a textual document to help a reader understand the evolution of that document over time. These tools generally support the comparison of only two versions of a document, requiring multiple comparisons to be made to derive a full history of the document across multiple versions. We present Deep Diffs, a novel visualisation technique that exposes the multiple layers of history of a document at once, directly in the text, highlighting areas that have changed over multiple successive versions, and drawing attention to passages that are new, potentially unpolished or contentious. These composite views facilitate the writing and editing process by assisting memory and encouraging the analysis of collaboratively-authored documents. We describe how this technique effectively supports common text editing tasks and heightens participants' understanding of the process in collaborative editing scenarios like wiki editing and paper writing.},
  Doi                      = {10.1145/1842993.1843063}
}

@InProceedings{1989:iwssd:shaw,
  Title                    = {Larger scale systems require higher-level abstractions},
  Author                   = {Shaw, Mary},
  Booktitle                = iwssd,
  Year                     = {1989},
  Number                   = {3},
  Pages                    = {143--146},
  Volume                   = {14},

  Abstract                 = {Over the past thirty years, abstraction techniques such as high level programming languages and abstract data types have improved our ability to specify and develop software. However, the increasing size and complexity of software systems have introduced new problems that are not solved by the current techniques. These new problems involve the system-level design of software, in which the important decisions are concerned with the kinds of modules and subsystems to use and the way these modules and subsystems are organized. This level of organization, the software archirecrure level, requires new kinds of abstractions that capture essential properties of major subsystems and the ways they interact.},
  Doi                      = {10.1145/75200.75222}
}

@Article{1995:tse:shaw,
  Title                    = {Abstractions for Software Architecture and Tools to Support Them},
  Author                   = {Mary Shaw and Robert DeLine and Daniel V. Klein and Theodore L. Ross and David M. Young and Gregory Zelesnik},
  Journal                  = tse,
  Year                     = {1995},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {314--335},
  Volume                   = {21},

  Abstract                 = {Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition of components into systems. These abstractions are higher level than the elements usually supported by programming languages and tools. They capture packaging and interaction issues as well as computational functionality. Well-established (if informal) patterns guide architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions used in practice by software designers. The implementation provides a testbed for experiments with a variety of system construction mechanisms. It distinguishes among different types of components and different ways these components can interact. It supports abstract interactions such as data flow and scheduling on the same footing as simple procedure call. It can express and check appropriate compatibility restrictions and configuration constraints. It accepts existing code as components, incurring no runtime overhead after initialization. It allows easy incorporation of specifications and associated analysis tools developed elsewhere. The implementation provides a base for extending the notation and validating the model.},
  Doi                      = {10.1109/32.385970}
}

@InCollection{2003:book:aurum:shepperd,
  Title                    = {Case-based reasoning and software engineering},
  Author                   = {Martin Shepperd},
  Booktitle                = {Managing Software Engineering Knowledge},
  Publisher                = {Springer},
  Year                     = {2003},
  Chapter                  = {9},
  Editor                   = {Ayb{\"u}ke Aurum and Ross Jeffery and Claes Wohlin and Meliha Handzic},

  Abstract                 = {Case-based reasoning (CBR) was first formalised in the 1980s following from the work of Schank and others on memory [1], and is based upon the fundamental premise that similar problems are best solved with similar solutions [2]. The idea is to learn from experience. However, a crucial aspect of CBR lies in the term ``similar". The technique does not require an identical problem to have been previously solved. Also CBR differs from many other artificial intelligence techniques in that it is not model based. This means, unlike knowledge based approaches that use rules, the developer does not have to explicitly define causalities and relationships within the domain of interest. For poorly understood problem domains this is a major benefit.},
  Doi                      = {10.1007/978-3-662-05129-0_9}
}

@InProceedings{2001:metrics:shepperd,
  Title                    = {Using Simulation to Evaluate Prediction Techniques},
  Author                   = {Shepperd, Martin and Kadoda, Gada},
  Booktitle                = metrics,
  Year                     = {2001},
  Pages                    = {349--359},

  Abstract                 = {The need for accurate software prediction systems increases as software becomes much larger and more complex. A variety of techniques have been proposed, however, none has proved consistently accurate and there is still much uncertainty as to what technique suits which type of prediction problem. We believe that the underlying characteristics---size, number of features, type of distribution, etc.---of the dataset influence the choice of the prediction system to be used. In previous work, it has proved difficult to obtain significant results over small datasets. Consequently we required large validation datasets, moreover, we wished to control the characteristics of such datasets in order to systematically explore the relationship between accuracy, choice of prediction system and dataset characteristic. Our solution has been to simulate data allowing both control and the possibility of large (1000) validation cases. In this paper we compared regression, rule induction and nearest neighbour (a form of case based reasoning). The results suggest that there are significant differences depending upon the characteristics of the dataset. Consequently researchers should consider prediction context when evaluating competing prediction systems. We also observed that the more ``messy" the data and the more complex the relationship with the dependent variable the more variability in the results. This became apparent since we sampled two different training sets from each simulated population of data. In the more complex cases we observed significantly different results depending upon the training set. This suggests that researchers will need to exercise caution when comparing diferent approaches and utilise procedures such as bootstrapping in order to generate multiple samples for training purposes.},
  Doi                      = {10.1109/METRIC.2001.915542}
}

@Article{1997:tse:shepperd,
  Title                    = {Estimating software project effort using analogies},
  Author                   = {Shepperd, Martin and Chris Schofield},
  Journal                  = tse,
  Year                     = {1997},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {736--743},
  Volume                   = {23},

  Abstract                 = {Accurate project effort prediction is an important goal for the software engineering community. To date most work has focused upon building algorithmic models of effort, for example COCOMO. These can be calibrated to local environments. We describe an alternative approach to estimation based upon the use of analogies. The underlying principle is to characterize projects in terms of features (for example, the number of interfaces, the development method or the size of the functional requirements document). Completed projects are stored and then the problem becomes one of finding the most similar projects to the one for which a prediction is required. Similarity is defined as Euclidean distance in n-dimensional space where n is the number of project features. Each dimension is standardized so all dimensions have equal weight. The known effort values of the nearest neighbors to the new project are then used as the basis for the prediction. The process is automated using a PC-based tool known as ANGEL. The method is validated on nine different industrial datasets (a total of 275 projects) and in all cases analogy outperforms algorithmic models based upon stepwise regression. From this work we argue that estimation by analogy is a viable technique that, at the very least, can be used by project managers to complement current estimation techniques.},
  Doi                      = {10.1109/32.637387}
}

@Book{1980:book:shneiderman,
  Title                    = {Software Psychology: Human Factors in Computer and Information Systems
},
  Author                   = {Shneiderman, Ben},
  Publisher                = {Winthrop Publishers},
  Year                     = {1980}
}

@Article{2008:tse:sillito,
  Title                    = {Asking and Answering Questions during a Programming Change Task},
  Author                   = {Sillito, Jonathan and Murphy, Gail C. and De Volder, Kris},
  Journal                  = tse,
  Year                     = {2008},

  Month                    = jul # {--} # aug,
  Number                   = {4},
  Pages                    = {434--451},
  Volume                   = {34},

  Abstract                 = {Little is known about the specific kinds of questions programmers ask when evolving a code base and how well existing tools support those questions. To better support the activity of programming, answers are needed to three broad research questions: 1) What does a programmer need to know about a code base when evolving a software system? 2) How does a programmer go about finding that information? 3) How well do existing tools support programmers in answering those questions? We undertook two qualitative studies of programmers performing change tasks to provide answers to these questions. In this paper, we report on an analysis of the data from these two user studies. This paper makes three key contributions. The first contribution is a catalog of 44 types of questions programmers ask during software evolution tasks. The second contribution is a description of the observed behavior around answering those questions. The third contribution is a description of how existing deployed and proposed tools do, and do not, support answering programmers' questions.},
  Doi                      = {10.1109/TSE.2008.26}
}

@Article{2009:entcs:dasilva,
  Title                    = {Refactoring of Crosscutting Concerns with Metaphor-Based Heuristics},
  Author                   = {da Silva, Bruno Carreiro and Figueiredo, Eduardo and Garcia, Alessandro and Nunes, Daltro},
  Journal                  = entcs,
  Year                     = {2009},

  Month                    = mar,
  Pages                    = {105--125},
  Volume                   = {233},

  Abstract                 = {It has been advocated that Aspect-Oriented Programming (AOP) is an effective technique to improve software maintainability through explicit support for modularising crosscutting concerns. However, in order to take the advantages of AOP, there is a need for supporting the systematic refactoring of crosscutting concerns to aspects. Existing techniques for aspect-oriented refactoring are too fine-grained and do not take the concern structure into consideration. This paper presents two categories towards a metaphor-based classification of crosscutting concerns driven by their manifested shapes through a system's modular structure. The proposed categories provide an intuitive and fundamental terminology for detecting concern-oriented design flaws and identifying refactorings in terms of recurring crosscutting structures. On top of this classification, we define a suite of metaphor-based refactorings to guide the ``aspectisation'' of each concern category. We evaluate our technique by classifying concerns of 23 design patterns and by proposing refactorings to aspectise them according to observations made in previous empirical studies. Based on our experience, we also determine a catalogue of potential additional categories and heuristics for refactoring of crosscutting concerns. },
  Doi                      = {10.1016/j.entcs.2009.02.064}
}

@Article{2012:csur:silva,
  Title                    = {A vocabulary of program slicing-based techniques},
  Author                   = {Silva, Josep},
  Journal                  = csur,
  Year                     = {2012},

  Month                    = jun,
  Number                   = {3},
  Pages                    = {12:1--12:41},
  Volume                   = {44},

  Doi                      = {10.1145/2187671.2187674}
}

@Article{2012:jss:desilva,
  Title                    = {Controlling software architecture erosion: A survey},
  Author                   = {de Silva, Lakshitha and Balasubramaniam, Dharini},
  Journal                  = jss,
  Year                     = {2012},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {132--151},
  Volume                   = {85},

  Abstract                 = {Software architectures capture the most significant properties and design constraints of software systems. Thus, modifications to a system that violate its architectural principles can degrade system performance and shorten its useful lifetime. As the potential frequency and scale of software adaptations increase to meet rapidly changing requirements and business conditions, controlling such architecture erosion becomes an important concern for software architects and developers. This paper presents a survey of techniques and technologies that have been proposed over the years either to prevent architecture erosion or to detect and restore architectures that have been eroded. These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation. Some of these strategies contain sub-categories under which survey results are presented. We discuss the merits and weaknesses of each strategy and argue that no single strategy can address the problem of erosion. Further, we explore the possibility of combining strategies and present a case for further work in developing a holistic framework for controlling architecture erosion.},
  Doi                      = {10.1016/j.jss.2011.07.036}
}

@InProceedings{2000:wcre:sim,
  Title                    = {A structured demonstration of program comprehension tools},
  Author                   = {Susan Elliott Sim and Margaret-Anne D. Storey},
  Booktitle                = wcre,
  Year                     = {2000},
  Pages                    = {184--194},

  Abstract                 = {This paper describes a structured tool demonstration, a hybrid evaluation technique that combines elements from experiments, case studies and technology demonstrations. Developers of program understanding tools were invited to bring their tools to a common location to participate in a scenario with a common subject system. Working simultaneously the tool teams were given reverse engineering tasks and maintenance tasks to complete on an unfamiliar subject system. Observers were assigned to each team to find out how useful the observed program comprehension tool would be in an industrial setting. The demonstration was followed by a workshop panel where the development teams and the observers presented their results and findings from this experience.}
}

@Article{1974:science:simon,
  Title                    = {How big Is a chunk?: By combining data from several experiments, a basic human memory unit can be identified and measured},
  Author                   = {Herbert A. Simon},
  Journal                  = science,
  Year                     = {1974},

  Month                    = {8 } # feb,
  Number                   = {4124},
  Pages                    = {482--488},
  Volume                   = {183},

  Abstract                 = {I have explored some of the interactions between research on higher mental processes over the past decade or two and laboratory experiments on simpler cognitive processes. I have shown that, by viewing experimentation in a parameter-estimating paradigm instead of a hypothesis-testing paradigm, one can obtain much more information from experiments---information that, combined with contemporary theoretical models of the cognitive processes, has implications for human performance on tasks quite different from those of the original experiments. The work of identifying and measuring the basic parameters of the human information processing system has just begun, but already important information has been gained. The psychological reality of the chunk has been fairly well demonstrated, and the chunk capacity of short-term memory has been shown to be in the range of five to seven. Fixation of information in longterm memory has been shown to take about 5 or 10 seconds per chunk. Some other ``magical numbers" have been estimated---for example, visual scanning speeds and times required for simple grammatical transformations---and no doubt others remain to be discovered. But even the two basic constants discussed in this article---short-term memory capacity and rate of fixation in long-term memory---organize, systematize, and explain a wide range of findings, about both simple tasks and more complex cognitive performances that have been reported in the psychological literature over the past 50 years or more.},
  Doi                      = {10.1126/science.183.4124.482}
}

@Article{1962:paps:simon,
  Title                    = {The architecture of complexity},
  Author                   = {Herbert A. Simon},
  Journal                  = paps,
  Year                     = {1962},

  Month                    = {12 } # dec,
  Number                   = {6},
  Pages                    = {467--482},
  Volume                   = {106},

  Abstract                 = {A number of proposals have been advanced in recent years for the development of ``general systems theory" which, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial.},
  Url                      = {http://www.jstor.org/stable/985254}
}

@Article{1955:qje:simon,
  Title                    = {A Behavioral Model of Rational Choice},
  Author                   = {Herbert A. Simon},
  Journal                  = qje,
  Year                     = {1955},
  Number                   = {1},
  Pages                    = {99--118},
  Volume                   = {69},

  Abstract                 = {Introduction, 99.--I. Some general features of rational choice, 100.--II. The essential simplifications, 103.--III. Existence and uniqueness of solutions, 111.--IV. Further comments on dynamics, 113.--V. Conclusion, 114.--Appendix, 115.},
  Doi                      = {10.2307/1884852}
}

@InProceedings{2006:icse:sindhgatta,
  Title                    = {Using an information retrieval system to retrieve source code samples},
  Author                   = {Sindhgatta, Renuka},
  Booktitle                = icse,
  Year                     = {2006},
  Pages                    = {905--908},

  Abstract                 = {Software developers often face steep learning curves in using a new framework, library, or new versions of frameworks for developing their piece of software. In large organizations, developers learn and explore use of frameworks, rarely realizing, several peers may have already explored the same. A tool that helps locate samples of code, demonstrating use of frameworks or libraries would provide benefits of reuse, improved code quality and faster development. This paper describes an approach for locating common samples of source code from a repository by providing extensions to an information retrieval system. The approach improves the existing approaches in two ways. First, it provides the scalability of an information retrieval system, supporting search over thousands of source code files of an organization. Second, it provides more specific search on source code by preprocessing source code files and understanding elements of the code as opposed to considering code as plain text.},
  Doi                      = {10.1145/1134285.1134448}
}

@InProceedings{1998:icsm:singer,
  Title                    = {Practices of software maintenance},
  Author                   = {Janice Singer},
  Booktitle                = icsm,
  Year                     = {1998},
  Pages                    = {139--145},

  Abstract                 = {This paper describes the results of an interview study conducted at ten industrial sites. The interview focused on the work practices of software engineers engaged in maintaining large scale systems. Five `truths' emerged from this study. First, software maintenance engineers are experts in the systems they are maintaining. Second, source code is the primary source of information about systems. Third, the documentation is used, but not necessarily trusted. Fourth, maintenance control systems are important repositories of information about systems. Finally, reproduction of problems and/or problem scenarios is essential to problem solutions. These truths confirm much of the conventional wisdom in the field. However, in fleshing them out, details were elaborated, and additionally new knowledge was acquired. These results are discussed with respect to tool design.}
}

@InProceedings{1997:cascon:singer,
  Title                    = {An examination of software engineering work practices},
  Author                   = {Janice Singer and Timothy Lethbridge and Norman Vinson and Nicolas Anquetil},
  Booktitle                = cascon,
  Year                     = {1997},
  Pages                    = {21:1--21:15},

  Abstract                 = {This paper presents work practice data of the daily activities of software engineers. Four separate studies are presented; one looking longitudinally at an individual SE; two looking at a software engineering group; and one looking at company-wide tool usage statistics. We also discuss the advantages in considering work practices in designing tools for software engineers, and include some requirements for a tool we have developed as a result of our studies.}
}

@Misc{2013:misc:architexa,
  Title                    = {Architexa},

  Author                   = {Vineet Sinha},
  HowPublished             = {http://www.architexa.com/},
  Year                     = {2013},

  Url                      = {http://www.architexa.com/}
}

@PhdThesis{2008:phd:sinha,
  Title                    = {Using Diagrammatic Explorations to Understand Code},
  Author                   = {Sinha, Vineet},
  School                   = {Massachusetts Institute of Technology},
  Year                     = {2008},

  Address                  = {Cambridge, MA, USA},
  Note                     = {AAI0820150}
}

@InProceedings{2006:vlhcc:sinha,
  Title                    = {Relo: Helping Users Manage Context during Interactive Exploratory Visualization of Large Codebases},
  Author                   = {Sinha, Vineet and Karger, David and Miller, Rob},
  Booktitle                = vlhcc,
  Year                     = {2006},
  Pages                    = {187--194},

  Abstract                 = {As software systems grow in size and use more third-party libraries and frameworks, the need for developers to understand unfamiliar large codebases is rapidly increasing. In this paper, we present a tool, Relo, which supports developers' understanding by allowing interactive exploration of code. As the developer explores relationships found in the code, Relo builds and automatically manages the context in visualization, thereby helping build the developer's mental representation of the code. Developers can group viewed artifacts or use the viewed items to ask Relo for further exploration suggestions, with Relo providing features to limit the growth of the diagram. To ensure developers don't get overwhelmed, Relo has been built with a user-centered approach, and preliminary evaluations with developers exploring new code have shown them to find the tool intuitive and helpful.},
  Doi                      = {10.1109/VLHCC.2006.40}
}

@InProceedings{2005:etx:sinha,
  Title                    = {Relo: Helping users manage context during interactive exploratory visualization of large codebases},
  Author                   = {Sinha, Vineet and Karger, David and Miller, Rob},
  Booktitle                = etx,
  Year                     = {2005},
  Pages                    = {21--25},

  Abstract                 = {As software systems grow in size and use more third-party libraries and frameworks, the need for developers to understand unfamiliar large codebases is rapidly increasing. In this paper, we present a tool, Relo, that supports developers' understanding by allowing interactive exploration of code. As the developer explores relationships found in the code, Relo builds and automatically manages the context in a visualization, thereby helping build the developer's mental representation of the code. Developers can group viewed artifacts or use the viewed items to ask Relo for further exploration suggestions. Relo is built as an Eclipse plug-in integrated into the Java Tooling (JDT), and uses a standard, RDF, based backend allowing for maintaining code relationships and performing inferences about the relationships.},
  Doi                      = {10.1145/1117696.1117701}
}

@InProceedings{2008:oopsla:sinha,
  Title                    = {Understanding code architectures via interactive exploration and layout of layered diagrams},
  Author                   = {Sinha, Vineet and Murnane, Elizabeth L. and Kurth, Scott W. and Liongosari, Edy S. and Miller, Rob and Karger, David},
  Booktitle                = oopslacomp,
  Year                     = {2008},
  Pages                    = {775--776},

  Abstract                 = {Visualization tools that target helping developers understand software have typically had visual scalability limitations, requiring significant input before providing useful results. In contrast, we present Strata, which has been designed to actively help users by providing layered diagrams. The defaults used are based on the package structure, and user interactions can allow for overriding these defaults and focusing on relevant parts of the codebase.},
  Doi                      = {10.1145/1449814.1449856}
}

@InProceedings{2000:icsr:sitaraman,
  Title                    = {Reasoning about Software-Component Behavior},
  Author                   = {Sitaraman, Murali and Atkinson, Steven and Kulczycki, Gregory and Weide, Bruce W. and Long, Timothy J. and Bucci, Paolo and Heym, Wayne and Pike, Scott and Hollingsworth, Joseph E.},
  Booktitle                = icsr,
  Year                     = {2000},
  Pages                    = {266--283},
  Series                   = lncs,
  Volume                   = {1844},

  Abstract                 = {The correctness of a component-based software system depends on the component client's ability to reason about the behavior of the components that comprise the system, both in isolation and as composed. The soundness of such reasoning is dubious given the current state of the practice. Soundness is especially troublesome for component technologies where source code for some components is inherently unavailable to the client. Fortunately, there is a simple, understandable, teachable, practical, and provably sound and relatively complete reasoning system for component-based software systems that addresses the reasoning problem.}
}

@InProceedings{2006:iccbss:sjachyn,
  Title                    = {Semantic component selection: {SemaCS}},
  Author                   = {Sjachyn, Maxym and Beus-Dukic, Ljerka},
  Booktitle                = iccbss,
  Year                     = {2006},
  Pages                    = {83--89},

  Abstract                 = {In component based software development, project success or failure largely depends on correct software component evaluation. All available evaluation methods require time to analyse components. Due to the black box nature of components, preliminary judgments are made based on vendor descriptions. As there is no standard way of describing components, descriptions have to be interpreted using semantics and domain knowledge. This paper presents a semi-automated generic method for component identification and classification based on generic domain taxonomy and user generated semantic input. Every query is semantically tailored to what is being looked for, arriving at better results then it is currently possible using available automated categorisation systems.},
  Doi                      = {10.1109/ICCBSS.2006.25}
}

@InProceedings{2004:icsm:skoglund,
  Title                    = {A case study on regression test suite maintenance in system evolution},
  Author                   = {Skoglund, Mats and Runeson, Per},
  Booktitle                = icsm,
  Year                     = {2004},
  Pages                    = {438--442},

  Doi                      = {10.1109/ICSM.2004.1357831}
}

@InProceedings{2008:apsec:slyngstad,
  Title                    = {Risks and Risk Management in Software Architecture Evolution: An Industrial Survey},
  Author                   = {Slyngstad, Odd Petter N. and Conradi, Reidar and Babar, M. Ali and Clerc, Viktor and van Vliet, Hans},
  Booktitle                = apsec,
  Year                     = {2008},
  Pages                    = {101--108},

  Abstract                 = {The effort that has been made to study risk management in the context of software architecture and its evolution, has so far focused on output from structured evaluations. However, earlier research shows that formal, structured evaluation is not commonly used in industry. We have performed a survey among software architects, in order to capture a more complete picture of the risk and management issues in software architecture evolution. Our survey is specifically about their current knowledge of actual challenges they have anticipated and experienced, as well as strategies they have employed in response. We received completely filled questionnaires from 82 respondents out of a total distribution of 511 architects from the software industry in Norway. While many of the risks we have identified can be aligned with results from earlier studies, we have also identified several risks which appear not to fit these risk categories. Additionally, we found a direct link to business risks, as well as a relatively low level of awareness that lack of software architecture evaluation represents a potential risk.}
}

@InProceedings{2007:ictp:smaragdakis,
  Title                    = {Combining static and dynamic reasoning for bug detection},
  Author                   = {Smaragdakis, Yannis and Csallner, Christoph},
  Booktitle                = ictp,
  Year                     = {2007},
  Pages                    = {1--16},

  Doi                      = {10.1007/978-3-540-73770-4_1}
}

@Article{2000:annsesneed,
  Title                    = {Encapsulation of legacy software: A technique for reusing legacy software components},
  Author                   = {Sneed, Harry M.},
  Journal                  = annse,
  Year                     = {2000},
  Number                   = {1--4},
  Pages                    = {293--313},
  Volume                   = {9},

  Abstract                 = {The following paper reviews the possibilities of encapsulating existing legacy software for reuse in new distributed architectures. It suggests wrapping as an alternative strategy to reengineering and redevelopment. It then defines the levels of granularity at which software can be encapsulated before going on to describe how to construct a wrapper and how to adapt host programs for wrapping. Some wrapping products are discussed and the state of the art summarized. The advantage of wrapping over conventional reengineering is the low cost and even lower risks involved. This is the driving force in the search for improved wrapping technology.},
  Doi                      = {10.1023/A:1018989111417}
}

@Article{1996:tosem:snelting,
  Title                    = {Reengineering of configurations based on mathematical concept analysis},
  Author                   = {Snelting, Gregor},
  Journal                  = tosem,
  Year                     = {1996},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {146--189},
  Volume                   = {5},

  Abstract                 = {We apply mathematical concept analysis to the problem of reengineering configurations. Concept analysis will reconstruct a taxonomy of concepts from a relation between objects and attributes. We use concept analysis to infer configuration structures from existing source code. Our tool NORA/RECS will accept source code, where configuration-specific code pieces are controlled by the preprocessor. The algorithm will compute a so-called concept lattice, which---when visually displayed---offers remarkable insight into the structure and properties of possible configurations. The lattice not only displays tine-grained dependencies between configurations, but also visualizes the overall quality of configuration structures according to software engineering principles. In a second step, interferences between configurations can be analyzed in order to restructure or simplify configurations. Interferences showing up in the lattice indicate high coupling and low cohesion between configuration concepts. Source files can then be simplified according to the lattice structure. Finally, we show how governing expressions can be simplified by utilizing an isomorphism theorem of mathematical concept analysis.},
  Doi                      = {10.1145/227607.227613}
}

@InProceedings{1986:oopsla:snyder,
  Title                    = {Encapsulation and inheritance in object-oriented programming languages},
  Author                   = {Alan Snyder},
  Booktitle                = oopsla,
  Year                     = {1986},
  Pages                    = {38--45},

  Abstract                 = {Object-oriented programming is a practical and useful programming methodology that encourages modular design and software reuse. Most object-oriented programming languages support data abstraction by preventing an object from being manipulated except via its defined external operations. In most languages, however, the introduction of inheritance severely compromises the benefits of this encapsulation. Furthermore, the use of inheritance itself is globally visible in most languages, so that changes to the inheritance hierarchy cannot be made safely. This paper examines the relationship between inheritance and encapsulation and develops requirements for full support of encapsulation with inheritance.},
  Doi                      = {10.1145/960112.28702}
}

@Article{2010:ai:solnon,
  Title                    = {{\emph{AllDifferent}}-based filtering for subgraph isomorphism},
  Author                   = {Solnon, Christine},
  Journal                  = ai,
  Year                     = {2010},

  Month                    = aug,
  Number                   = {12--13},
  Pages                    = {850--864},
  Volume                   = {174},

  Abstract                 = {The subgraph isomorphism problem involves deciding if there exists a copy of a pattern graph in a target graph. This problem may be solved by a complete tree search combined with filtering techniques that aim at pruning branches that do not contain solutions. We introduce a new filtering algorithm based on local all different constraints. We show that this filtering is stronger than other existing filterings---i.e., it prunes more branches---and that it is also more efficient---i.e., it allows one to solve more instances quicker.},
  Doi                      = {10.1016/j.artint.2010.05.002}
}

@InProceedings{2012:icse:song,
  Title                    = {Metadata invariants: Checking and inferring metadata coding conventions},
  Author                   = {Myoungkyu Song and Eli Tilevich},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {694--704},

  Abstract                 = {As the prevailing programming model of enterprise applications is becoming more declarative, programmers are spending an increasing amount of their time and efforts writing and maintaining metadata, such as XML or annotations. Although metadata is a cornerstone of modern software, automatic bug finding tools cannot ensure that metadata maintains its correctness during refactoring and enhancement. To address this shortcoming, this paper presents metadata invariants, a new abstraction that codifies various naming and typing relationships between metadata and the main source code of a program. We reify this abstraction as a domain-specific language. We also introduce algorithms to infer likely metadata invariants and to apply them to check metadata correctness in the presence of program evolution. We demonstrate how metadata invariant checking can help ensure that metadata remains consistent and correct during program evolution; it finds metadata-related inconsistencies and recommends how they should be corrected. Similar to static bug finding tools, a metadata invariant checker identifies metadata-related bugs as a program is being refactored and enhanced. Because metadata is omnipresent in modern software applications, our approach can help ensure the overall consistency and correctness of software as it evolves.},
  Doi                      = {10.1109/ICSE.2012.6227148}
}

@Article{1992:tnn:sontag,
  Title                    = {Feedback stabilization using two-hidden-layer nets},
  Author                   = {Sontag, E. D.},
  Journal                  = tnn,
  Year                     = {1992},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {981--990},
  Volume                   = {3},

  Doi                      = {10.1109/72.165599}
}

@InProceedings{2005:gbrpr:sorlin,
  Title                    = {Reactive tabu search for measuring graph similarity},
  Author                   = {Sorlin, S{\'e}bastien and Solnon, Christine},
  Booktitle                = gbrpr,
  Year                     = {2005},
  Pages                    = {172--182},

  Abstract                 = {Graph matching is often used for image recognition. Different kinds of graph matchings have been proposed such as (sub)graph isomorphism or error-tolerant graph matching, giving rise to different graph similarity measures. A first goal of this paper is to show that these different measures can be viewed as special cases of a generic similarity measure introduced in [8]. This generic similarity measure is based on a non-bijective graph matching (like [4] and [2]) so that it is well suited to image recognition. In particular, over/under-segmentation problems can be handled by linking one vertex to a set of vertices. In a second part, we address the problem of computing this measure and we describe two algorithms: a greedy algorithm, that quickly computes sub-optimal solutions, and a reactive Tabu search algorithm, that may improve these solutions. Some experimental results are given.},
  Doi                      = {10.1007/978-3-540-31988-7_16}
}

@InProceedings{2004:cpaior:sorlin,
  Title                    = {A Global Constraint for Graph Isomorphism Problems},
  Author                   = {S{\'e}bastien Sorlin and Christine Solnon},
  Booktitle                = cpaior,
  Year                     = {2004},
  Pages                    = {287--302},
  Series                   = lncs,
  Volume                   = {3011},

  Abstract                 = {The graph isomorphism problem consists in deciding if two given graphs have an identical structure. This problem can be modeled as a constraint satisfaction problem in a very straightforward way, so that one can use constraint programming to solve it. However, constraint programming is a generic tool that may be less efficient than dedicated algorithms which can take advantage of the global semantic of the original problem. Hence, we introduce in this paper a new global constraint dedicated to graph isomorphism problems, and we define an associated filtering algorithm that exploits all edges of the graphs in a global way to narrow variable domains. We then show how this global constraint can be decomposed into a set of ``distance'' constraints which propagate more domain reductions than ``edge'' constraints that are usually generated for this problem.},
  Doi                      = {10.1007/978-3-540-24664-0_20}
}

@InCollection{2007:book:kandel:sorlin,
  Title                    = {A Generic Graph Distance Measure Based on Multivalent Matchings},
  Author                   = {S{\'e}bastien Sorlin and Christine Solnon and Jean-Michel Jolion},
  Booktitle                = {Applied Graph Theory in Computer Vision and Pattern Recognition},
  Publisher                = {Springer},
  Year                     = {2007},
  Editor                   = {Abraham Kandel and Horst Bunke and Mark Last},
  Pages                    = {151--181},
  Series                   = sci,
  Volume                   = {52},

  Abstract                 = {Many applications such as information retrieval and classification, involve measuring graph distance or similarity, i.e., matching graphs to identify and quantify their common features. Different kinds of graph matchings have been proposed, giving rise to different graph similarity or distance measures. Graph matchings may be univalent---when each vertex is associated with at most one vertex of the other graph---or multivalent---when each vertex is associated with a set of vertices of the other graph. Also, graph matchings may be exact---when all vertex and edge features must be preserved by the matching---or error-tolerant---when some vertex and edge features may not be preserved by the matching. The first goal of this chapter is to propose a new graph distance measure based on the search of a best matching between the vertices of two graphs, i.e., a matching minimizing vertex and edge distance functions. This distance measure is generic in the sense that it allows both univalent and multivalent matchings and it is parameterized by vertex and edge distance functions defined by the user depending on the considered application. The second goal of this chapter is to show how to use this generic measure to model and to solve classical graph matching problems such as (sub-)graph isomorphism problem, error-tolerant graph matching, and nonbijective graph matching.},
  Doi                      = {10.1007/978-3-540-68020-8_6}
}

@InProceedings{2012:raise:de_souza_alcantara,
  Title                    = {Learning Gestures for Interacting with Low-Fidelity Prototypes},
  Author                   = {de Souza Alcantara, Tulio and Denzinger, J{\"o}rg and Ferreia, Jennifer and Maurer, Frank},
  Booktitle                = raise,
  Year                     = {2012},
  Pages                    = {32--36},

  Abstract                 = {This paper presents an approach to help designers create their own application-specific gestures and evaluate them in user-studies based on low fidelity prototypes of the application they are designing. In order to learn custom gestures, we developed a machine learning tool that uses an anti-unification algorithm to learn based on samples of the gesture provided by the designer.},
  Doi                      = {10.1109/RAISE.2012.6227967}
}

@Article{1904:ajp:spearman,
  Title                    = {The proof and measurement of association between two things},
  Author                   = {C. Spearman},
  Journal                  = ajp,
  Year                     = {1904},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {72--101},
  Volume                   = {15},

  Abstract                 = {All knowledge---beyond that of bare isolated occurrence---deals with uniformities. Of the latter, some few have a claim to be considered absolute, such as mathematical implications and mechanical laws. But the vast majority are only partial; medicine does not teach that smallpox is inevitably escaped by vaccination, but that it is so generally; biology has not shown that all animals require organic food, but that nearly all do so; in daily life, a dark sky is no proof that it will rain, but merely a warning; even in morality, the sole categorical imperative alleged by Kant was the sinfulness of telling a lie, and few thinkers since have admitted so much as this to be valid universally. In psychology, more perhaps than in any other science, it is hard to find absolutely inflexible coincidences; occasionally, indeed, there appear uniformities sufficiently regular to be practically treated as laws, but infinitely the greater part of the observations hitherto recorded concern only more or less pronounced tendencies of one event or aftribute to accompany another. Under these circumstances, one might well have expected that the evidential evaluation and precise mensuration of tendencies had long been the subject of exhaustive investigation and now formed one of the earliest sections in a beginner's psychological course. Instead, we find only a general naive ignorance that there is anything about it requiring to be learnt. One after another, laborious series of experiments are executed and published with the purpose of demonstrating some connection between two events, wherein the otherwise learned psychologist reveals that his art of proving and measuring correspondence has not advanced beyond that of lay persons. The consequence has been that the significance of the experiments is not at all rightly understood, nor have any definite facts been elicited that may be either confirmed or refuted. The present article is a commencement at attempting to remedy this deficiency of scientific correlation. With this view, it will be strictly confined to the needs of practical workers, and all theoretical mathematical demonstrations will be omitted; it may, however, be said that the relations stated have already received a large amount of empirical verification. Great thanks are due from me to Professor Haussdorff and to Dr. G. Lipps, each of whom have supplied a useful theorem in polynomial probability; the former has also very kindly given valuable advice concerning the proof of the important formulae for elimination of ``systematic deviations.'' At the same time, and for the same reason, the meaning and working of the various formulae have been explained sufficiently, it is hoped, to render them readily usable even by those whose knowledge of mathematics is elementary. The fundamental procedure is accompanied by simple imaginary examples, while the more advanced parts are illustrated by cases that have actually occurred in my personal experience. For more abundant and positive exemplification, the reader is requested to refer to the under cited research, ['General Intelligence,' determined and measured, to appear in a subsequent number of this Journal.] which is entirely built upon the principles and mathematical relations here laid down. In conclusion, the general value of the methodics recommended is emphasized by a brief criticism of the best correlational work hitherto made public, and also the important question is discussed as to the number of ``cases'' required for an experimental series.},
  Doi                      = {10.2307/1412159}
}

@InProceedings{2008:icpc:sridhara,
  Title                    = {Identifying Word Relations in Software: A Comparative Study of Semantic Similarity Tools},
  Author                   = {Sridhara, Giriprasad and Hill, Emily and Pollock, Lori and Vijay-Shanker, K.},
  Booktitle                = icpc,
  Year                     = {2008},
  Pages                    = {123--132},

  Abstract                 = {Modern software systems are typically large and complex, making comprehension of these systems extremely difficult. Experienced programmers comprehend code by seamlessly processing synonyms and other word relations. Thus, we believe that automated comprehension and software tools can be significantly improved by leveraging word relations in software. In this paper, we perform a comparative study of six state of the art, English-based semantic similarity techniques and evaluate their effectiveness on words from the comments and identifiers in software. Our results suggest that applying English-based semantic similarity techniques to software without any customization could be detrimental to the performance of the client software tools. We propose strategies to customize the existing semantic similarity techniques to software, and describe how various program comprehension tools can benefit from word relation information.},
  Doi                      = {10.1007/978-3-540-31988-7\_16}
}

@Article{1996:tkde:srinivas,
  Title                    = {Genetic search: Analysis using fitness moments},
  Author                   = {Srinivas, M. and Patnaik, L. M.},
  Journal                  = tkde,
  Year                     = {1996},

  Month                    = feb,
  Number                   = {1},
  Pages                    = {120--133},
  Volume                   = {8},

  Abstract                 = {Genetic Algorithms are efficient and robust search methods that are being employed in a plethora of applications with extremely large search spaces. The directed search mechanism employed in Genetic Algorithms performs a simultaneous and balanced, exploration of new regions in the search space and exploitation of already discovered regions. This paper introduces the notion of fitness moments for analyzing the working of Genetic Algorithms (GAs). We show that the fitness moments in any generation may be predicted from those of the initial population. Since a knowledge of the fitness moments allows us to estimate the fitness distribution of strings, this approach provides for a method of characterizing the dynamics of GAs. In particular the average fitness and fitness variance of the population in any generation may be predicted. We introduce the technique of fitness-based disruption of solutions for improving the performance of GAs. Using fitness moments, we demonstrate the advantages of using fitness-based disruption. We also present experimental results comparing the performance of a standard GA and GAs (CDGA and AGA) that incorporate the principle of fitness-based disruption. The experimental evidence clearly demonstrates the power of fitness based disruption.},
  Doi                      = {10.1109/69.485641}
}

@InProceedings{2011:qosa_isarcs:stal,
  Title                    = {Good is not good enough: Evaluating and improving software architecture},
  Author                   = {Stal, Michael},
  Booktitle                = qosa_isarcs,
  Year                     = {2011},
  Pages                    = {73--74},

  Abstract                 = {Software engineering has an increasing impact on the diverse products, services, and solutions offered by technology-oriented industries. For example, within SIEMENS AG over 60\% of revenues depend on software. Consequently, failures in software development projects lead to significant costs. If software is so important, we need sustainable software architectures designed by well-educated and skilled software architects as backbone of high quality software systems. However, due to the complexity of problem and solution domains and the increased desiderata of customers, software systems cannot be created in a big bang approach. Instead, software architectures must be systematically designed and then implemented using piecemeal growth which implies that software architects need to incrementally and iteratively refine, assess, and improve a software system. Otherwise, wrong or inefficient design decisions will be hard and costly to detect and eliminate inevitably causing design erosion to creep in. Thus, Software Architecture Evaluation and Improvement play an important role in the development of sustainable software systems. But how can we seamlessly integrate Software Architecture and Improvement into the architecture design process? A systematic process for creating software architecture comprises among many other ingredients \begin{itemize} \item the (stepwise) clarification and prioritization of requirements, \item the incremental and iterative evolution of the software architecture driven by requirements and risks, \item Continuous quality assessment for assuring that the software system meets its internal and external qualities as well as its functional expectations. \end{itemize} For continuous quality assessment Test-Driven Development and Design for Testability denote approaches that are applicable when the implementation is available. In order to assess the architecture itself, even in absence of a (full) implementation, qualitative and quantitative architecture assessment methods are available. For example, the experience-based industry review method applies a SWOT analysis to identify potential threats and weaknesses as well as potential means to resolve these issues. Architecture Analysis tools support software architects in this endeavor. But how can software architects improve the internal or external quality of the software system after they detected some architecture smells? It is essential to get rid of these issues by continuous software architecture improvement which exactly is the place where architecture refactoring, reengineering, and rewriting come in. In the keynote, I will illustrate a systematic design process for iterative and incremental software architecture creation. This design process includes refinement activities, but also evaluation and improvement activities. In particular, I will address activities such as architecture review, testing, and architecture refactoring to improve internal and external quality.},
  Doi                      = {10.1145/2000259.2000272}
}

@Article{1984:tse:standish,
  Title                    = {An essay on software reuse},
  Author                   = {Thomas A. Standish},
  Journal                  = tse,
  Year                     = {1984},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {494--497},
  Volume                   = {10},

  Abstract                 = {This paper explores software reuse. It discusses briefly some economic incentives for developing effective software reuse technology and notes that different kinds of software reuse, such as direct use without modification and reuse of abstract software modules after refinement, have different technological implications. It sketches some problem areas to be addressed if we are to achieve the goal of devising practical software reuse systems. These include information retrieval problems and finding effective methods to aid us in understanding how programs work. There is a philosophical epilogue which stresses the importance of having realistic expectations about the benefits of software reuse.},
  Doi                      = {10.1109/TSE.1984.5010272}
}

@InProceedings{2008:eit:stanek,
  Title                    = {Method of Comparing Graph Differencing Algorithms for Software Differencing},
  Author                   = {Jason Stanek and Suraj Kothari and Kang Gui},
  Booktitle                = eit,
  Year                     = {2008},
  Pages                    = {482--487},

  Abstract                 = {Software differencing is the process of identifying differences between two versions of software. Finding the differences is important for applications such as efficient testing, merging, and auditing. Software differencing reduces to the problem of graph differencing for which the most general case is intractable. Graph differencing is important in several fields and several notions of graph differences exist. In this paper we describe an experimental method for comparing graph differencing algorithms for software differencing, develop a new definition of graph difference for identifying the semantic differences in software and conjecture about the definitionpsilas relation to another mathematical definition, and finally use these in a comparative study of graph differencing algorithms.},
  Doi                      = {10.1109/EIT.2008.4554351}
}

@Book{2003:book:stapleton,
  Title                    = {{DSDM}: A Framework for Business-Centered Development},
  Author                   = {Stapleton, Jennifer},
  Publisher                = {Addison-Wesley Professional},
  Year                     = {2003},
  Edition                  = {2nd}
}

@InProceedings{2009:icsm:starke,
  Title                    = {Searching and skimming: An exploratory study},
  Author                   = {Starke, Jamie and Luce, Chris and Sillito, Jonathan},
  Booktitle                = icsm,
  Year                     = {2009},
  Pages                    = {157--166},

  Doi                      = {10.1109/ICSM.2009.5306335}
}

@InProceedings{2009:suite:starke,
  Title                    = {Working with Search Results},
  Author                   = {Starke, Jamie and Luce, Chris and Sillito, Jonathan},
  Booktitle                = suite,
  Year                     = {2009},
  Pages                    = {53--56},

  Abstract                 = {Source code search is an important activity for programmers working on a change task to a software system. We are at the early stages of a research program that is aiming to answer three research questions: (1) How effectively can programmers express (using today's tools) the information they are seeking? (2) How effectively can programmers determine which of the matches returned from their searches are relevant to their task? and (3) In what ways can tools be improved to support programmers in more effectively expressing their information needs and exploring the results of searches? To begin answering these questions we have conducted a study in which we gathered both qualitative and quantitative data about programmers' search activities. Our analysis of this data is still incomplete, however this paper presents several of our initial observations about how programmers interact with the results from their searches.}
}

@InProceedings{2011:codes+isss:stattelmann,
  Title                    = {Dominator homomorphism based code matching for source-level simulation of embedded software},
  Author                   = {Stattelmann, Stefan and Bringmann, Oliver and Rosenstiel, Wolfgang},
  Booktitle                = codes+isss,
  Year                     = {2011},
  Pages                    = {305--314},

  Abstract                 = {Relating optimized binary code and the source-level statements from which it was created can be challenging if an optimizing compiler was used to create the machine code. Moreover, this relation is crucial if a compiler-optimized program must be debugged or results from a low-level analysis need to be mapped to the source code to perform manual optimizations. Existing approaches for the debugging of optimized code usually require pervasive changes in the compiler and hence are not available for all architectures. Methods for analyzing non-functional properties of software components in complex systems (i.e. execution time and power consumption) often have similar constraints, if compiler optimizations are supported at all. This paper proposes two novel concepts to overcome these issues. To precisely relate source-level statements with the respective compiler-generated machine code, a method to reconstruct and disambiguate debug information is presented. Based on this information, an instrumentation technique is introduced which allows accurately simulating the execution of optimized binary code at the source code level. Experimental results show that by using this technique, arbitrary low-level properties of software components can be evaluated in a fast and accurate manner without running the software on the actual target hardware.},
  Doi                      = {10.1145/2039370.2039417}
}

@InProceedings{1985:usenixwtc:steffen,
  Title                    = {Interactive examination of a {C} program with {Cscope}},
  Author                   = {Steffen, Joseph L},
  Booktitle                = usenixwtc,
  Year                     = {1985},
  Pages                    = {170--175}
}

@InProceedings{2000:issta:steven,
  Title                    = {{jRapture}: A capture/replay tool for observation-based testing},
  Author                   = {Steven, John and Chandra, Pravir and Fleck, Bob and Podgurski, Andy},
  Booktitle                = issta,
  Year                     = {2000},
  Pages                    = {158--167},

  Doi                      = {10.1145/347324.348993}
}

@Article{1946:science:stevens,
  Title                    = {On the theory of scales of measurement},
  Author                   = {Stevens, S. S.},
  Journal                  = science,
  Year                     = {1946},

  Month                    = {7 } # jun,
  Number                   = {2684},
  Pages                    = {677--680},
  Volume                   = {103},

  Abstract                 = {For seven years a committee of the British Association for the Advancement of Science debated the problem of measurement. Appointed in 1932 to represent Section A (Mathematical and Physical Sciences) and Section J (Psychology), the committee was instructed to consider and report upon the possibility of ``quantitative estimates of sensory events''---meaning simply: Is it possible to measure human sensation? Deliberation led only to disagreement, mainly about what is meant by the term measurement. An interim report in 1938 found one member complaining that his colleagues ``came out by that same door as they went in," and in order to have another try at agreement, the committee begged to be continued---for another year. For its final report (1940) the committee chose a common bone for its contentions, directing its arguments at a concrete example of a sensory scale. This was the Sone scale of loudness (S. S. Stevens and H. Davis. Hearing. New York: Wiley, 1938), which purports to measure the subjective magnitude of an auditory sensation against a scale having the formal properties of other basic scales, such as those used to measure length and weight. Again the 19 members of the committee came out by the routes they entered, and their views ranged widely between two extremes. One member submitted ``that any law purporting to express a quantitative relation between sensation intensity and stimulus intensity is not merely false but is in fact meaningless unless and until a meaning can be given to the concept of addition as applied to sensation" (Final Report, p. 245). It is plain from this and from other statements by the committee that the real issue is the meaning of measurement. This, to be sure, is a semantic issue, but one susceptible of orderly discussion. Perhaps agreement can better be achieved if we recognize that measurement exists in a variety of forms and that scales of measurement fall into certain definite classes. These classes are determined both by the empirical operations invoked in the process of ``measuring" and by the formal (mathematical) properties of the scales. Furthermore-and this is of great concern to several of the sciences-the statistical manipulations that can legitimately be applied to empirical data depend upon the type of scale against which the data are ordered.},
  Doi                      = {10.1126/science.103.2684.677}
}

@Article{1974:ibmsj:stevens,
  Title                    = {Structured design},
  Author                   = {W. P. Stevens and G. J. Myers and L. L. Constantine},
  Journal                  = ibmsj,
  Year                     = {1974},
  Number                   = {2},
  Pages                    = {115--139},
  Volume                   = {13},

  Abstract                 = {The HIPO Hierarchy chart is being used as an aid during general systems design. The considerations and techniques presented here are useful for evaluating alternatives for thos portions of the system that will be programmed on a computer. The charting technique used here depicts more details about the interfaces than the HIPO Hierarchy chart. This facilitates consideration during general program design of each individual connection and its associated passed parameters. The resulting design can be documented with the HIPO charts. (if the designer decides to have more than one function in any module, the structure chart should show them in the same block. However, the HIPO Hierarchy chart would still show all the functions in separate blocks.) The output of the general program design is the input for the detailed module design. The HIPO input-process-output chart is useful for describing and designing each module.},
  Doi                      = {10.1147/sj.132.0115}
}

@InProceedings{1996:oopsla:steyaert,
  Title                    = {Reuse contracts: Managing the evolution of reusable assets},
  Author                   = {Steyaert, Patrick and Lucas, Carine and Mens, Kim and D'Hondt, Theo},
  Booktitle                = oopsla,
  Year                     = {1996},
  Pages                    = {268--285},

  Abstract                 = {A critical concern in the reuse of software is the propagation of changes made to reusable artifacts. Without techniques to manage these changes, multiple versions of these artifacts will propagate through different systems and reusers will not be able to benefit from improvements to the original artifact. We propose to codify the management of change in a software system by means of reuse contracts that record the protocol between managers and users of a reusable asset. Just as real world contracts can be extended, amended and customised, reuse contracts are subject to parallel changes encoded by formal reuse operators: extension, refinement and concretisation. Reuse contracts and their operators serve as structured documentation and facilitate the propagation of changes to reusable assets by indicating how much work is needed to update previously built applications, where and how to test and how to adjust these applications.},
  Doi                      = {10.1145/236337.236363}
}

@Article{1974:jrssb:stone,
  Title                    = {Cross-validatory Choice and Assessment of Statistical Predictions},
  Author                   = {M. Stone},
  Journal                  = jrssb,
  Year                     = {1974},
  Number                   = {2},
  Pages                    = {111--147},
  Volume                   = {36},

  Abstract                 = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
  Url                      = {http://www.jstor.org/stable/2984809}
}

@InProceedings{2005:iwpc:storey,
  Title                    = {Theories, methods and tools in program comprehension: Past, present and future},
  Author                   = {Storey, Margaret-Anne},
  Booktitle                = iwpc,
  Year                     = {2005},
  Pages                    = {181--191},

  Abstract                 = {Program comprehension research can be characterized by both the theories that provide rich explanations about how programmers comprehend software, as well as the tools that are used to assist in comprehension tasks. During this talk the author review some of the key cognitive theories of program comprehension that have emerged. Using these theories as a canvas, the author then explores how tools that are popular today have evolved to support program comprehension. Specifically, the author discusses how the theories and tools are related and reflect on the research methods that were used to construct the theories and evaluate the tools. The reviewed theories and tools will be further differentiated according to human characteristics, program characteristics, and the context for the various comprehension tasks. Finally, the author predicts how these characteristics will change in the future and speculate on how a number of important research directions could lead to improvements in program comprehension tools and methods.}
}

@InProceedings{1997:icse:storey,
  Title                    = {Rigi: A Visualization Environment for Reverse Engineering},
  Author                   = {Storey, Margaret-Anne D. and Wong, Kenny and M{\"u}ller, Hausi A.},
  Booktitle                = icse,
  Year                     = {1997},
  Pages                    = {606--607},

  Abstract                 = {The Rigi reverse engineering system provides two contrasting approaches for presenting software structures in its graph editor. The first displays the structures through multiple, individual NindoNs. The second (neNer) approach, Simple Hierarchical Multi-Perspective (SHriMP) views, employs fisheye views of nested graphs. We compare and contrast these two interfaces for visualizing software graphs, and provide results from user experiments.},
  Doi                      = {10.1109/ICSE.1997.610428}
}

@Article{2000:scp:storey,
  Title                    = {How do program understanding tools affect how programmers understand programs?},
  Author                   = {M.-A. D. Storey and K. Wong and H. A. M\"{u}ller},
  Journal                  = scp,
  Year                     = {2000},

  Month                    = mar,
  Number                   = {2--3},
  Pages                    = {183--207},
  Volume                   = {36},

  Abstract                 = {In this paper, we explore the question of whether program understanding tools enhance or change the way that programmers understand programs. The strategies that programmers use to comprehend programs vary widely. Program understanding tools should enhance or ease the programmer's preferred strategies, rather than impose a fixed strategy that may not always be suitable. We present observations from a user study that compares three tools for browsing program source code and exploring software structures. In this study, 30 participants used these tools to solve several high-level program understanding tasks. These tasks required a broad range of comprehension strategies. We describe how these tools supported or hindered the diverse comprehension strategies used.},
  Doi                      = {10.1016/S0167-6423(99)00036-2}
}

@Book{1993:book:strauss,
  Title                    = {Continual Permutations of Action},
  Author                   = {Strauss, Anselm L.},
  Publisher                = {Aldine de Gruyter},
  Year                     = {1993},

  Address                  = {New York},

  ISBN                     = {0202304728}
}

@Article{2001:tse:strike,
  Title                    = {Software Cost Estimation with Incomplete Data},
  Author                   = {Strike, Kevin and El Emam, Khaled and Madhavji, Nazim},
  Journal                  = tse,
  Year                     = {2001},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {890--908},
  Volume                   = {27},

  Abstract                 = {The construction of software cost estimation models remains an active topic of research. The basic premise of cost modeling is that a historical database of software project cost data can be used to develop a quantitative model to predict the cost of future projects. One of the difficulties faced by workers in this area is that many of these historical databases contain substantial amounts of missing data. Thus far, the common practice has been to ignore observations with missing data. In principle, such a practice can lead to gross biases and may be detrimental to the accuracy of cost estimation models. In this paper, we describe an extensive simulation where we evaluate different techniques for dealing with missing data in the context of software cost modeling. Three techniques are evaluated: listwise deletion, mean imputation, and eight different types of hot-deck imputation. Our results indicate that all the missing data techniques perform well with small biases and high precision. This suggests that the simplest technique, listwise deletion, is a reasonable choice. However, this will not necessarily provide the best performance. Consistent best performance (minimal bias and highest precision) can be obtained by using hot-deck imputation with Euclidean distance and a z-score standardization.},
  Doi                      = {10.1109/32.962560}
}

@InProceedings{2007:hopl:stroustrup,
  Title                    = {Evolving a language in and for the real world: {C++} 1991--2006},
  Author                   = {Bjarne Stroustrup},
  Booktitle                = hopl,
  Year                     = {2007},
  Pages                    = {1--59},

  Doi                      = {10.1145/1238844.1238848}
}

@InProceedings{2009:fqas:stuckenschmidt,
  Title                    = {A Semantic Similarity Measure for Ontology-Based Information},
  Author                   = {Heiner Stuckenschmidt},
  Booktitle                = fqas,
  Year                     = {2009},
  Pages                    = {406--417},
  Series                   = lncs,
  Volume                   = {5822},

  Abstract                 = {Computing the similarity between data elements is a basic functionality in flexible query answering systems. In the case of complex data definitions, for instance in terms of an ontology, computing the similarity between data elements becomes a non-trivial problem. In this paper, we propose a similarity measure for data described in terms of the DL-lite ontology language. In this measure, we take implicit information contained in the definition of classes and relations into account. In contrast to many other proposals for similarity measures, our proposal does not rely on structural criteria of the definitions involved but is solely based on the logical consequences that can be drawn.},
  Doi                      = {10.1007/978-3-642-04957-6_35}
}

@Article{2005:pnas:stumpf,
  Title                    = {Subnets of scale-free networks are not scale-free: Sampling properties of networks},
  Author                   = {Michael P. H. Stumpf and Carsten Wiuf and Robert M. May},
  Journal                  = pnas,
  Year                     = {2005},

  Month                    = {22 } # mar,
  Number                   = {12},
  Pages                    = {4221--4224},
  Volume                   = {102},

  Abstract                 = {Most studies of networks have only looked at small subsets of the true network. Here, we discuss the sampling properties of a network's degree distribution under the most parsimonious sampling scheme. Only if the degree distributions of the network and randomly sampled subnets belong to the same family of probability distributions is it possible to extrapolate from subnet data to properties of the global network. We show that this condition is indeed satisfied for some important classes of networks, notably classical random graphs and exponential random graphs. For scale-free degree distributions, however, this is not the case. Thus, inferences about the scale-free nature of a network may have to be treated with some caution. The work presented here has important implications for the analysis of molecular networks as well as for graph theory and the theory of networks in general.},
  Doi                      = {10.1073/pnas.0501179102}
}

@Article{2003:tse:subramanyam,
  Title                    = {Empirical analysis of {CK} metrics for object-oriented design complexity: Implications for software defects},
  Author                   = {Ramanath Subramanyam and M. S. Krishnan},
  Journal                  = tse,
  Year                     = {2003},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {297--310},
  Volume                   = {29},

  Abstract                 = {To produce high quality object-oriented (OO) applications, a strong emphasis on design aspects, especially during the early phases of software development, is necessary. Design metrics play an important role in helping developers understand design aspects of software and, hence, improve software quality and developer productivity. In this paper, we provide empirical evidence supporting the role of OO design complexity metrics, specifically a subset of the Chidamber and Kemerer (1991, 1994) suite (CK metrics), in determining software defects. Our results, based on industry data from software developed in two popular programming languages used in OO development, indicate that, even after controlling for the size of the software, these metrics are significantly associated with defects. In addition, we find that the effects of these metrics on defects vary across the samples from two programming languages---C++ and Java. We believe that these results have significant implications for designing high-quality software products using the OO approach.},
  Doi                      = {10.1109/TSE.2003.1191795}
}

@Article{2001:tse:succi,
  Title                    = {Analysis of the effects of software reuse on customer satisfaction in an {RPG} environment},
  Author                   = {Giancarlo Succi and Luigi Benedicenti and Tullio Vernazza},
  Journal                  = tse,
  Year                     = {2001},

  Month                    = may,
  Number                   = {5},
  Pages                    = {473--479},
  Volume                   = {27},

  Abstract                 = {This paper reports on an empirical research based on two software products. The research goal is to ascertain the impact of the adoption of a reuse policy on customer satisfaction. The results show that when a systematic reuse policy is implemented, such as the adoption of a domain specific library, 1) reuse is significantly positively correlated with customer satisfaction and 2) there is a significant increase in customer satisfaction. The results have been extended to the underlying populations, supposed normal.},
  Doi                      = {10.1109/32.922717}
}

@InProceedings{2001:esec_fse:sullivan,
  Title                    = {The structure and value of modularity in software design},
  Author                   = {Sullivan, Kevin J. and Griswold, William G. and Cai, Yuanfang and Hallen, Ben},
  Booktitle                = esec_fse,
  Year                     = {2001},
  Pages                    = {99--108},

  Abstract                 = {The concept of information hiding modularity is a cornerstone of modern software design thought, but its formulation remains casual and its emphasis on changeability is imperfectly related to the goal of creating added value in a given context. We need better explanatory and prescriptive models of the nature and value of information hiding. We evaluate the potential of a new theory---developed to account for the influence of modularity on the evolution of the computer industry---to inform software design. The theory uses design structure matrices to model designs and real options techniques to value them. To test the potential utility of the theory for software we apply it to Parnas's KWIC designs. We contribute an extension to design structure matrices, and we show that the options results are consistent with Parnas's conclusions. Our results suggest that such a theory does have potential to help inform software design.}
}

@InProceedings{2011:ase:sun,
  Title                    = {Towards more accurate retrieval of duplicate bug reports},
  Author                   = {Sun, Chengnian and Lo, David and Khoo, Siau-Cheng and Jiang, Jing},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {253--262},

  Abstract                 = {In a bug tracking system, different testers or users may submit multiple reports on the same bugs, referred to as duplicates, which may cost extra maintenance efforts in triaging and fixing bugs. In order to identify such duplicates accurately, in this paper we propose a retrieval function (REP) to measure the similarity between two bug reports. It fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields, but also similarity of non-textual fields such as product, component, version, etc. For more accurate measurement of textual similarity, we extend BM25F---an effective similarity formula in information retrieval community, specially for duplicate report retrieval. Lastly we use a two-round stochastic gradient descent to automatically optimize REP for specific bug repositories in a supervised learning manner. We have validated our technique on three large software bug repositories from Mozilla, Eclipse and OpenOffice. The experiments show 10--27\% relative improvement in recall rate@k and 17--23\% relative improvement in mean average precision over our previous model. We also applied our technique to a very large dataset consisting of 209,058 reports from Eclipse, resulting in a recall rate@k of 37--71\% and mean average precision of 47\%.},
  Doi                      = {10.1109/ASE.2011.6100061}
}

@InProceedings{2010:compsac:sun,
  Title                    = {Change impact analysis based on a taxonomy of change types},
  Author                   = {Sun, Xiaobing and Li, Bixin and Tao, Chuanqi and Wen, Wanzhi and Zhang, Sai},
  Booktitle                = compsac,
  Year                     = {2010},
  Pages                    = {373--382},

  Doi                      = {10.1109/COMPSAC.2010.45}
}

@Article{2008:ijase:sutcliffe,
  Title                    = {The socio-economics of software architecture},
  Author                   = {Alistair Sutcliffe},
  Journal                  = ijase,
  Year                     = {2008},

  Month                    = dec,
  Number                   = {3--4},
  Pages                    = {343--363},
  Volume                   = {15},

  Abstract                 = {The paper argues that strategic decisions about software architectures need to be based on a social and economic analysis of which designs are likely to succeed and become accepted by users. Software architecture is increasingly having to take account of customisation, reuse, end-user development and system configuration. The relationship between architecture and end users' requirements is investigated, to propose a cost-benefit framework to support reasoning about architectural choices from the perspective of end users. The relationships between architectural decisions and non-functional requirements is reviewed, and the impact on architecture is assessed using a case study of developing configurable, semi-intelligent software to support medical researchers in e-science domains.},
  Doi                      = {10.1007/s10515-008-0029-5}
}

@PhdThesis{2003:phd:svahnberg,
  Title                    = {Supporting Software Architecture Evolution},
  Author                   = {Mikael Svahnberg},
  School                   = {Blekinge Institute of Technology},
  Year                     = {2003},

  Address                  = {Blekinge, Sweden},
  Month                    = oct,

  Abstract                 = {Today it is more a rule than an exception that software systems have a lifecycle of more than several years. Hence, software evolution is inevitable. During the life span of a software system the domain in which the system is working evolves and changes. This causes changes to the software system, and the software system may also be evolved to satisfy new markets. The ability to evolve gracefully, and thus the long-term success of a software system, is to a large extent governed by its software architecture and the ability of the software architecture to fulfil requirements on quality attributes and to adapt to evolving requirements. In this thesis we study evolution of software architectures and what can be done to support this evolution. We focus on three particular aspects of evolution support: how to ensure that the correct blend of quality attributes is met (architecture selection), the technical means available for supporting changes in the software system (variability), and what types of changes that are likely to occur during evolution (categories of evolution). We introduce a method for architecture evaluation and selection that focus on ensuring that the selected software architecture is the architecture candidate with the most potential for fulfilling a particular blend of quality attributes. The method is based on quantification of expert opinions and focused discussions where these expert opinions differ. The architecture evaluation and selection method is studied in both an academic and in an industry setting. We also introduce a taxonomy of techniques for realising variability in a software system and study how the techniques in this taxonomy are applied in different evolution situations. The taxonomy is based on several industry case studies. Two industry cases are studied in further detail and the evolution of these systems are followed over a number of releases and generations. During this evolution it is shown how variability mechanisms are used to also support evolution, and that there are typical cases of evolution that a software system can be prepared to cope with. The contribution of this thesis is that it increases the understanding of how evolution occurs in a software system, how to create software that is flexible enough to support evolution and how to evaluate and select a software architecture that meets a particular blend of quality attributes. Together this ensures that a software system is based on a software architecture that fits the current quality requirements and that is flexible in the right places so that it is able to evolve gracefully.},
  Institution              = {Blekinge Institute of Technology},
  Url                      = {http://www.bth.se/fou/forskinfo.nsf/01f1d3898cbbd490c12568160037fb62/e95b68d11a232842c1256db9002d91b3/\$file/Mikael_Svahnberg-Supporting_Software_Architecture_Evolution.pdf}
}

@InProceedings{2003:cascon:synytskyy,
  Title                    = {Robust multilingual parsing using island grammars},
  Author                   = {Nikita Synytskyy and James R. Cordy and Thomas R. Dean},
  Booktitle                = cascon,
  Year                     = {2003},
  Pages                    = {266--278},

  Abstract                 = {Any attempt at automated software analysis or modification must be preceded by a comprehension step, i.e. parsing. This task, while often considered straightforward, can in fact be very challenging for some source code. Files that make up web applications serve as an example of such difficult-to-parse artifacts, for two reasons. First, these files often contain several programming languages at once, sometimes with widely varying syntaxes, and intermingled at the statement level. Second, the code routinely contains syntax errors. Understanding such files calls for a robust parser that can handle multiple languages simultaneously.An approach to creating such a parser, based on the concept of island grammars, is presented here. Island grammars have been used in the past for robust parsing and lightweight analysis of software. Some of the features of these grammars make them uniquely fit for parsing multiple languages simultaneously.}
}

@Book{2002:book:szyperski,
  Title                    = {Component Software: Beyond Object-Oriented Programming},
  Author                   = {Clemens Szyperski},
  Publisher                = {ACM Press},
  Year                     = {2002}
}

@InProceedings{2000:ccece:tahvildari,
  Title                    = {Categorization of object-oriented software metrics},
  Author                   = {Ladan Tahvildari and Ajit Singh},
  Booktitle                = ccece,
  Year                     = {2000},
  Pages                    = {235--239},
  Volume                   = {1},

  Abstract                 = {As software engineering matures as a discipline, practitioners are looking for ways to use measurements in the engineering of their products. As development technology changes, they find their metrics must change as well. Object oriented software development requires a different approach from traditional development methods including the metrics used to evaluate the software. It means that traditional metrics for procedural approaches are not adequate for evaluating object oriented software, primarily because they are not designed to measure basic elements like classes, objects, polymorphism and message passing. Even when adjusted to syntactically analyze object oriented software, they can only capture a small part of such software and therefore can just provide a weak quality indication. The article focuses on the specific demands placed on measures by object oriented software development, and also highlights some of the fundamental differences between object oriented and the more traditional structured development techniques. Also, the article investigates the software product measures that exist in literature and categorizes them to provide a better insight into potential areas of concern, such as depth of inheritance, cohesion, size of objects, and system structure. Such a categorization should enable a practitioner to select a set of metrics that is better suited to one's specific objectives.}
}

@InProceedings{2006:spice:taipale,
  Title                    = {A survey on software testing},
  Author                   = {Taipale, O. and Smolander, K. and K{\"a}lvi{\"a}inen, H.},
  Booktitle                = spice,
  Year                     = {2006}
}

@InProceedings{2010:ecbs:tamzalit,
  Title                    = {Guiding Architectural Restructuring through Architectural Styles},
  Author                   = {Tamzalit, Dalila and Mens, Tom},
  Booktitle                = ecbs,
  Year                     = {2010},
  Pages                    = {69--78},

  Abstract                 = {Software architectures constitute one of the main artefacts of software-intensive system development. They outline the essential components and interconnections of a software system at a high level of abstraction, ignoring unnecessary details. How to address the evolution of software architectures, however, is still an important topic of current research. In this article, we use UML 2 as architectural description language notation and formalise it with graph transformation, with a proof-of-concept implemented in the AGG tool. We use this formalisation to express and reason about architectural evolution patterns that introduce architectural styles.}
}

@InProceedings{2007:compsac:tamzalit,
  Title                    = {Connectors conveying Software Architecture Evolution},
  Author                   = {Dalila Tamzalit and Nassima Sadou and Mourad Oussalah},
  Booktitle                = compsac,
  Year                     = {2007},
  Pages                    = {391--396},
  Volume                   = {1},

  Abstract                 = {This paper presents our work on component-based software evolution. More precisely, we highlight how connectors enhance component-based software architecture evolution. Indeed, connectors, by their intermediary's position between components, can play an important role for automating the propagation of evolution impacts, while preserving the architecture coherence. For that, we outline connectors' characteristics that we consider as principal for aims of evolution management. We validate and exploit these characteristics on our evolution model SAEV (Software Architecture Evolution Model). To achieve this, we need to enrich the concept of connector with information about the degree of correlation and of dependency existing between components. For that, we propose to characterize the connector concept with semantic properties, namely: exclusivity/sharing, dependency/ independency, predominance/non predominance, cardinality and reverse cardinality.}
}

@InProceedings{2006:seke:tamzalit,
  Title                    = {Evolution Problem within Component-Based Software Architecture},
  Author                   = {Dalila Tamzalit and Nassima Sadou and Mourad Oussalah},
  Booktitle                = seke,
  Year                     = {2006},
  Pages                    = {296--301},

  Abstract                 = {TBD}
}

@Book{2005:book:tan,
  Title                    = {Introduction to Data Mining},
  Author                   = {Pang-Ning Tan and Michael Steinbach and Vipin Kumar},
  Publisher                = {Addison Wesley},
  Year                     = {2005},

  Abstract                 = {Introduction to Data Mining presents fundamental concepts and algorithms for those learning data mining for the first time. Each concept is explored thoroughly and supported with numerous examples. The text requires only a modest background in mathematics. Each major topic is organized into two chapters, beginning with basic concepts that provide necessary background for understanding each data mining technique, followed by more advanced concepts and algorithms.}
}

@InProceedings{2007:ase:taneja,
  Title                    = {Automated Detection of {API} Refactorings in Libraries},
  Author                   = {Taneja, Kunal and Dig, Danny and Xie, Tao},
  Booktitle                = ase,
  Year                     = {2007},
  Pages                    = {377--380}
}

@Article{2010:jss:tang,
  Title                    = {A comparative study of architecture knowledge management tools},
  Author                   = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Ali Babar, Muhammad},
  Journal                  = jss,
  Year                     = {2010},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {352--370},
  Volume                   = {83},

  Abstract                 = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.},
  Doi                      = {10.1016/j.jss.2009.08.032}
}

@InProceedings{1999:metrics:tang,
  Title                    = {An empirical study on object-oriented metrics},
  Author                   = {Mei-Huei Tang and Ming-Hung Kao and Mei-Hwa Chen},
  Booktitle                = metrics,
  Year                     = {1999},
  Pages                    = {242--249},

  Abstract                 = {The objective of this study is the investigation of the correlation between object-oriented design metrics and the likelihood of the occurrence of object oriented faults. Such a relationship, if identified, can be utilized to select effective testing techniques that take the characteristics of the program under test into account. Our empirical study was conducted on three industrial real-time systems that contain a number of natural faults reported for the past three years. The faults found in these three systems are classified into three types: object-oriented faults, object management faults and traditional faults. The object-oriented design metrics suite proposed by Chidamber and Kemerer (1994) is validated using these faults. Moreover, we propose a set of new metrics that can serve as an indicator of how strongly object-oriented a program is, so that the decision to adopt object oriented testing techniques can be made, to achieve more reliable testing and yet minimize redundant testing efforts.}
}

@InProceedings{2008:oopsla:tansey,
  Title                    = {Annotation refactoring: Inferring upgrade transformations for legacy applications},
  Author                   = {Wesley Tansey and Eli Tilevich},
  Booktitle                = oopsla,
  Year                     = {2008},
  Pages                    = {295--312},

  Abstract                 = {Since annotations were added to the Java language, many frameworks have moved to using annotated Plain Old Java Objects (POJOs) in their newest releases. Legacy applications are thus forced to undergo extensive restructuring in order to migrate from old framework versions to new versions based on annotations (Version Lock-in). Additionally, because annotations are embedded in the application code, changing between framework vendors may also entail largescale manual changes (Vendor Lock-in). This paper presents a novel refactoring approach that effectively solves these two problems. Our approach infers a concise set of semantics-preserving transformation rules from two versions of a single class. Unlike prior approaches that detect only simple structural refactorings, our algorithm can infer general composite refactorings and is more than 97\% accurate on average. We demonstrate the effectiveness of our approach by automatically upgrading more than 80K lines of the unit testing code of four open-source Java applications to use the latest version of the popular JUnit testing framework.}
}

@InProceedings{1999:icse:tarr,
  Title                    = {{$N$} Degrees of Separation: Multi-Dimensional Separation of Concerns},
  Author                   = {Peri L. Tarr and Harold Ossher and William H. Harrison and Sutton, Jr., Stanley M.},
  Booktitle                = icse,
  Year                     = {1999},
  Pages                    = {107--119},

  Abstract                 = {Done well, separation of concerns can provide many software engineering benefits, including reduced complexity, improved reusability, and simpler evolution. The choice of boundaries for separate concerns depends on both requirements on the system and on the kind(s) of decomposition and composition a given formalism supports. The predominant methodologies and formalisms available, however, support only orthogonal separations of concerns, along single dimensions of composition and decomposition. These characteristics lead to a number of well-known and difficult problems. This paper describes a new paradigm for modeling and implementing software artifacts, one that permits sparation of overlapping concerns along multiple dimensions of composition and decomposition. This approach addresses numerous problems throughout the software lifecycle in achieving well-engineered, evolvable, flexible software artifacts and traceability across artifacts.}
}

@TechReport{2002:nist:tassey,
  Title                    = {The economic impacts of inadequate infrastructure for software testing},
  Author                   = {Tassey, Gregory},
  Institution              = {National Institute of Standards and Technology},
  Year                     = {2002},
  Number                   = {02-3},
  Type                     = {Planning report}
}

@InProceedings{1997:cbr:tautz,
  Title                    = {Using case-based reasoning for reusing software knowledge},
  Author                   = {Tautz, Carsten and Klaus-Dieter Althoff},
  Booktitle                = cbr,
  Year                     = {1997},
  Pages                    = {156--165},
  Series                   = lncs,
  Volume                   = {1266},

  Abstract                 = {Reuse of software knowledge is a principle for improving productivity and reliability of software development. To achieve this, reuse must be done systematically. This means that processes for retrieving, reusing, revising, and retaining have to be defined. At the same time organizational issues (such as the establishment of a separate organizational unit responsible for organizational learning) must be considered. In this paper we compare software knowledge reuse models to the CBR cycle of Aamodt and Plaza and show that the approaches are very similar. We suggest to extend the CBR cycle by including organizational issues explicitly and conclude that CBR is a promising technology for realizing software knowledge reuse if our suggested organizational extensions are considered.},
  Doi                      = {10.1007/3-540-63233-6_488}
}

@Book{1996:book:taylor,
  Title                    = {An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements},
  Author                   = {John R. Taylor},
  Publisher                = {University Science Books},
  Year                     = {1996},
  Edition                  = {2nd}
}

@Manual{2010:manual:r,
  Title                    = {{R}: A Language and Environment for Statistical Computing},
  Author                   = {R Development Core Team},
  Organization             = {R Foundation for Statistical Computing}
}

@InProceedings{2010:apsec:tempero,
  Title                    = {The {Q}ualitas {C}orpus: A Curated Collection of {J}ava Code for Empirical Studies},
  Author                   = {Ewan Tempero and Craig Anslow and Jens Dietrich and Ted Han and Jing Li and Markus Lumpe and Hayden Melton and James Noble},
  Booktitle                = apsec,
  Year                     = {2010},
  Pages                    = {336--345},

  Abstract                 = {In order to increase our ability to use measurement to support software development practise we need to do more analysis of code. However, empirical studies of code are expensive and their results are difficult to compare. We describe the Qualitas Corpus, a large curated collection of open source Java systems. The corpus reduces the cost of performing large empirical studies of code and supports comparison of measurements of the same artifacts. We discuss its design, organisation, and issues associated with its development.}
}

@InProceedings{1973:scs:teorey,
  Title                    = {Considerations on the level of detail in simulation},
  Author                   = {Teorey, Tobey J. and Merten, Alan G.},
  Booktitle                = scs,
  Year                     = {1973},
  Pages                    = {137--143},

  Abstract                 = {The application of digital computers to simulate large discrete systems can be severely limited by time and cost considerations when the level of detail attempted is very small. For feasibility in actual simulation execution time, many systems are necessarily modeled at higher, less detailed levels in order to reduce the time parameter range and to keep the amount of simulation event processing low. Such an approach frequently leads to results that are invalid or of little practical use to systems designers. An approach to modeling various levels of detail automatically within a single computer run is described. The feasibility of integrating two or more levels by describing sub-models within each level as separate procedures, only to be called from procedures at the next higher level, is investigated. Typically, each simulation level would have a specific set of inputs (workloads), model parameters, and output responses. The design of an integrated simulation model must consider the problems of interfacing the input of level j with the output of level j+1, and so on for each 1 $\le$ j $\le$ N where N is the number of levels integrated into the model. The paper describes the problems of interfacing two levels of simulation and obtaining meaningful results within a feasible real time period relative to single-level simulations.}
}

@InProceedings{2012:csmr:terra,
  Title                    = {Recommending Refactorings to Reverse Software Architecture Erosion},
  Author                   = {Ricardo Terra and Marco Tulio Valente and Krzysztof Czarnecki and Roberto S. Bigonha},
  Booktitle                = csmr,
  Year                     = {2012},
  Pages                    = {335--340},

  Abstract                 = {Architectural erosion is a recurrent problem faced by software architects. Despite this fact, the process is usually tackled in ad hoc way, without adequate tool support at the architecture level. To address this issue, we describe the preliminary design of a recommendation system whose main purpose is to provide refactoring guidelines for developers and maintainers during the task of reversing an architectural erosion process. The paper formally describes first recommendations proposed in our current research and results of their application in a web-based application.},
  Doi                      = {10.1109/CSMR.2012.40}
}

@InProceedings{2012:cbsoft:terra,
  Title                    = {{DCLfix}: A Recommendation System for Repairing Architectural Violations},
  Author                   = {Ricardo Terra and Marco T{\'u}lio Valente and Roberto S. Bigonha and Krzysztof Czarnecki},
  Booktitle                = cbsoft,
  Year                     = {2012},
  Note                     = {6 pages},

  Abstract                 = {Architectural erosion is a recurrent problem in software evolution. Despite this fact, the process is usually tackled in ad hoc ways, without adequate tool support at the architecture level. To address this shortcoming, this paper presents a recommendation system---called DCLfix---that provides refactoring guidelines for maintainers when tackling architectural erosion. In short, DCLfix suggests refactoring recommendations for violations detected after an architecture conformance process using DCL, an architectural constraint language.}
}

@InCollection{2001:book:carroll:terveen,
  Title                    = {Beyond Recommender Systems: Helping People Help Each Other},
  Author                   = {Loren Terveen and Will Hill},
  Booktitle                = {Human-Computer Interaction in the New Millennium},
  Publisher                = {Addison Wesley},
  Year                     = {2001},
  Editor                   = {John M. Carroll},
  Pages                    = {487--509},

  Abstract                 = {The Internet and World Wide Web have brought us into a world of endless possibilities: interactive Web sites to experience, music to listen to, conversations to participate in, and every conceivable consumer item to order. But this world also is one of endless choice: how can we select from a huge universe of items of widely varying quality? Computational recommender systems have emerged to address this issue. They enable people to share their opinions and benefit from each other's experience. We present a framework for understanding recommender systems and survey a number of distinct approaches in terms of this framework. We also suggest two main research challenges: (1) helping people form communities of interest while respecting personal privacy, and (2) developing algorithms that combine multiple types of information to compute recommendations.}
}

@InProceedings{2011:icse:thomas,
  Title                    = {Mining software repositories using topic models},
  Author                   = {Thomas, Stephen W.},
  Booktitle                = icse,
  Year                     = {2011},
  Pages                    = {1138--1139},

  Abstract                 = {Software repositories, such as source code, email archives, and bug databases, contain unstructured and unlabeled text that is difficult to analyze with traditional techniques. We propose the use of statistical topic models to automatically discover structure in these textual repositories. This discovered structure has the potential to be used in software engineering tasks, such as bug prediction and traceability link recovery. Our research goal is to address the challenges of applying topic models to software repositories.},
  Doi                      = {10.1145/1985793.1986020}
}

@InProceedings{2007:ase:thummalapenta,
  Title                    = {{PARSEWeb}: A programmer assistant for reusing open source code on the web},
  Author                   = {Suresh Thummalapenta and Tao Xie},
  Booktitle                = ase,
  Year                     = {2007},
  Pages                    = {204--213},

  Abstract                 = {Programmers commonly reuse existing frameworks or libraries to reduce software development efforts. One common problem in reusing the existing frameworks or libraries is that the programmers know what type of object that they need, but do not know how to get that object with a specific method sequence. To help programmers to address this issue, we have developed an approach that takes queries of the form ``Source object type $\rightarrow$ Destination object type" as input, and suggests relevant method-invocation sequences that can serve as solutions that yield the destination object from the source object given in the query. Our approach interacts with a code search engine (CSE) to gather relevant code samples and performs static analysis over the gathered samples to extract required sequences. As code samples are collected on demand through CSE, our approach is not limited to queries of any specific set of frameworks or libraries. We have implemented our approach with a tool called PARSEWeb, and conducted four different evaluations to show that our approach is effective in addressing programmer's queries. We also show that PARSEWeb performs better than existing related tools: Prospector and Strathcona.},
  Doi                      = {10.1145/1321631.1321663}
}

@InProceedings{2003:oopsla:tip,
  Title                    = {Refactoring for Generalization using Type Constraints},
  Author                   = {Frank Tip and Adam Kiezun and Dirk B{\"a}umer},
  Booktitle                = oopsla,
  Year                     = {2003},
  Pages                    = {13--26},

  Abstract                 = {Refactoring is the process of applying behavior-preserving transformations (called ``refactorings") in order to improve a program's design. Associated with a refactoring is a set of preconditions that must be satisfied to guarantee that program behavior is preserved, and a set of source code modifications. An important category of refactorings is concerned with generalization (e.g., Extract Interface for re-routing the access to a class via a newly created interface, and Pull Up Members for moving members into a superclass). For these refactorings, both the preconditions and the set of allowable source code modifications depend on interprocedural relationships between types of variables. We present an approach in which type constraints are used to verify the preconditions and to determine the allowable source code modifications for a number of generalization-related refactorings. This work is implemented in the standard distribution of Eclipse (see www.eclipse.org).},
  Doi                      = {10.1145/949305.949308}
}

@InProceedings{2004:issta:tonella,
  Title                    = {Evolutionary testing of classes},
  Author                   = {Tonella, Paolo},
  Booktitle                = issta,
  Year                     = {2004},
  Pages                    = {119--128},

  Doi                      = {10.1145/1007512.1007528}
}

@InProceedings{2004:vlhcc:toomim,
  Title                    = {Managing duplicated code with linked editing},
  Author                   = {Michael Toomim and Andrew Begel and Susan L. Graham},
  Booktitle                = vlhcc,
  Year                     = {2004},
  Pages                    = {173--180},

  Abstract                 = {We present linked editing, a novel, lightweight editor-based technique for managing duplicated source code. Linked editing is implemented in a prototype editor called Codelink. We argue that the use of programming abstractions like functions and macros---the traditional solution to duplicated code---has inherent cognitive costs, leading programmers to chronically copy and paste code instead. Our user study compares functional abstraction with linked editing and shows that linked editing can give the benefits of abstraction with orders of magnitude decrease in programming time.}
}

@InProceedings{2003:sste:torkar,
  Title                    = {A Survey on Testing and Reuse},
  Author                   = {Torkar, Richard and Mankefors, Stefan},
  Booktitle                = sste,
  Year                     = {2003},
  Pages                    = {164--173},

  Abstract                 = {This survey tries to give an account of what type of trends exist today in software reuse and testing. The focuswas to try to find out how developers use different tools today and what tools are lacking, especially in the field ofreuse and testing. The population came from different types of communities and organizations, to better give us a generalized picture of today's developers. We found that a majority of the developers participating in the survey did not test reused code and other testing methodologies were not used to the extent that the scientific community takes for granted. A more automated approach to testing in combination with code coverage analysis and statistical analysis was found to be needed.},
  Doi                      = {10.1109/SWSTE.2003.1245437}
}

@InProceedings{2009:chi:torrey,
  Title                    = {Learning how: The search for craft knowledge on the internet},
  Author                   = {Torrey, Cristen and Churchill, Elizabeth F. and McDonald, David W.},
  Booktitle                = chi,
  Year                     = {2009},
  Pages                    = {1371--1380},

  Abstract                 = {Communicating the subtleties of a craft technique, like putting a zipper into a garment or throwing a clay pot, can be challenging even when working side by side. Yet How-To content---including text, images, animations, and videos---is available online for a wide variety of crafts. We interviewed people engaged in various crafts to investigate how online resources contributed to their craft practice. We found that participants sought creative inspiration as well as technical clarification online. In this domain, keyword search can be difficult, so supplemental strategies are used. Participants sought information iteratively, because they often needed to enact their knowledge in order to evaluate it. Our description of people learning how allows us to elaborate existing understandings of information-seeking behavior by considering how search originates and is evaluated in knowledge domains involving physical objects and physical processes.}
}

@InProceedings{1995:ssr:tracz,
  Title                    = {Confessions of a used-program salesman: Lessons learned},
  Author                   = {Tracz, Will},
  Booktitle                = ssr,
  Year                     = {1995},
  Pages                    = {11--13},

  Abstract                 = {Software reuse is the second oldest programming profession. Ever since the first program logic board was wired, people have been looking for ways of saving time and money by building upon other's efforts and not ``not re-inventing any wheels.'' This article summarizes the lessons I have learned as used-program salesman. Using this analogy, I will examine efforts made to institutionalize software reuse.},
  Doi                      = {10.1145/223427.211785}
}

@Article{1990:sen:tracz,
  Title                    = {Where does reuse start?},
  Author                   = {Will Tracz},
  Journal                  = sen,
  Year                     = {1990},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {42--46},
  Volume                   = {15},

  Abstract                 = {Software reuse is the type of thing some people swear by. It is also the type of thing that some people swear at. Software reuse is a religion, a religion that all of us here today pretty much have accepted and embraced. The goal of this talk is to question the foundation of our faith--- to test the depth of our convictions with the hope of shedding new light on our intuitions. I do not claim to have experienced divine intervention. You don't need to take what I say as gospel truth. I believe in what I say, but what you hear may be something different. Again, let me encourage you to disagree---to challenge the position I have taken on the issues I will be presenting. Before I proceed further, I need to qualify software reuse by providing a definition. Software reuse, to me, is the process of reusing software that was designed to be reused. Software reuse is distinct from software salvaging, that is reusing software that was not designed to be reused. Furthermore, software reuse is distinct from carrying-over code, that is reusing code from one version of an application to another. To summarize, reusable software is software that was designed to be reused. The major portion of my talk will focus on examining the rhetorical question, "Where does reuse start?"},
  Doi                      = {10.1145/382296.382702}
}

@InProceedings{2000:iwpc:tran,
  Title                    = {Architectural Repair of Open Source Software},
  Author                   = {John B. Tran and Michael W. Godfrey and Eric H. S. Lee and Richard C. Holt},
  Booktitle                = iwpc,
  Year                     = {2000},
  Pages                    = {48--59},

  Abstract                 = {As a software system evolves, its architecture will drift. System changes are often done without considering their effects on the system structure. These changes often introduce structural anomalies between the concrete (as-built) and the conceptual (as-designed) architecture, which can impede program understanding. The problem of architectural drift is especially pronounced in open source systems, where many developers work in isolation on distinct features with little co-ordination. In this paper, we present our experiences with repairing the architectures of two large open source systems (the Linux operating system kernel and the VIM text editor) to aid program understanding. For both systems, we were successful in removing many structural anomalies from their architectures.}
}

@InProceedings{2004:isoortdc:tsang,
  Title                    = {An Evaluation of Aspect-Oriented Programming for {J}ava-Based Real-Time Systems Development},
  Author                   = {Shiu Lun Tsang and Siobh{\'a}n Clarke and Elisa Baniassad},
  Booktitle                = isoortdc,
  Year                     = {2004},
  Pages                    = {291--300},

  Abstract                 = {Some concerns, such as debugging or logging functionality, cannot be captured cleanly, and are often tangled and scattered throughout the code base. These concerns are called crosscutting concerns. Aspect-Oriented Programming (AOP) is a paradigm that enables developers to capture crosscutting concerns in separate aspect modules. The use of aspects has been shown to improve understandability and maintainability of systems. It has been shown that real-time concerns, such as memory management and thread scheduling, are crosscutting concerns [5, 6, 9, 11]. However it is unclear whether encapsulating these concerns provides benefits. We were interested in determining whether using AOP to encapsulate real-time crosscutting concerns afforded benefits in system properties such as understandability and maintainability. This paper presents research comparing the system properties of two systems: a real-time sentient traffic simulator and its Aspect-Oriented equivalent. An evaluation of AOP is presented indicating both benefits and drawbacks with this approach.}
}

@Article{2006:tse:tsantalis,
  Title                    = {Design Pattern Detection Using Similarity Scoring},
  Author                   = {Tsantalis, Nikolaos and Chatzigeorgiou, Alexander and Stephanides, George and Halkidis, Spyros T.},
  Journal                  = tse,
  Year                     = {2006},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {896--909},
  Volume                   = {32},

  Abstract                 = {The identification of design patterns as part of the reengineering process can convey important information to the designer. However, existing pattern detection methodologies generally have problems in dealing with one or more of the following issues: identification of modified pattern versions, search space explosion for large systems and extensibility to novel patterns. In this paper, a design pattern detection methodology is proposed that is based on similarity scoring between graph vertices. Due to the nature of the underlying graph algorithm, this approach has the ability to also recognize patterns that are modified from their standard representation. Moreover, the approach exploits the fact that patterns reside in one or more inheritance hierarchies, reducing the size of the graphs to which the algorithm is applied. Finally, the algorithm does not rely on any pattern-specific heuristic, facilitating the extension to novel design structures. Evaluation on three open-source projects demonstrated the accuracy and the efficiency of the proposed method.},
  Doi                      = {10.1109/TSE.2006.112}
}

@InCollection{2010:book:aggarwal:tsuda,
  Title                    = {Graph Classification},
  Author                   = {Koji Tsuda and Hiroto Saigo},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {11},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {337--363},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {Supervised learning on graphs is a central subject in graph data processing. In graph classification and regression, we assume that the target values of a certain number of graphs or a certain part of a graph are available as a training dataset, and our goal is to derive the target values of other graphs or the remaining part of the graph. In drug discovery applications, for example, a graph and its target value correspond to a chemical compound and its chemical activity. In this chapter, we review state-of-the-art methods of graph classification. In particular, we focus on two representative methods, graph kernels and graph boosting, and we present other methods in relation to the two methods. We describe the strengths and weaknesses of different graph classification methods and recent efforts to overcome the challenges.},
  Doi                      = {10.1007/978-1-4419-6045-0_11}
}

@InProceedings{2002:iwpc:tu,
  Title                    = {An integrated approach for studying architectural evolution},
  Author                   = {Qiang Tu and Michael W. Godfrey},
  Booktitle                = iwpc,
  Year                     = {2002},
  Pages                    = {127--136},

  Abstract                 = {Studying how a software system has evolved over time is difficult, time consuming, and costly; existing techniques are often limited in their applicability, are hard to extend, and provide little support for coping with architectural change. The paper introduces an approach to studying software evolution that integrates the use of metrics, software visualization, and origin analysis, which is a set of techniques for reasoning about structural and architectural change. Our approach incorporates data from various statistical and metrics tools, and provides a query engine as well as a Web-based visualization and navigation interface. It aims to provide an extensible, integrated environment for aiding software maintainers in understanding the evolution of long-lived systems that have undergone significant architectural change. We use the evolution of GCC as an example to demonstrate the uses of various functionalities of BEAGLE, a prototype implementation of the proposed environment.}
}

@Book{2008:book:tulach,
  Title                    = {Practical {API} Design: Confessions of a {J}ava Framework Architect},
  Author                   = {Tulach, Jaroslav},
  Publisher                = {Apress},
  Year                     = {2008}
}

@Article{1985:infocon:ukkonen,
  Title                    = {Algorithms for approximate string matching},
  Author                   = {Esko Ukkonen},
  Journal                  = infocon,
  Year                     = {1985},

  Month                    = jan # {--} # mar,
  Number                   = {1--3},
  Pages                    = {100--118},
  Volume                   = {64},

  Abstract                 = {The edit distance between strings $a_1 \ldots a_m$ and $b_1 \ldots b_n$ is the minimum cost $s$ of a sequence of editing steps (insertions, deletions, changes) that convert one string into the other. A well-known tabulating method computes $s$ as well as the corresponding editing sequence in time and in space $O(mn)$ (in space $O(\min(m, n))$ if the editing sequence is not required). Starting from this method, we develop an improved algorithm that works in time and in space $O(s \cdot \min(m, n))$. Another improvement with time $O(s \cdot \min(m, n))$ and space $O(s \cdot \min(s, m, n))$ is given for the special case where all editing steps have the same cost independently of the characters involved. If the editing sequence that gives cost $s$ is not required, our algorithms can be implemented in space $O(\min(s, m, n))$. Since $s = O(\max(m, n))$, the new methods are always asymptotically as good as the original tabulating method. As a by-product, algorithms are obtained that, given a threshold value $t$, test in time $O(t \cdot \min(m, n))$ and in space $O(\min(t, m, n))$ whether $s \le t$. Finally, different generalized edit distances are analyzed and conditions are given under which our algorithms can be used in conjunction with extended edit operation sets, including, for example, transposition of adjacent characters.},
  Doi                      = {10.1016/S0019-9958(85)80046-2}
}

@Article{1976:jacm:ullmann,
  Title                    = {An Algorithm for Subgraph Isomorphism},
  Author                   = {Ullmann, J. R.},
  Journal                  = jacm,
  Year                     = {1976},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {31--42},
  Volume                   = {23},

  Abstract                 = {Subgraph isomorphism can be determined by means of a brute-force tree-search enumeration procedure. In this paper a new algorithm is introduced that attains efficiency by inferentially eliminating successor nodes in the tree search. To assess the time actually taken by the new algorithm, subgraph isomorphism, clique detection, graph isomorphism, and directed graph isomorphism experiments have been carried out with random and with various nonrandom graphs. A parallel asynchronous logic-in-memory implementation of a vital part of the algorithm is also described, although this hardware has not actually been built. The hardware implementation would allow very rapid determination of isomorphism.},
  Doi                      = {10.1145/321921.321925}
}

@Article{1988:tpami:umeyama,
  Title                    = {An Eigendecomposition Approach to Weighted Graph Matching Problems},
  Author                   = {Umeyama, Shinji},
  Journal                  = tpami,
  Year                     = {1988},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {695--703},
  Volume                   = {10},

  Abstract                 = {An approximate solution to the weighted-graph-matching problem is discussed for both undirected and directed graphs. The weighted-graph-matching problem is that of finding the optimum matching between two weighted graphs, which are graphs with weights at each arc. The proposed method uses an analytic instead of a combinatorial or iterative approach to the optimum matching problem. Using the eigendecompositions of the adjacency matrices (in the case of the undirected-graph-matching problem) or Hermitian matrices derived from the adjacency matrices (in the case of the directed-graph-matching problem), a matching close to the optimum can be found efficiently when the graphs are sufficiently close to each other. Simulation results are given to evaluate the performance of the proposed method.},
  Doi                      = {10.1109/34.6778}
}

@Article{2002:europhyslett:valverde,
  Title                    = {Scale-free networks from optimal design},
  Author                   = {S. Valverde and Ferrer Cancho, R. and R. V. Sol{\'e}},
  Journal                  = europhyslett,
  Year                     = {2002},
  Number                   = {4},
  Pages                    = {512--517},
  Volume                   = {60},

  Abstract                 = {A large number of complex networks, both natural and artificial, share the presence of highly heterogeneous, scale-free degree distributions. A few mechanisms for the emergence of such patterns have been suggested, optimization not being one of them. In this letter we present the first evidence for the emergence of scaling (and the presence of small-world behavior) in software architecture graphs from a well-defined local optimization process. Although the rules that define the strategies involved in software engineering should lead to a tree-like structure, the final net is scale-free, perhaps reflecting the presence of conflicting constraints unavoidable in a multidimensional optimization process. The consequences for other complex networks are outlined.},
  Doi                      = {10.1209/epl/i2002-00248-2}
}

@Article{2005:europhyslett:valverde,
  Title                    = {Logarithmic growth dynamics in software networks},
  Author                   = {Sergi Valverde and Ricard V. Sol{\'e}},
  Journal                  = europhyslett,
  Year                     = {2005},
  Number                   = {5},
  Pages                    = {858--864},
  Volume                   = {72},

  Abstract                 = {In a recent paper, Krapivsky and Redner (Phys. Rev. E, 71 (2005) 036118) proposed a new growing network model with new nodes being attached to a randomly selected node, as well to all ancestors of the target node. The model leads to a sparse graph with an average degree growing logarithmically with the system size. Here we present compeling evidence for software networks being the result of a similar class of growing dynamics. The predicted pattern of network growth, as well as the stationary in- and out-degree distributions are consistent with the model. Our results confirm the view of large-scale software topology being generated through duplication-rewiring mechanisms. Implications of these findings are outlined.},
  Doi                      = {10.1209/epl/i2005-10314-9}
}

@InProceedings{2008:wcre:vanrompaey,
  Title                    = {Estimation of test code changes using historical release data},
  Author                   = {Van Rompaey, Bart and Demeyer, Serge},
  Booktitle                = wcre,
  Year                     = {2008},
  Pages                    = {269--278},

  Doi                      = {10.1109/WCRE.2008.29}
}

@InProceedings{2009:icsm:vasa,
  Title                    = {Comparative analysis of evolving software systems using the {G}ini coefficient},
  Author                   = {Rajesh Vasa and Markus Lumpe and Philip Branch and Oscar Nierstrasz},
  Booktitle                = icsm,
  Year                     = {2009},
  Pages                    = {179--188},

  Abstract                 = {Software metrics offer us the promise of distilling useful information from vast amounts of software in order to track development progress, to gain insights into the nature of the software, and to identify potential problems. Unfortunately, however, many software metrics exhibit highly skewed, non-Gaussian distributions. As a consequence, usual ways of interpreting these metrics---for example, in terms of ``average" values---can be highly misleading. Many metrics, it turns out, are distributed like wealth---with high concentrations of values in selected locations. We propose to analyze software metrics using the Gini coefficient, a higher-order statistic widely used in economics to study the distribution of wealth. Our approach allows us not only to observe changes in software systems efficiently, but also to assess project risks and monitor the development process itself. We apply the Gini coefficient to numerous metrics over a range of software projects, and we show that many metrics not only display remarkably high Gini values, but that these values are remarkably consistent as a project evolves over time.}
}

@InProceedings{2006:icse:verbaere,
  Title                    = {{JunGL}: A scripting language for refactoring},
  Author                   = {Verbaere, Mathieu and Ettinger, Ran and de Moor, Oege},
  Booktitle                = icse,
  Year                     = {2006},
  Pages                    = {172--181},

  Abstract                 = {Refactorings are behaviour-preserving program transformations, typically for improving the structure of existing code. A few of these transformations have been mechanised in interactive development environments. Many more refactorings have been proposed, and it would be desirable for programmers to script their own refactorings. Implementing such source-to-source transformations, however, is quite complex: even the most sophisticated development environments contain significant bugs in their refactoring tools. We present a domain-specific language for refactoring, named JunGL. It manipulates a graph representation of the program: all information about the program, including ASTs for its compilation units, variable binding, control flow and so on is represented in a uniform graph format. The language is a hybrid of a functional language (in the style of ML) and a logic query language (akin to Datalog). JunGL furthermore has a notion of demand-driven evaluation for constructing computed information in the graph, such as control flow edges. Borrowing from earlier work on the specification of compiler optimisations, JunGL uses socalled `path queries' to express dataflow properties. We motivate the design of JunGL via a number of nontrivial refactorings, and describe its implementation on the .NET platform.}
}

@InProceedings{2010:gcms:verl,
  Title                    = {Analysis of different approaches to the reuse of models for the virtual knowledge-based product development},
  Author                   = {Verl, Alexander and Mueller, Verena},
  Booktitle                = gcms,
  Year                     = {2010},
  Pages                    = {7--11},

  Abstract                 = {In order to increase the reliability and quality of mechatronic systems the systematic testing in an early stage of the development process is essential. Especially in the development of complex and expensive products simulation has proved its value for riskless testing. To make simulation in product development more efficient the project ``Reusable Models for the virtual knowledge-based product development (WieMod)" which is promoted by the Federal Ministry of Education and Research analyzes methods which allow the reuse of simulation models for miscellaneous discipline-specific simulation tools. Thereby quality and at the same time efficiency of simulation can be increased. The gain of quality results from the fact, that this way a simulation model can be tested in varying simulation tools and thereby under different aspects. This repeated testing leads to well-proven simulation models, which are of benefit to every further simulation model due to the possibility of reuse. Now that simulation models do not have to be newly developed for every simulation tool their period of amortization is reduced which increases the efficiency of simulation. For this kind of reuse a meta model has been developed that is universally valid on the one hand so it can be used in different simulation tools, but is still specific enough on the other hand, that no simulation tool-specific information is lost.}
}

@InCollection{2004:book:lengauer:visser,
  Title                    = {Program Transformation with {Stratego/XT}: Rules, Strategies, Tools, and Systems in {Stratego/XT} 0.9},
  Author                   = {Visser, Eelco},
  Booktitle                = {Domain-Specific Program Generation},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2004},
  Editor                   = {Lengauer, Christian and Batory, Don and Consel, Charles and Odersky, Martin},
  Pages                    = {216--238},
  Volume                   = {3016},

  Abstract                 = {Stratego/XT is a framework for the development of transformation systems aiming to support a wide range of program transformations. The framework consists of the transformation language Stratego and the XT collection of transformation tools. Stratego is based on the paradigm of rewriting under the control of programmable rewriting strategies. The XT tools provide facilities for the infrastructure of transformation systems including parsing and pretty-printing. The framework addresses the entire range of the development process; from the specification of transformations to their composition into transformation systems. This chapter gives an overview of the main ingredients involved in the composition of transformation systems with Stratego/XT, where we distinguish the abstraction levels of rules, strategies, tools, and systems.},
  Doi                      = {10.1007/978-3-540-25935-0_13}
}

@Misc{2012:misc:vogel,
  Title                    = {{Rich Client Platform}},

  Author                   = {Lars Vogel},
  HowPublished             = {http://\discretionary{}{}{}wiki.eclipse.org/\discretionary{}{}{}index.php/\discretionary{}{}{}Rich\_Client\_Platform},
  Month                    = jan,
  Year                     = {2012},

  Url                      = {http://wiki.eclipse.org/index.php/Rich_Client_Platform}
}

@MastersThesis{2002:msc:wagner,
  Title                    = {Combinatorically Restricted Higher Order Anti-Unification: An Application to Programming by Analogy},
  Author                   = {Ulrich Wagner},
  School                   = {Technische Universit\"{a}t Berlin},
  Year                     = {2002},

  Address                  = {Berlin, Germany},
  Month                    = apr,

  Abstract                 = {Unification, a well known method of symbolic AI, is used to find substitutions that, when applied to the given terms, lead to the same resulting term. Anti-unification (AU), on the contrary, generates the maximally specific term, called anti-instance, which can be transformed into the input terms by instantiation of its variables. Thus, the latter is a natural means to capture structural similarities between terms. This can be used for analogical programming in the context of the IPAL-project, where combination of recursive program folding with AI methods is investigated. Given an initial program and the associated recursive program together with a new initial program, the recursive solution for the new problem can be constructed as follows: First, AU is used to compute similarities and differences between the initial programs, yielding an analogical mapping. Second, the mapping is applied to the existing solution, transforming it in such a way that it solves the new problem. As it turns out, first order AU is too weak to capture structural similarities embedded in different context, while unrestricted second order AU is no more computable. This motivates the search for suitable restrictions which combine the well-definedness of the first with the expressive power of the second. Using the theoretical framework of R. W. Hasker, the notion of relevant combinators leads to a computable higher order AU algorithm that can be applied to the folding problems of IPAL.},
  Url                      = {http://www2.informatik.uni-osnabrueck.de/schmid/tubhp/diparbstub/wagner/DIPLOM.pdf}
}

@InProceedings{2004:scam:wahler,
  Title                    = {Clone Detection in Source Code by Frequent Itemset Techniques},
  Author                   = {Wahler, Vera and Seipel, Dietmar and Wolff von Gudenberg, J{\"u}rgen and Fischer, Gregor},
  Booktitle                = scam,
  Year                     = {2004},
  Pages                    = {128--135},

  Abstract                 = {In this paper we describe a new approach for the detection of clones in source code, which is inspired by the concept of frequent itemsets from data mining. The source code is represented as an abstract syntax tree in XML. Currently, such XML representations exist for instance for Java, C++, or PROLOG. Our approach is very flexible; it can be configured easily to work with multiple programming languages.}
}

@Article{2013:tosem:walkinshaw,
  Title                    = {Automated comparison of state-based software models in terms of their language and structure},
  Author                   = {Walkinshaw, Neil and Bogdanov, Kirill},
  Journal                  = tosem,
  Year                     = {2013},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {13:1--13:37},
  Volume                   = {22},

  Abstract                 = {State machines capture the sequential behavior of software systems. Their intuitive visual notation, along with a range of powerful verification and testing techniques render them an important part of the model-driven software engineering process. There are several situations that require the ability to identify and quantify the differences between two state machines (e.g. to evaluate the accuracy of state machine inference techniques is measured by the similarity of a reverse-engineered model to its reference model). State machines can be compared from two complementary perspectives: (1) In terms of their language---the externally observable sequences of events that are permitted or not, and (2) in terms of their structure---the actual states and transitions that govern the behavior. This article describes two techniques to compare models in terms of these two perspectives. It shows how the difference can be quantified and measured by adapting existing binary classification performance measures for the purpose. The approaches have been implemented by the authors, and the implementation is openly available. Feasibility is demonstrated via a case study to compare two real state machine inference approaches. Scalability and accuracy are assessed experimentally with respect to a large collection of randomly synthesized models.},
  Doi                      = {10.1145/2430545.2430549}
}

@InCollection{2010:book:aggarwal:wang,
  Title                    = {A Survey of Algorithms for Keyword Search on Graph Data},
  Author                   = {Haixun Wang and Charu C. Aggarwal},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {8},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {249--273},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {In this chapter, we survey methods that perform keyword search on graph data. Keyword search provides a simple but user-friendly interface to retrieve information from complicated data structures. Since many real life datasets are represented by trees and graphs, keyword search has become an attractive mechanism for data of a variety of types. In this survey, we discuss methods of keyword search on schema graphs, which are abstract representation for XML data and relational data, and methods of keyword search on schema-free graphs. In our discussion, we focus on three major challenges of keyword search on graphs. First, what is the semantics of keyword search on graphs, or, what qualifies as an answer to a keyword search; second, what constitutes a good answer, or, how to rank the answers; third, how to perform keyword search efficiently. We also discuss some unresolved challenges and propose some new research directions on this topic.},
  Doi                      = {10.1007/978-1-4419-6045-0_8}
}

@Article{2001:jsmerp:wang,
  Title                    = {A case study in repeated maintenance},
  Author                   = {Shuanglin Wang and Stephen R. Schach and Gillian Z. Heller},
  Journal                  = jsmerp,
  Year                     = {2001},

  Month                    = mar # {/} # apr,
  Number                   = {2},
  Pages                    = {127--141},
  Volume                   = {13},

  Abstract                 = {RTP is a widely used commercial real-time product that has been maintained over a period of 13 years. We have analyzed multiple versions of RTP, which is written in C and Assembler. We measured increases in dependencies within the code between successive versions and performed statistical analyses on the data. There was no significant difference between the maintenance of Assembler files and C files. Also, there was no significant difference between the versions written by the original developers and those written by maintenance programmers not involved in the original development. The differences between individual programmers were very highly significant. Our interpretation of these results is that the skill of the individual programmer is an important factor in ensuring that a software product remains maintainable over its lifetime and that software engineering education and training are therefore of major importance.},
  Doi                      = {10.1002/smr.227}
}

@InProceedings{2008:icse:wang,
  Title                    = {An approach to detecting duplicate bug reports using natural language and execution information},
  Author                   = {Wang, Xiaoyin and Zhang, Lu and Xie, Tao and Anvik, John and Sun, Jiasu},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {461--470},

  Abstract                 = {An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as DUPLICATE and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. In this paper we present a new approach that further involves execution information. In our approach, when a new bug report arrives, its natural language information and execution information are compared with those of the existing bug reports. Then, a small number of existing bug reports are suggested to the triager as the most similar bug reports to the new bug report. Finally, the triager examines the suggested bug reports to determine whether the new bug report duplicates an existing bug report. We calibrated our approach on a subset of the Eclipse bug repository and evaluated our approach on a subset of the Firefox bug repository. The experimental results show that our approach can detect 67\%--93\% of duplicate bug reports in the Firefox bug repository, compared to 43\%--72\% using natural language information alone.},
  Doi                      = {10.1145/1368088.1368151}
}

@Article{2000:csur:wang,
  Title                    = {On built-in test reuse in object-oriented framework design},
  Author                   = {Yingxu Wang and Graham King and Mohamed Fayad and Dilip Patel and Ian Court and Geoff Staples and Margaret Ross},
  Journal                  = csur,
  Year                     = {2000},

  Month                    = mar,
  Number                   = {1es},
  Pages                    = {7--12},
  Volume                   = {32},

  Abstract                 = {Object-oriented frameworks have extended reusability of software from code modules to architectural and domain information. This paper further extends software reusability from code and architecture to built-in tests (BITs) in object-oriented framework development. Methods for embedding BITs at object and object-oriented framework levels are addressed. Behaviours of objects and object-oriented frameworks with BITs in the normal and test modes are analyzed. Systematic reuse methods of BITs in object-oriented framework development are provided. The most interesting development in the paper is that the BITs in object-oriented frameworks can be inherited and reused as that of code. Therefore testability and maintainability of the test-built-in object-oriented frameworks can be improved by the BIT approach. The BIT method can be used in analysis, design and coding of object-oriented frameworks.},
  Doi                      = {10.1145/351936.351943}
}

@Article{2000:jilp:wang,
  Title                    = {{BMAT}: A Binary Matching Tool for Stale Profile Propagation},
  Author                   = {Zheng Wang and Ken Pierce and Scott McFarling},
  Journal                  = jilp,
  Year                     = {2000},
  Pages                    = {2:1--2:20},
  Volume                   = {2},

  Abstract                 = {A major challenge of applying profile-based optimization on large real-world applications is how to capture adequate profile information. A large program, especially a GUI-based application, may be used in a large variety of ways by different users on different machines. Extensive collection of profile data is necessary to fully characterize this type of program behavior. Unfortunately, in a realistic software production environment, many developers and testers need fast access to the latest build, leaving little time for collecting profiles. To address this dilemma, we would like to re-use stale profile information from a prior program build. In this paper we present BMAT, a fast and effective tool that matches two versions of a binary program without knowledge of source code changes. BMAT enables the propagation of profile information from an older, extensively profiled build to a newer build, thus greatly reducing or even eliminating the need for re-profiling. We use two metrics to evaluate the quality of the results using propagated profile information: static branch prediction and the accuracy of code coverage. These metrics measure how well the matching algorithm works for the frequently executed core code and across the whole program, respectively. Experiments on a set of large DLLs from Microsoft Windows 2000 and Internet Explorer show that compared to freshly collected profiles, propagated information using BMAT is typically over 99\% as effective in branch prediction and over 98\% as accurate in code coverage information.},
  Url                      = {http://www.jilp.org/vol2/v2paper2.pdf}
}

@InProceedings{1994:icse:wasmund,
  Title                    = {Reuse facts and myths},
  Author                   = {Michael Wasmund},
  Booktitle                = icse,
  Year                     = {1994},
  Pages                    = {273},

  Abstract                 = {This paper on software reuse is the view of a practitioner rather than of a scientist. Myth 1: OO technology eats up reuse. Fact 1: OO does not automatically yield high reuse rates-both OO and reuse can complement each other. Myth 2: Incentives are key to reuse success. Fact 2: Incentives create awareness, are cheap but don't change much. Myth 3: Reuse is for free. Fact 3: Reuse is a mid-term investment impacting the entire software development process. It must be based on a product strategy which spans several releases or a family of products},
  Doi                      = {10.1109/ICSE.1994.296789}
}

@Article{2007:nature:watts,
  Title                    = {Connections: A twenty-first century science},
  Author                   = {Duncan J. Watts},
  Journal                  = nature,
  Year                     = {2007},

  Month                    = {1 } # feb,
  Pages                    = {489},
  Volume                   = {445},

  Abstract                 = {If handled appropriately, data about Internet-based communication and interactivity could revolutionize our understanding of collective human behaviour.},
  Doi                      = {10.1038/445489a}
}

@Article{2002:pnas:watts,
  Title                    = {A simple model of global cascades on random networks},
  Author                   = {Duncan J. Watts},
  Journal                  = pnas,
  Year                     = {2002},

  Month                    = {30 } # apr,
  Number                   = {9},
  Pages                    = {5766--5771},
  Volume                   = {99},

  Abstract                 = {The origin of large but rare cascades that are triggered by small initial shocks is a phenomenon that manifests itself as diversely as cultural fads, collective action, the diffusion of norms and innovations, and cascading failures in infrastructure and organizational networks. This paper presents a possible explanation of this phenomenon in terms of a sparse, random network of interacting agents whose decisions are determined by the actions of their neighbors according to a simple threshold rule. Two regimes are identified in which the network is susceptible to very large cascades---herein called global cascades---that occur very rarely. When cascade propagation is limited by the connectivity of the network, a power law distribution of cascade sizes is observed, analogous to the cluster size distribution in standard percolation theory and avalanches in self-organized criticality. But whenthe network is highly connected, cascade propagation is limited instead by the local stability of the nodes themselves, and the size distribution of cascades is bimodal, implying a more extreme kind of instability that is correspondingly harder to anticipate. In the first regime, where the distribution of network neighbors is highly skewed, it is found that the most connected nodes are far more likely than average nodes to trigger cascades, but not in the second regime. Finally, it is shown that heterogeneity plays an ambiguous role in determining a system's stability: increasingly heterogeneous thresholdsmake the system more vulnerable to global cascades; but an increasingly heterogeneous degree distribution makes it less vulnerable.},
  Doi                      = {10.1073/pnas.082090499}
}

@Article{1999:ajs:watts,
  Title                    = {Networks, Dynamics, and the Small-World Phenomenon},
  Author                   = {Duncan J. Watts},
  Journal                  = ajs,
  Year                     = {1999},

  Month                    = sep,
  Number                   = {2},
  Pages                    = {493--527},
  Volume                   = {105},

  Abstract                 = {The small-world phenomenon formalized in this article as the coincidence of high local clustering and short global separation, is shown to be a general feature of sparse, decentralized networks that are neither completely ordered nor completely random. Networks of this kind have received little attention, yet they appear to be widespread in the social and natural sciences, as is indicated here by three distinct examples. Furthermore, small admixtures of randomness to an otherwise ordered network can have a dramatic impact on its dynamical, as well as structural, properties---a feature illustrated by a simple model of disease transmission.},
  Doi                      = {10.1086/210318}
}

@Article{2005:pnas:watts,
  Title                    = {Multiscale, resurgent epidemics in a hierarchical metapopulation model},
  Author                   = {Duncan J. Watts and Roby Muhamad and Daniel C. Medina and Peter S. Dodds},
  Journal                  = pnas,
  Year                     = {2005},

  Month                    = {9 } # aug,
  Number                   = {32},
  Pages                    = {11157--11162},
  Volume                   = {102},

  Abstract                 = {Although population structure has long been recognized as relevant to the spread of infectious disease, traditional mathematical models have understated the role of nonhomogenous mixing in populations with geographical and social structure. Recently, a wide variety of spatial and network models have been proposed that incorporate various aspects of interaction structure among individuals. However, these more complex models necessarily suffer from limited tractability, rendering general conclusions difficult to draw. In seeking a compromise between parsimony and realism, we introduce a class of metapopulation models in which we assume homogeneous mixing holds within local contexts, and that these contexts are embedded in a nested hierarchy of successively larger domains. We model the movement of individuals between contexts via simple transport parameters and allow diseases to spread stochastically. Our model exhibits some important stylized features of real epidemics, including extreme size variation and temporal heterogeneity, that are difficult to characterize with traditional measures. In particular, our results suggest that when epidemics do occur the basic reproduction number $R_0$ may bear little relation to their final size. Informed by our model's behavior, we suggest measures for characterizing epidemic thresholds and discuss implications for the control of epidemics. },
  Doi                      = {10.1073/pnas.0501226102}
}

@Article{1998:nature:watts,
  Title                    = {Collective dynamics of ``small-world'' networks},
  Author                   = {D. J. Watts and S. H. Strogatz},
  Journal                  = nature,
  Year                     = {1998},

  Month                    = {4 } # jun,
  Number                   = {6684},
  Pages                    = {409--410},
  Volume                   = {393},

  Abstract                 = {Networks of coupled dynamical systems have been used to model biological oscillators, Josephson junction arrays, excitable media, neural networks, spatial games, genetic control networks and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks `rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them `small-world' networks, by analogy with the small-world phenomenon (popularly known as six degrees of separation. The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
  Doi                      = {10.1038/30918}
}

@InProceedings{2011:cbr:weber,
  Title                    = {Fast subgraph isomorphism detection for graph-based retrieval},
  Author                   = {Weber, Markus and Langenhan, Christoph and Roth-Berghofer, Thomas and Liwicki, Marcus and Dengel, Andreas and Petzold, Frank},
  Booktitle                = cbr,
  Year                     = {2011},
  Pages                    = {319--333},

  Abstract                 = {In this paper we present a method for a graph-based retrieval and its application in architectural floor plan retrieval. The proposed method is an extension of a well-known method for subgraph matching. This extension significantly reduces the storage amount and indexing time for graphs where the nodes are labeled with a rather small amount of different classes. In order to reduce the number of possible permutations, a weight function for labeled graphs is introduced and a well-founded total order is defined on the weights of the labels. Inversions which violate the order are not allowed. A computational complexity analysis of the new preprocessing is given and its completeness is proven. Furthermore, in a number of practical experiments with randomly generated graphs the improvement of the new approach is shown. In experiments performed on random sample graphs, the number of permutations has been decreased to a fraction of $10^{-18}$ in average compared to the original approach by Messmer. This makes indexing of larger graphs feasible, allowing for fast detection of subgraphs.},
  Doi                      = {10.1007/978-3-642-23291-6_24}
}

@InProceedings{2010:cbr:weber,
  Title                    = {{a.SCatch}: Semantic structure for architectural floor plan retrieval},
  Author                   = {Weber, Markus and Langenhan, Christoph and Roth-Berghofer, Thomas and Liwicki, Marcus and Dengel, Andreas and Petzold, Frank},
  Booktitle                = cbr,
  Year                     = {2010},
  Pages                    = {510--524},

  Abstract                 = {Architects' daily routine involves working with drawings. They use either a pen or a computer to sketch out their ideas or to do a drawing to scale. We therefore propose the use of a sketch-based approach when using the floor plan repository for queries. This enables the user of the system to sketch a schematic abstraction of a floor plan and search for floor plans that are structurally similar. We also propose the use of a visual query language, and a semantic structure as put forward by Langenhan. An algorithm extracts the semantic structure sketched by the architect on DFKI's Touch& Write table and compares the structure of the sketch with that of those from the floor plan repository. The a.SCatch system enables the user to access knowledge from past projects easily. Based on CBR strategies and shape detection technologies, a sketch-based retrieval gives access to a semantic floor plan repository. Furthermore, details of a prototypical application which allows semantic structure to be extracted from image data and put into the repository semi-automatically are provided.},
  Doi                      = {10.1007/978-3-642-14274-1\_37}
}

@InProceedings{2011:gbrpr:weber,
  Title                    = {Indexing with well-founded total order for faster subgraph isomorphism detection},
  Author                   = {Markus Weber and Marcus Liwicki and Andreas Dengel},
  Booktitle                = gbrpr,
  Year                     = {2011},
  Pages                    = {185--194},

  Abstract                 = {In this paper an extension of index-based subgraph matching is proposed. This extension significantly reduces the storage amount and indexing time for graphs where the nodes are labeled with a rather small amount of different classes. In order to reduce the number of possible permutations, a weight function for labeled graphs is introduced and a well-founded total order is defined on the weights of the labels. Inversions which violate the order are not allowed. A computational complexity analysis of the new preprocessing is given and its completeness is proven. Furthermore, in a number of practical experiments with randomly generated graphs the improvement of the new approach is shown. In experiments performed on random sample graphs, the number of permutations has been decreased to a fraction of $10^{-18}$ in average compared to the original approach by Messmer. This makes indexing of larger graphs feasible, allowing for fast detection of subgraphs.},
  Doi                      = {10.1007/978-3-642-20844-7_19}
}

@InProceedings{2003:cbr:weber,
  Title                    = {Predicting Software Development Project Outcomes},
  Author                   = {Rosina Weber and Michael Waller and June Verner and William Evanco},
  Booktitle                = cbr,
  Year                     = {2003},
  Pages                    = {595--609},
  Series                   = lncs,
  Volume                   = {2689},

  Abstract                 = {Case-based reasoning is a flexible methodology to manage software development related tasks. However, when the reasoner's task is prediction, there are a number of different CBR techniques that could be chosen to address the characteristics of a dataset. We examine several of these techniques to assess their accuracy in predicting software development project outcomes (i.e., whether the project is a success or failure) and identify critical success factors within our data. We collected the data from software developers who answered a questionnaire targeting a software development project they had recently worked on. The questionnaire addresses both technical and managerial features of software development projects. The results of these evaluations are compared with results from logistic regression analysis, which serves as a comparative baseline. The research in this paper can guide design decisions in future CBR implementations to predict the outcome of projects described with managerial factors.},
  Doi                      = {10.1007/3-540-45006-8_45}
}

@InProceedings{2006:ase:weissgerber,
  Title                    = {Identifying Refactorings from Source-Code Changes},
  Author                   = {Wei{\ss}gerber, Peter and Diehl, Stephan},
  Booktitle                = ase,
  Year                     = {2006},
  Pages                    = {231--240},

  Abstract                 = {Software has been and is still mostly refactored without tool support. Moreover, as we found in our case studies, programmers tend not to document these changes as refactorings, or even worse label changes as refactorings, although they are not. In this paper we present a technique to detect changes that are likely to be refactorings and rank them according to the likelihood. The evaluation shows that the method has both a high recall and a high precision---it finds most of the refactorings, and most of the found refactoring candidates are really refactorings.},
  Doi                      = {10.1109/ASE.2006.41}
}

@InProceedings{2006:msr:weissgerber:a,
  Title                    = {Are refactorings less error-prone than other changes?},
  Author                   = {Wei{\ss}gerber, Peter and Diehl, Stephan},
  Booktitle                = msrw,
  Year                     = {2006},
  Pages                    = {112--118},

  Abstract                 = {Refactorings are program transformations which should preserve the program behavior. Consequently, we expect that during phases when there are mostly refactorings in the change history of a system, only few new bugs are introduced. For our case study we analyzed the version histories of several open source systems and reconstructed the refactorings performed. Furthermore, we obtained bug reports from various sources depending on the system. Based on this data we identify phases when the above hypothesis holds and those when it doesn't.}
}

@InProceedings{2006:msr:weissgerber:b,
  Title                    = {Mining refactorings in {ArgoUML}},
  Author                   = {Wei{\ss}gerber, Peter and Diehl, Stephan and G{\"o}rg, Carsten},
  Booktitle                = msrw,
  Year                     = {2006},
  Pages                    = {175--176},

  Abstract                 = {In this paper we combine the results of our refactoring reconstruction technique with bug, mail and release information to perform process and bug analyses of the ArgoUML CVS archive.}
}

@InProceedings{2005:oopsla:weissgerber,
  Title                    = {An interactive visualization of refactorings retrieved from software archives},
  Author                   = {Wei{\ss}gerber, Peter and Diehl, Stephan and G{\"o}rg, Carsten},
  Booktitle                = oopslacomp,
  Year                     = {2005},
  Pages                    = {176--177},

  Abstract                 = {We perform knowledge discovery in software archives in order to detect refactorings on the level of classes and methods. Our REFVIS prototype finds these refactorings in CVS repositories and relates them to transactions. Additionally, REFVIS relates movements of methods to the class inheritance hierarchy of the analyzed project. REFVIS creates visualizations that show these refactorings in two different layouts and uses color-coding to distinguish between different kinds of refactorings. Moreover, our visualizations are interactive as they can be zoomed, scrolled and filtered; mouse-over-tooltips allow to examine details of the particular refactoring on demand.}
}

@Article{1991:aic:weide,
  Title                    = {Reusable software components},
  Author                   = {Bruce W. Weide and William F. Ogden and Stuart H. Zweben},
  Journal                  = aic,
  Year                     = {1991},
  Number                   = {2},
  Pages                    = {1--65},
  Volume                   = {33},

  Abstract                 = {The chapter focuses on consolidating important recent technical advances that help in making reusable software components more feasible. It describes a general model of software structure and then uses that model to clarify several key ideas, including software component. The model leads to a natural vision of the likely scope of a mature software-components industry and highlights the technical issues such as software specification, parameterization of behavior, and certification of correctness of component implementations. The chapter emphasizes that the 3C reference model has the potential to become the accepted basis for discourse on reusable software components among members of the reuse community. The 3C model defines and distinguishes three ideas: concept, content, and context. The 3C model of software structure makes no commitment as to whether any component is reused. It also makes no commitment as to the source of the components, i.e., whether they are purchased piece-parts or leftovers from a past company project or developed as custom components for the current project. The model thus provides a framework in which reusable components can be studied but it does not mandate reuse. The chapter describes both general and specific guidelines to direct designers of reusable software components toward superior abstract designs that have efficient implementations. It also discusses the influences of programming-language mechanisms on component reuse and the influences of component reuse on programming-language design.},
  Doi                      = {10.1016/S0065-2458(08)60164-3}
}

@Article{2001:csur:weihe,
  Title                    = {A software engineering perspective on algorithmics},
  Author                   = {Weihe, Karsten},
  Journal                  = csur,
  Year                     = {2001},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {89--134},
  Volume                   = {33},

  Abstract                 = {An algorithm component is an implementation of an algorithm which is not intended to be a stand-alone module, but to perform a specific task within a large software package or even within several distinct software packages. Therefore, the design of algorithm components must also incorporate software-engineering aspects. A key design goal is adaptability. This goal is important for maintenance throughout a project, prototypical development, and reuse in new, unforseen contexts. From a theoretical viewpoint most algorithms apply to a range of possible use scenarios. Ideally, each algorithm is implemented by one algorithm component, which is easily, safely, and efficiently adaptable to all of these contexts. Various techniques have been developed for the design and implementation of algorithm components. However, a common basis for systematic, detailed evaluations and comparisons in view of the real practical needs is still missing. Basically, this means a set of concrete criteria, which specify what sort of adaptability is really required in practice, and which are well-justified by convincing, representative use scenarios. This paper is intended to be a first ``milestone" on the way towards such a system of criteria. We will present a set of concrete goals, which are general and problem-independent and might appear ubiquitously in the algorithmic realm. These goals are illustrated, motivated, and justified by an extensive requirements analysis for a particular algorithm from a particular algorithmic domain: Dijkstra's algorithm for shortest paths in networks. Clearly, the field of algorithmics might be too versatile to allow a comprehensive, yet concise set of precise, justified criteria. Even a domain as restricted as graph and network algorithms includes aspects that are not fully understood. The analysis will include a discussion of the limits of the case study and the scope of the goals. The case study was chosen because it seems to be close to the ``borderline" between the aspects that are well understood and the aspects that are not. Hence, this example may well serve as an ``acid test" for programming techniques in view of the state of the art.},
  Doi                      = {10.1145/375360.375367}
}

@Book{1971:book:weisenberg,
  Title                    = {The Psychology of Computer Programming},
  Author                   = {Gerald M. Weisenberg},
  Publisher                = {Dorset House},
  Year                     = {1971}
}

@Article{1984:tse:weiser,
  Title                    = {Program slicing},
  Author                   = {Mark Weiser},
  Journal                  = tse,
  Year                     = {1984},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {352--357},
  Volume                   = {10},

  Abstract                 = {Program slicing is a method for automatically decomposing programs by analyzing their data flow and control flow. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a ``slice,'' is an independent program guaranteed to represent faithfully the original program within the domain of the specified subset of behavior. Some properties of slices are presented. In particular, finding statement-minimal slices is in general unsolvable, but using data flow analysis is sufficient to find approximate slices. Potential applications include automatic slicing tools for debuggng and parallel processing of slices.},
  Doi                      = {10.1109/TSE.1984.5010248}
}

@InProceedings{2004:icsm:wen,
  Title                    = {Evaluating similarity measures for software decompositions},
  Author                   = {Zhihua Wen and Tzerpos, Vassilios},
  Booktitle                = icsm,
  Year                     = {2004},
  Pages                    = {368--377},

  Abstract                 = {One of the central questions that a similarity measure for software decompositions has to address is whether to consider discrepancies in terms of the nodes of a particular decomposition, or assess similarity based on differences in clustering the edges of the system's dependency graph. We argue that considering nodes or edges in isolation is too one-sided. We outline shortcomings of previous approaches, and introduce the first dissimilarity measure that takes both nodes and edges into account. We also present experiments on real and synthetic data sets that illustrate the differences between various measures.}
}

@InProceedings{2008:msr:wermelinger,
  Title                    = {Analyzing the evolution of {E}clipse plugins},
  Author                   = {Michel Wermelinger and Yijun Yu},
  Booktitle                = msrwc,
  Year                     = {2008},
  Pages                    = {133--136},

  Abstract                 = {Eclipse is a good example of a modern component-based complex system that is designed for long-term evolution, due to its architecture of reusable and extensible components. This paper presents our preliminary results about the evolution of Eclipse's architecture, based on a lightweight and scalable analysis of the metadata in Eclipse's sources. We find that the development of Eclipse follows a systematic process: most architectural changes take place in milestones, and maintenance releases only make exceptional changes to component dependencies. We also found a stable architectural core that remains since the first release.}
}

@InProceedings{2008:icsm:wermelinger,
  Title                    = {Design principles in architectural evolution: A case study},
  Author                   = {Michel Wermelinger and Yijun Yu and Angela Lozano},
  Booktitle                = icsm,
  Year                     = {2008},
  Pages                    = {395--405},

  Abstract                 = {We wish to investigate how structural design principles are used in practice, in order to assess the utility and relevance of such principles to the maintenance of large, complex, long-lived, successful systems. In this paper we take Eclipse as the case study and check whether its architecture follows, throughout multiple releases, some principles proposed in the literature.},
  Doi                      = {0.1109/ICSM.2008.4658088}
}

@Article{2011:ese:wermelinger,
  Title                    = {Assessing architectural evolution: A case study},
  Author                   = {Wermelinger, Michel and Yu, Yijun and Lozano, Angela and Capiluppi, Andrea},
  Journal                  = ese,
  Year                     = {2011},

  Month                    = oct,
  Number                   = {5},
  Pages                    = {623--666},
  Volume                   = {16},

  Abstract                 = {This paper proposes to use a historical perspective on generic laws, principles, and guidelines, like Lehman's software evolution laws and Martin's design principles, in order to achieve a multi-faceted process and structural assessment of a system's architectural evolution. We present a simple structural model with associated historical metrics and visualizations that could form part of an architect's dashboard. We perform such an assessment for the Eclipse SDK, as a case study of a large, complex, and long-lived system for which sustained effective architectural evolution is paramount. The twofold aim of checking generic principles on a well-know system is, on the one hand, to see whether there are certain lessons that could be learned for best practice of architectural evolution, and on the other hand to get more insights about the applicability of such principles. We find that while the Eclipse SDK does follow several of the laws and principles, there are some deviations, and we discuss areas of architectural improvement and limitations of the assessment approach.},
  Doi                      = {10.1007/s10664-011-9164-x}
}

@InProceedings{2012:www:west,
  Title                    = {Human wayfinding in information networks},
  Author                   = {West, Robert and Leskovec, Jure},
  Booktitle                = www,
  Year                     = {2012},
  Pages                    = {619--628},

  Abstract                 = {Navigating information spaces is an essential part of our everyday lives, and in order to design efficient and user-friendly information systems, it is important to understand how humans navigate and find the information they are looking for. We perform a large-scale study of human wayfinding, in which, given a network of links between the concepts of Wikipedia, people play a game of finding a short path from a given start to a given target concept by following hyperlinks. What distinguishes our setup from other studies of human Web-browsing behavior is that in our case people navigate a graph of connections between concepts, and that the exact goal of the navigation is known ahead of time. We study more than 30,000 goal-directed human search paths and identify strategies people use when navigating information spaces. We find that human wayfinding, while mostly very efficient, differs from shortest paths in characteristic ways. Most subjects navigate through high-degree hubs in the early phase, while their search is guided by content features thereafter. We also observe a trade-off between simplicity and efficiency: conceptually simple solutions are more common but tend to be less efficient than more complex ones. Finally, we consider the task of predicting the target a user is trying to reach. We design a model and an efficient learning algorithm. Such predictive models of human wayfinding can be applied in intelligent browsing interfaces.}
}

@InProceedings{2008:wcre:wettel,
  Title                    = {Visual Exploration of Large-Scale System Evolution},
  Author                   = {Wettel, Richard and Lanza, Michele},
  Booktitle                = wcre,
  Year                     = {2008},
  Pages                    = {219--228},

  Abstract                 = {The goal of reverse engineering is to obtain a mental model of software systems. However, evolution adds another dimension to their implicit complexity, effectively making them moving targets: The evolution of software systems still remains an intangible and complex process. Metrics have been extensively used to quantify various facets of evolution, but even the usage of complex metrics often leads to overly simplistic insights, thus failing at adequately characterizing the complex evolutionary processes. We present an approach based on real-time interactive 3D visualizations, whose goal is to render the structural evolution of object-oriented software systems at both a coarse-grained and a fine-grained level. By providing insights into a system's history, our visualizations allow us to reason about the origins and the causalities which led to the current state of a system. We illustrate our approach on three large open-source systems and report on our findings, which were confirmed by developers of the studied systems.},
  Doi                      = {10.1109/WCRE.2008.55}
}

@Article{1998:software:weyuker,
  Title                    = {Testing component-based software: A cautionary tale},
  Author                   = {Elaine J. Weyuker},
  Journal                  = software,
  Year                     = {1998},

  Month                    = sep # {/} # oct,
  Number                   = {5},
  Pages                    = {54--59},
  Volume                   = {15},

  Abstract                 = {Components designed for reuse are expected to lower costs and shorten the development life cycle, but this may not prove so simple. The author emphasizes the need to closely examine a problematic aspect of component reuse: the necessity and potential expense of validating components in their new environments.},
  Doi                      = {10.1109/52.714817}
}

@InProceedings{2001:scam:wheeldon,
  Title                    = {Power law distributions in class relationships},
  Author                   = {Richard Wheeldon and Steve Counsell},
  Booktitle                = scam,
  Year                     = {2001},
  Pages                    = {45--54},

  Abstract                 = {Power law distributions have been found in many natural and social phenomena, and more recently in the source code and run-time characteristics of Object-Oriented (OO) systems. A power law implies that small values are extremely common, whereas large values are extremely rare. We identify twelve new power laws relating to the static graph structures of Java programs. The graph structures analyzed represented different forms of OO coupling, namely, inheritance, aggregation, interface, parameter type and return type. Identification of these new laws provides the basis for predicting likely features of classes in future developments. The research ties together work in object-based coupling and World Wide Web structures.}
}

@InProceedings{2007:www:white,
  Title                    = {Investigating behavioral variability in web search},
  Author                   = {White, Ryen W. and Drucker, Steven M.},
  Booktitle                = www,
  Year                     = {2007},
  Pages                    = {21--30},

  Abstract                 = {Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit. Our findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone.},
  Doi                      = {10.1145/1242572.1242576}
}

@InProceedings{2006:csmr:wierda,
  Title                    = {Using version information in architectural clustering: A case study},
  Author                   = {Wierda, Andreas and Dortmans, Eric and Somers, Lou Lou},
  Booktitle                = csmr,
  Year                     = {2006},
  Pages                    = {214--228},

  Abstract                 = {This paper describes a case study that uses clustering to group classes of an existing object-oriented system of significant size into subsystems. The clustering process is based on the structural relations between the classes: associations, generalizations and dependencies. We experiment with different combinations of relationships and different ways to use this information in the clustering process. The results clearly show that dependency relations are vital to achieve good clusterings. The clustering is performed with a third party tool called Bunch. Compared to other clustering methods the results come relatively close to the result of a manual reconstruction. Performance wise the clustering takes a significant amount of time, but not too much to make it unpractical. In our case study, we base the clustering on information from multiple versions and compare the result to that obtained when basing the clustering on a single version. We experiment with several combinations of versions. If the clustering is based on relations that were present in both the reconstructed and the first version this leads to a significantly better clustering result compared to that obtained when using only information from the reconstructed version},
  Doi                      = {10.1109/CSMR.2006.56}
}

@Article{1945:bb:wilcoxon,
  Title                    = {Individual comparisons by ranking methods},
  Author                   = {Wilcoxon, Frank},
  Journal                  = bb,
  Year                     = {1945},

  Month                    = dec,
  Number                   = {6},
  Pages                    = {80--83},
  Volume                   = {1},

  Abstract                 = {The comparison of two treatments generally falls into one of the following two categories: (a) we may have a number of replications for each of the two treatments, which are unpaired, or (b) we may have a number of paired comparisons leading to a series of differences, some of which may be positive and some negative. The appropriate methods for testing the significance of the differences of the means in these two cases are described in most of the textbooks on statistical methods.
The object of the present paper is to indicate the possibility of using ranking methods, that is, methods in which scores 1, 2, 3, ... $n$ are substituted for the actual numerical data, in order to obtain a rapid approximate idea of the significance of the differences in experiments of this kind.},
  Url                      = {http://www.jstor.org/stable/3001968}
}

@Article{2000:jss:wilkie,
  Title                    = {Coupling measures and change ripples in {C++} application software},
  Author                   = {F. G. Wilkie and B. A. Kitchenham},
  Journal                  = jss,
  Year                     = {2000},

  Month                    = {1 } # jun,
  Number                   = {2--3},
  Pages                    = {157--164},
  Volume                   = {52},

  Abstract                 = {This paper describes an investigation into the effects of class couplings on changes made to a commercial C++ application over a period of 2 1/2 yr. The Chidamber and Kemerer CBO metric is used to measure class couplings within the application and its limitations are identified. Through an in-depth study of the ripple effects of changes to the source code, practical insight into the nature and extent of software couplings is provided.},
  Doi                      = {10.1016/S0164-1212(99)00142-9}
}

@InProceedings{2008:cec:wilkinson,
  Title                    = {Strong regularities in online peer production},
  Author                   = {Wilkinson, Dennis M.},
  Booktitle                = cec,
  Year                     = {2008},
  Pages                    = {302--309},

  Abstract                 = {Online peer production systems have enabled people to coactively create, share, classify, and rate content on an unprecedented scale. This paper describes strong macroscopic regularities in how people contribute to peer production systems, and shows how these regularities arise from simple dynamical rules. First, it is demonstrated that the probability a person stops contributing varies inversely with the number of contributions he has made. This rule leads to a power law distribution for the number of contributions per person in which a small number of very active users make most of the contributions. The rule also implies that the power law exponent is proportional to the effort required to contribute, as justified by the data. Second, the level of activity per topic is shown to follow a lognormal distribution generated by a stochastic reinforcement mechanism. A small number of very popular topics thus accumulate the vast majority of contributions. These trends are demonstrated to hold across hundreds of millions of contributions to four disparate peer production systems of differing scope, interface style, and purpose.},
  Doi                      = {10.1145/1386790.1386837}
}

@Article{2010:ist:williams,
  Title                    = {Characterizing software architecture changes: A systematic review},
  Author                   = {Byron J. Williams and Jeffrey C. Carver},
  Journal                  = ist,
  Year                     = {2010},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {31--51},
  Volume                   = {52},

  Abstract                 = {With today's ever increasing demands on software, software developers must produce software that can be changed without the risk of degrading the software architecture. One way to address software changes is to characterize their causes and effects. A software change characterization mechanism allows developers to characterize the effects of a change using different criteria, e.g. the cause of the change, the type of change that needs to be made, and the part of the system where the change must take place. This information then can be used to illustrate the potential impact of the change. This paper presents a systematic literature review of software architecture change characteristics. The results of this systematic review were used to create the Software Architecture Change Characterization Scheme (SACCS). This report addresses key areas involved in making changes to software architecture. SACCS's purpose is to identify the characteristics of a software change that will have an impact on the high-level software architecture. },
  Doi                      = {10.1016/j.infsof.2009.07.002}
}

@InProceedings{2008:msr:williams,
  Title                    = {Branching and merging in the repository},
  Author                   = {Williams, Chadd C. and Spacco, Jaime W.},
  Booktitle                = msrwc,
  Year                     = {2008},
  Pages                    = {19--22},

  Abstract                 = {Two of the most complex operations version control software allows a user to perform are branching and merging. Branching provides the user the ability to create a copy of the source code to allow changes to be stored in version control but outside of the trunk. Merging provides the user the ability to copy changes from a branch to the trunk. Performing a merge can be a tedious operation and one that may be error prone. In this paper, we compare file revisions found on branches with those found on the trunk to determine when a change that is applied to a branch is moved to the trunk. This will allow us to study how developers use merges and to determine if merges are in fact more error prone than other commits.},
  Doi                      = {10.1145/1370750.1370754}
}

@Article{2008:patrecog:wilson,
  Title                    = {A study of graph spectra for comparing graphs and trees},
  Author                   = {Wilson, Richard C. and Zhu, Ping},
  Journal                  = patrecog,
  Year                     = {2008},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {2833--2841},
  Volume                   = {41},

  Abstract                 = {The spectrum of a graph has been widely used in graph theory to characterise the properties of a graph and extract information from its structure. It has also been employed as a graph representation for pattern matching since it is invariant to the labelling of the graph. There are, however, a number of potential drawbacks in using the spectrum as a representation of a graph. Firstly, more than one graph may share the same spectrum. It is well known, for example, that very few trees can be uniquely specified by their spectrum. Secondly, the spectrum may change dramatically with a small change structure. There are a wide variety of graph matrix representations from which the spectrum can be extracted. Among these are the adjacency matrix, combinatorial Laplacian, normalised Laplacian and unsigned Laplacian. Spectra can also be derived from the heat kernel matrix and path length distribution matrix. The choice of matrix representation clearly has a large effect on the suitability of spectrum in a number of pattern recognition tasks. In this paper we investigate the performance of the spectra as a graph representation in a variety of situations. Firstly, we investigate the cospectrality of the various matrix representations over large graph and tree sets, extending the work of previous authors. We then show that the Euclidean distance between spectra tracks the edit distance between graphs over a wide range of edit costs, and we analyse the accuracy of this relationship. We then use the spectra to both cluster and classify the graphs and demonstrate the effect of the graph matrix formulation on error rates. These results are produced using both synthetic graphs and trees and graphs derived from shape and image data.},
  Doi                      = {10.1016/j.patcog.2008.03.011}
}

@Article{2003:philsci:winsberg,
  Title                    = {Simulated Experiments: Methodology for a Virtual World},
  Author                   = {Eric Winsberg},
  Journal                  = philsci,
  Year                     = {2003},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {105--125},
  Volume                   = {70},

  Abstract                 = {This paper examines the relationship between simulation and experiment. Many discussions of simulation, and indeed the term ``numerical experiments," invoke a strong metaphor of experimentation. On the other hand, many simulations begin as attempts to apply scientific theories. This has lead many to characterize simulation as lying between theory and experiment. The aim of the paper is to try to reconcile these two points of view---to understand what methodological and epistemological features simulation has in common with experimentation, while at the same time keeping a keen eye on simulation's ancestry as a form of scientific theorizing. In so doing, it seeks to apply some of the insights of recent work on the philosophy of experiment to an aspect of theorizing that is of growing philosophical interest: the construction of local models.},
  Doi                      = {10.1086/367872}
}

@Article{1971:cacm:wirth,
  Title                    = {Program development by stepwise refinement},
  Author                   = {Wirth, Niklaus},
  Journal                  = cacm,
  Year                     = {1971},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {221--227},
  Volume                   = {14},

  Abstract                 = {The creative activity of programming---to be distinguished from coding---is usually taught by examples serving to exhibit certain techniques. It is here considered as a sequence of design decisions concerning the decomposition of tasks into subtasks and of data into data structures. The process of successive refinement of specifications is illustrated by a short but nontrivial example, from which a number of conclusions are drawn regarding the art and the instruction of programming.},
  Doi                      = {10.1145/362575.362577}
}

@InProceedings{1996:sigcse:wise,
  Title                    = {{YAP3}: Improved detection of similarities in computer program and other texts},
  Author                   = {Wise, Michael J.},
  Booktitle                = sigcse,
  Year                     = {1996},
  Pages                    = {130--134},

  Abstract                 = {In spite of years of effort, plagiarism in student assignment submissions still causes considerable difficulties for course designers; if students' work is not their own, how can anyone be certain they have learnt anything? YAP is a system for detecting suspected plagiarism in computer programs and other texts submitted by students. The paper reviews YAP3, the third version of YAP, focusing on its novel underlying algorithm---Running-Karp-Rabin Greedy-String-Tiling (or RKS-GST), whose development arose from the observation with YAP and other systems that students shuffle independent code segments. YAP3 is able to detect transposed subsequences, and is less perturbed by spurious additional statements. The paper concludes with a discussion of recent extension of YAP to English texts, further illustrating the flexibility of the YAP approach.},
  Doi                      = {10.1145/236452.236525}
}

@Book{2011:book:witten,
  Title                    = {Data Mining: Practical Machine Learning Tools and Techniques},
  Author                   = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2011},
  Edition                  = {3rd},

  Abstract                 = {Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research.}
}

@Article{1995:software:wong,
  Title                    = {Structural Redocumentation: A Case Study},
  Author                   = {Kenny Wong and Scott R. Tilley and Hausi A. M{\"u}ller and Margaret-Anne D. Storey},
  Journal                  = software,
  Year                     = {1995},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {46--54},
  Volume                   = {12},

  Doi                      = {10.1109/52.363166}
}

@InProceedings{1981:icse:woodfield,
  Title                    = {The effect of modularization and comments on program comprehension},
  Author                   = {Woodfield, S. N. and Dunsmore, H. E. and Shen, V. Y.},
  Booktitle                = icse,
  Year                     = {1981},
  Pages                    = {215--223},

  Abstract                 = {An experiment was conducted to investigate how different types of modularization and comments are related to programmers' ability to understand programs. Forty-eight experienced programmers were given eight different versions of the same program and asked to answer a twenty question quiz used to measure comprehension. These eight different versions were the result of the program being constructed with four types of modularization (monolithic, functional, super, and abstract data type), each with and without comments. Those subjects whose programs contained comments were able to answer more questions than those without comments. Also, those subjects who were given the abstract data type version of the program were able to do significantly better than those with any other type of modularization.}
}

@InProceedings{2005:icsm:wu,
  Title                    = {Comparison of clustering algorithms in the context of software evolution},
  Author                   = {Wu, Jingwei and Hassan, Ahmed E. and Holt, Richard C.},
  Booktitle                = icsm,
  Year                     = {2005},
  Pages                    = {525--535},

  Abstract                 = {To aid software analysis and maintenance tasks, a number of software clustering algorithms have been proposed to automatically partition a software system into meaningful subsystems or clusters. However, it is unknown whether these algorithms produce similar meaningful clusterings for similar versions of a real-life software system under continual change and growth. This paper describes a comparative study of six software clustering algorithms. We applied each of the algorithms to subsequent versions from five large open source systems. We conducted comparisons based on three criteria respectively: stability (Does the clustering change only modestly as the system undergoes modest updating?), authoritative-ness (Does the clustering reasonably approximate the structure an authority provides?) and extremity of cluster distribution (Does the clustering avoid huge clusters and many very small clusters?). Experimental results indicate that the studied algorithms exhibit distinct characteristics. For example, the clusterings from the most stable algorithm bear little similarity to the implemented system structure, while the clusterings from the least stable algorithm has the best cluster distribution. Based on obtained results, we claim that current automatic clustering algorithms need significant improvement to provide continual support for large software projects.},
  Doi                      = {10.1109/ICSM.2005.31}
}

@InProceedings{2011:ese_fse:wu,
  Title                    = {{ReLink}: Recovering links between bugs and changes},
  Author                   = {Wu, Rongxin and Zhang, Hongyu and Kim, Sunghun and Cheung, Shing-Chi},
  Booktitle                = esec_fse,
  Year                     = {2011},
  Pages                    = {15--25},

  Abstract                 = {Software defect information, including links between bugs and committed changes, plays an important role in software maintenance such as measuring quality and predicting defects. Usually, the links are automatically mined from change logs and bug reports using heuristics such as searching for specific keywords and bug IDs in change logs. However, the accuracy of these heuristics depends on the quality of change logs. Bird et al. found that there are many missing links due to the absence of bug references in change logs. They also found that the missing links lead to biased defect information, and it affects defect prediction performance. We manually inspected the explicit links, which have explicit bug IDs in change logs and observed that the links exhibit certain features. Based on our observation, we developed an automatic link recovery algorithm, ReLink, which automatically learns criteria of features from explicit links to recover missing links. We applied ReLink to three open source projects. ReLink reliably identified links with 89\% precision and 78\% recall on average, while the traditional heuristics alone achieve 91\% precision and 64\% recall. We also evaluated the impact of recovered links on software maintainability measurement and defect prediction, and found the results of ReLink yields significantly better accuracy than those of traditional heuristics.},
  Doi                      = {10.1145/2025113.2025120}
}

@InProceedings{1992:usenix:wu,
  Title                    = {Agrep: A fast approximate pattern-matching tool},
  Author                   = {Sun Wu and Udi Manber},
  Booktitle                = usenixwtc,
  Year                     = {1992},
  Pages                    = {153--162},

  Abstract                 = {Searching for a pattern in a text file is a very common operation in many applications ranging from text editors and databases to applications in molecular biology. In many instances the pattern does not appear in the text exactly. Errors in the text or in the query can result from misspelling or from experimental errors (e.g., when the text is a DNA sequence). The use of such approximate pattern matching has been limited until now to specific applications. Most text editors and searching programs do not support searching with errors because of the complexity involved in implementing it. In this paper we describe a new tool, called agrep, for approximate pattern matching. Agrep is based on a new efficient and flexible algorithm for approximate string matching. Agrep is also competitive with other tools for exact string matching; it include many options that make searching more powerful and convenient.}
}

@InProceedings{2010:icse:wu,
  Title                    = {{AURA}: A hybrid approach to identify framework evolution},
  Author                   = {Wu, Wei and Gu{\'e}h{\'e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano and Kim, Miryung},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {325--334},
  Volume                   = {1},

  Abstract                 = {Software frameworks and libraries are indispensable to today's software systems. As they evolve, it is often time-consuming for developers to keep their code up-to-date, so approaches have been proposed to facilitate this. Usually, these approaches cannot automatically identify change rules for one-replaced-by-many and many-replaced-by-one methods, and they trade off recall for higher precision using one or more experimentally-evaluated thresholds. We introduce AURA, a novel hybrid approach that combines call dependency and text similarity analyses to overcome these limitations. We implement it in a Java system and compare it on five frameworks with three previous approaches by Dagenais and Robillard, M. Kim et al., and Sch{\"a}fer et al. The comparison shows that, on average, the recall of AURA is 53.07\% higher while its precision is similar, e.g., 0.10\% lower.},
  Doi                      = {10.1145/1806799.1806848}
}

@InProceedings{2011:icsr:wu,
  Title                    = {Architecture evolution in software product line: An industrial case study},
  Author                   = {Wu, Yijian and Peng, Xin and Zhao, Wenyun},
  Booktitle                = icsr,
  Year                     = {2011},
  Pages                    = {135--150},

  Abstract                 = {A software product line (SPL) usually involves a shared set of core assets and a series of application products. To ensure consistency, the evolution of the core assets and all the application products should be coordinated and synchronized under a unified evolution process. Therefore, SPL evolution often involves cross-product propagation and synchronization besides application derivation based on core assets, presenting quite different characteristic from the evolution of individual software products. As software architectures, including the product line architecture (PLA) and application architectures, play a central role in SPL engineering and evolution, architecture-based evolution analysis is a natural way for analyzing and managing SPL evolution. In this paper, we explore common practices of architecture evolution and the rationale behind in industrial SPL development. To this end, we conduct a case study with Wingsoft examination system product line (WES-PL), an industrial product line with an evolution history of eight years and more than 10 application products. In the case study, we reviewed the evolution history of WES-PL architecture and analyzed several typical evolution cases. Based on the historical analysis, we identify some special problems in industrial SPL practice from the aspect of architecture evolution and summarize some useful experiences about SPL evolution decisions to complement classical SPL methodology. On the other hand, we also propose some possible improvements for the evolution management in WES-PL.},
  Doi                      = {10.1007/978-3-642-21347-2_11}
}

@Article{2009:cviu:xiao,
  Title                    = {A generative model for graph matching and embedding},
  Author                   = {Xiao, Bai and Hancock, Edwin R. and Wilson, Richard C.},
  Journal                  = cviu,
  Year                     = {2009},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {777--789},
  Volume                   = {113},

  Abstract                 = {This paper shows how to construct a generative model for graph structure through the embedding of the nodes of the graph in a vector space. We commence from a sample of graphs where the correspondences between nodes are unknown ab initio. We also work with graphs where there may be structural differences present, i.e. variations in the number of nodes in each graph and their edge structure. We characterise the graphs using the heat-kernel, and this is obtained by exponentiating the Laplacian eigensystem with time. The idea underpinning the method is to embed the nodes of the graphs into a vector space by performing a Young-Householder decomposition of the heat-kernel into an inner product of node co-ordinate matrices. The co-ordinates of the nodes are determined by the eigenvalues and eigenvectors of the Laplacian matrix, together with a time-parameter which can be used to scale the embedding. Node correspondences are located by applying Scott and Longuet-Higgins algorithm to the embedded nodes. We capture variations in graph structure using the covariance matrix for corresponding embedded point positions. We construct a point-distribution model for the embedded node positions using the eigenvalues and eigenvectors of the covariance matrix. We show how to use this model to both project individual graphs into the eigenspace of the point position covariance matrix and how to fit the model to potentially noisy graphs to reconstruct the Laplacian matrix. We illustrate the utility of the resulting method for shape analysis using data from the Caltech-Oxford and COIL databases.},
  Doi                      = {10.1016/j.cviu.2009.01.004}
}

@InProceedings{2005:csmr:xiao,
  Title                    = {Software Clustering Based on Dynamic Dependencies},
  Author                   = {Chenchen Xiao and Tzerpos, Vassilios},
  Booktitle                = csmr,
  Year                     = {2005},
  Pages                    = {124--133},

  Abstract                 = {The reverse engineering literature contains many software clustering approaches that attempt to cluster large software systems based on the static dependencies between software artifacts. However, the usefulness of clustering based on dynamic dependencies has not been investigated. It is possible that dynamic clusterings can provide a fresh outlook on the structure of a large software system. In this paper, we present an approach for the evaluation of dynamic clusterings. We apply this approach to a large open source software system, and present experimental results that suggest that dynamic clusterings have considerable merit.},
  Doi                      = {10.1109/CSMR.2005.49}
}

@InProceedings{2006:ecoop:xie,
  Title                    = {Augmenting automatically generated unit-test suites with regression oracle checking},
  Author                   = {Xie, Tao},
  Booktitle                = ecoop,
  Year                     = {2006},
  Pages                    = {380--403},

  Doi                      = {10.1007/11785477_23}
}

@InProceedings{2006:msr:xie,
  Title                    = {{MAPO}: Mining {API} usages from open source repositories},
  Author                   = {Tao Xie and Jian Pei},
  Booktitle                = msrw,
  Year                     = {2006},
  Pages                    = {54--57},

  Abstract                 = {To improve software productivity, when constructing new software systems, developers often reuse existing class libraries or frameworks by invoking their APIs. Those APIs, however, are often complex and not well documented, posing barriers for developers to use them in new client code. To get familiar with how those APIs are used, developers may search the Web using a general search engine to find relevant documents or code examples. Developers can also use a source code search engine to search open source repositories for source files that use the same APIs. Nevertheless, the number of returned source files is often large. It is difficult for developers to learn API usages from a large number of returned results. In order to help developers understand API usages and write API client code more effectively, we have developed an API usage mining framework and its supporting tool called MAPO (for Mining API usages from Open source repositories). Given a query that describes a method, class, or package for an API, MAPO leverages the existing source code search engines to gather relevant source files and conducts data mining. The mining leads to a short list of frequent API usages for developers to inspect. MAPO currently consists of five components: a code search engine, a source code analyzer, a sequence preprocessor, a frequent sequence miner, and a frequent sequence post processor. We have examined the effectiveness of MAPO using a set of various queries. The preliminary results show that the framework is practical for providing informative and succinct API usage patterns.},
  Doi                      = {10.1145/1137983.1137997}
}

@Article{2007:ijase:xing,
  Title                    = {Differencing logical {UML} models},
  Author                   = {Xing, Zhenchang and Stroulia, Eleni},
  Journal                  = ijase,
  Year                     = {2007},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {215--259},
  Volume                   = {14},

  Abstract                 = {UMLDiff is a heuristic algorithm for automatically detecting the changes that the logical design of an object-oriented software system has gone through, as the subject system evolved from one version to the next. UMLDiff requires as input two models of the logical design of the system, corresponding to two of its versions. It produces as output a set of change facts, reporting the differences between the two logical-design versions in terms of (a) additions, removals, moves, renamings of model elements, i.e., subsystems, packages, classes, interfaces, attributes and operations, (b) changes to their attributes, and (c) changes to the relations among these model elements. In this paper, we detail the underlying metamodel, the UMLDiff algorithm and its heuristics for establishing lexical and structural similarity. We report on our experimental evaluation of the correctness and robustness of UMLDiff through a real-world case study.},
  Doi                      = {10.1007/s10515-007-0007-3}
}

@Article{2007:tse:xing,
  Title                    = {{API}-evolution support with {Diff-CatchUp}},
  Author                   = {Zhengchang Xing and Eleni Stroulia},
  Journal                  = tse,
  Year                     = {2007},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {818--836},
  Volume                   = {33},

  Abstract                 = {Applications built on reusable component frameworks are subject to two independent, and potentially conflicting, evolution processes. The application evolves in response to the specific requirements and desired qualities of the application's stakeholders. On the other hand, the evolution of the component framework is driven by the need to improve the framework functionality and quality while maintaining its generality. Thus, changes to the component framework frequently change its API on which its client applications rely and, as a result, these applications break. To date, there has been some work aimed at supporting the migration of client applications to newer versions of their underlying frameworks, but it usually requires that the framework developers do additional work for that purpose or that the application developers use the same tools as the framework developers. In this paper, we discuss our approach to tackle the API-evolution problem in the context of reuse-based software development, which automatically recognizes the API changes of the reused framework and proposes plausible replacements to the ``obsolete" API based on working examples of the framework code base. This approach has been implemented in the Diff-CatchUp tool. We report on two case studies that we have conducted to evaluate the effectiveness of our approach with its Diff-CatchUp prototype.},
  Doi                      = {10.1109/TSE.2007.70747}
}

@InProceedings{2006:icsm:xing,
  Title                    = {Refactoring Practice: How it is and How it Should be Supported---{A}n {E}clipse Case Study},
  Author                   = {Zhenchang Xing and Eleni Stroulia},
  Booktitle                = icsm,
  Year                     = {2006},
  Pages                    = {458--468},

  Abstract                 = {Refactoring is an important activity in the evolutionary development of object-oriented software systems. Yet, several questions about the practice of refactoring remain unanswered, such as what fraction of code modifications are refactorings and what are the most frequent types of refactorings. To gain some insight in this matter, we conducted a detailed case study on the structural evolution of Eclipse, an integrated-development environment (IDE) and a plugin-based framework. Our study indicates that: 1) about 70\% of structural changes may be due to refactorings; 2) for about 60\% of these changes, the references to the affected entities in a component-based application can be automatically updated by a refactoring-migration tool if the relevant information of refactored components can be gathered through the refactoring engine; and 3) state-of-the-art IDEs, such as Eclipse, support only a subset of commonly applied low-level refactorings and lack support for more complex ones, which are also frequent. Based on our findings, we draw some conclusions on high-level design requirements for a refactoring-based development environment.},
  Doi                      = {10.1109/ICSM.2006.52}
}

@InProceedings{2006:wcre:xing,
  Title                    = {Refactoring Detection based on {UMLDiff} Change-Facts Queries},
  Author                   = {Zhenchang Xing and Eleni Stroulia},
  Booktitle                = wcre,
  Year                     = {2006},
  Pages                    = {263--274},

  Abstract                 = {Refactoring is an important activity in the evolutionary development of object-oriented software systems. Several IDEs today support the automated application of some refactorings; at the same time, there is substantial on-going research aimed at developing support for deciding when and how software should be refactored and for estimating the effect of the refactoring on the quality requirements of the software. On the other hand, understanding the refactorings in the evolutionary history of a software system is essential in understanding its design rationale. Yet, only very limited support exists for detecting refactorings. In this paper, we present our approach for detecting refactorings by analyzing the system evolution at the design level. We evaluate our method with case studies, examining two realistic examples of object-oriented software},
  Doi                      = {10.1109/WCRE.2006.48}
}

@InProceedings{2005:ase:xing,
  Title                    = {{UMLDiff}: An algorithm for object-oriented design differencing},
  Author                   = {Xing, Zhenchang and Stroulia, Eleni},
  Booktitle                = ase,
  Year                     = {2005},
  Pages                    = {54--65},

  Abstract                 = {This paper presents UMLDiff, an algorithm for automatically detecting structural changes between the designs of subsequent versions of object-oriented software. It takes as input two class models of a Java software system, reverse engineered from two corresponding code versions. It produces as output a change tree, i.e., a tree of structural changes, that reports the differences between the two design versions in terms of (a) additions, removals, moves, renamings of packages, classes, interfaces, fields and methods, (b) changes to their attributes, and (c) changes of the dependencies among these entities. UMLDiff produces an accurate report of the design evolution of the software system, and enables subsequent design-evolution analyses from multiple perspectives in support of various evolution activities. UMLDiff and the analyses it enables can assist software engineers in their tasks of understanding the rationale of design evolution of the software system and planning future development and maintenance activities. We evaluate UMLDiff's correctness and robustness through a real-world case study.},
  Doi                      = {10.1145/1101908.1101919}
}

@Article{2005:tse:xing,
  Title                    = {Analyzing the evolutionary history of the logical design of object-oriented software},
  Author                   = {Xing, Zhenchang and Stroulia, Eleni},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {850--868},
  Volume                   = {31},

  Abstract                 = {Today, most object-oriented software systems are developed using an evolutionary process model. Therefore, understanding the phases that the system's logical design has gone through and the style of their evolution can provide valuable insights in support of consistently maintaining and evolving the system, without compromising the integrity and stability of its architecture. In this paper, we present a method for analyzing the evolution of object-oriented software systems from the point of view of their logical design. This method relies on UMLDiff, a UML-structure differencing algorithm, which, given a sequence of UML class models corresponding to the logical design of a sequence of system code releases, produces a sequence of ``change records" that describe the design-level changes between subsequent system releases. This change-records sequence is subsequently analyzed from the perspective of each individual system class, to produce the class-evolution profile, i.e., a class-specific change-records' sequence. Three types of longitudinal analyses---phasic, gamma, and optimal matching analysis---are applied to the class-evolution profiles to recover a high-level abstraction of distinct evolutionary phases and their corresponding styles and to identify class clusters with similar evolution trajectories. The recovered knowledge facilitates the overall understanding of system evolution and the planning of future maintenance activities. We report on one real-world case study evaluating our approach.},
  Doi                      = {10.1109/TSE.2005.106}
}

@InProceedings{2004:etx:xing,
  Title                    = {Design mentoring based on design evolution analysis},
  Author                   = {Xing, Zhenchang and Stroulia, Eleni},
  Booktitle                = etx,
  Year                     = {2004},
  Pages                    = {83--87},

  Abstract                 = {Developing and consistently evolving quality software designs requires both theoretical knowledge and practical skills. The former can be communicated in a classroom; the latter has to be acquired with hands-on experience in software development. Our recent work on design evolution has resulted in a framework for analyzing the structural differences of subsequent versions of design artifacts, such as the logical design of OO software and the user-interface design of interactive applications. In this paper, we discuss how design-evolution analysis can be used to assist developers in their tasks of understanding the design rationale of the system at hand and to advise them on how to consistently maintain and evolve it.},
  Doi                      = {10.1109/ICSE.2005.1553640}
}

@InProceedings{2004:iwpc:xing,
  Title                    = {Understanding class evolution in object-oriented software},
  Author                   = {Zhenchang Xing and Stroulia, Eleni},
  Booktitle                = iwpc,
  Year                     = {2004},
  Pages                    = {34--43},

  Abstract                 = {In the context of object-oriented design, software systems model real-world entities abstractly represented in the system classes. As the system evolves through its lifecycle, its class design also evolves. Thus, understanding class evolution is essential in understanding the current design of the system and the rationale behind its evolution. In this paper, we describe a taxonomy of class-evolution profiles, a method for automatically categorizing a system's classes in one (or more) of eight types in the taxonomy, and a data-mining method for eliciting co-evolution relations among them. These methods rely on our UMLDiff algorithm that, given a sequence of UML class models of a system, surfaces the design-level changes over its lifecycle. The recovered knowledge about class evolution facilitates the overall understanding of the system class-design evolution and the identification of the specific classes that should be investigated in more detail towards improving the system-design qualities. We report on two case studies evaluating our approach.},
  Doi                      = {10.1109/WPC.2004.1311045}
}

@PhdThesis{2006:phd:xu,
  Title                    = {Cascaded refactoring for framework development and evolution},
  Author                   = {Xu, Lugang},
  School                   = {Concordia University},
  Year                     = {2006},

  Address                  = {Montr\'eal, Canada},

  Abstract                 = {This thesis addresses three problems of framework development and evolution: identification and realization of variability, framework evolution, and framework documentation. A solution, called the cascaded refactoring methodology, is proposed. The methodology is validated by a case study, the Know-It-All framework for relational Database Management Systems. The cascaded refactoring methodology views framework development as framework evolution, which consists of framework refactoring followed by framework extension. A framework is specified by a set of models: feature model, use case model, architectural model, design model, and source code. Framework refactoring is achieved by a set of refactorings cascaded from the feature model, to use case model, architectural model, design model, and source code. The constraints of refactorings on a model are derived from the refactorings performed on its previous model. Alignment maps are defined to maintain the traceability amongst the models. The thesis broadens the refactoring concept from the design and source code level to include the feature model, use case model, and architectural model. Metamodels and refactorings are defined for the feature model and architectural model. A document template is proposed to document the framework refactoring.},
  Publisher                = {Concordia University},
  Url                      = {http://spectrum.library.concordia.ca/8775/1/NR16287.pdf}
}

@InProceedings{2010:wcre:xue,
  Title                    = {Understanding Feature Evolution in a Family of Product Variants},
  Author                   = {Yinxing Xue and Zhenchang Xing and Jarzabek, Stan},
  Booktitle                = wcre,
  Year                     = {2010},
  Pages                    = {109--118},

  Abstract                 = {Existing software product variants, developed by ad hoc reuse such as copy-paste-modify, are often a starting point for building Software Product Line (SPL). Understanding of how features evolved in product variants is a prerequisite to transition from ad hoc to systematic SPL reuse. We propose a method that assists analysts in detecting changes to product features during evolution. We first entail that features and their inter-dependencies for each product variant are documented as product feature model. We then apply model differencing algorithm to identify evolutionary changes that occurred to features of different product variants. We evaluate the effectiveness of our approach on a family of medium-size financial systems. We also investigate the scalability of our approach with synthetic data. The evaluation demonstrates that our approach yields good results and scales to large systems. Our approach enables the subsequent variability analysis and consolidation of product variants in the task of reengineering product variants into SPL.},
  Doi                      = {10.1109/WCRE.2010.20}
}

@InProceedings{2005:profes:yamamoto,
  Title                    = {Measuring Similarity of Large Software Systems Based on Source Code Correspondence},
  Author                   = {Yamamoto, Tetsuo and Matsushita, Makoto and Kamiya, Toshihiro and Inoue, Katsuro},
  Booktitle                = profes,
  Year                     = {2005},
  Pages                    = {530--544},

  Abstract                 = {It is an important and intriguing issue to know the quantitative similarity of large software systems. In this paper, a similarity metric between two sets of source code files based on the correspondence of overall source code lines is proposed. A Software similarity MeAsurement Tool SMAT was developed and applied to various versions of an operating system (BSD UNIX). The resulting similarity valuations clearly revealed the evolutionary history characteristics of the BSD UNIX Operating System.},
  Doi                      = {10.1007/11497455_41}
}

@InCollection{2010:book:aggarwal:yan,
  Title                    = {Graph Indexing},
  Author                   = {Xifeng Yan and Jiawei Han},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {5},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {161--180},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {Advanced database systems face a great challenge arising from the emergenceof massive, complex structural data in bioinformatics, chem-informatics, business processes, etc. One of the most important functions needed in these areas is efficient search of complex graph data. Given a graph query, it is desirable to retrieve relevant graphs quickly from a large database via efficient graph indices. This chapter gives an introduction to graph substructure search, approximate substructure search and their related graph indexing techniques, particularly feature-based graph indexing.},
  Doi                      = {10.1007/978-1-4419-6045-0_5}
}

@Article{1991:spe:yang,
  Title                    = {Identifying syntactic differences between two programs},
  Author                   = {Wuu Yang},
  Journal                  = spe,
  Year                     = {1991},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {739--755},
  Volume                   = {21},

  Abstract                 = {Programmers frequently face the need to identify the differences between two programs, usually two different versions of a program. Text-based tools such as the UNIX utility diff often produce unsatisfactory comparisons because they cannot accurately pinpoint the differences and because they sometimes produce irrelevant differences. Since programs have a rigid syntactic structure as described by the grammar of the programming language in which they are written, we develop a comparison algorithm that exploits knowledge of the grammar. The algorithm, which is based on a dynamic programming scheme, can point out the differences between two programs more accurately than previous text comparison tools. Finally, the two programs are pretty-printed `synchronously' with the differences highlighted so that the differences are easily identified.},
  Doi                      = {10.1002/spe.4380210706}
}

@Article{1985:tse:yau,
  Title                    = {Design Stability Measures for Software Maintenance},
  Author                   = {Stephen S. Yau and James S. Collofello},
  Journal                  = tse,
  Year                     = {1985},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {849--856},
  Volume                   = {11},

  Abstract                 = {The high cost of software during its life cycle can be attributed largely to software maintenance activities, and a major portion of these activities is to deal with the modifications of the software. In this paper, design stability measures which indicate the potential ripple effect characteristics due to modifications of the program at the design level are presented. These measures can be generated at any point in the design phase of the software life cycle which enables early maintainability feedback to the software developers. The validation of these measures and future research efforts involvlng the development of a user-oriented maintainability measure, which incorporates the design stability measures as weli as other design measures, are discussed.},
  Doi                      = {10.1109/TSE.1985.232544}
}

@Article{1980:tse:yau,
  Title                    = {Some Stability Measures for Software Maintenance},
  Author                   = {Stephen S. Yau and James S. Collofello},
  Journal                  = tse,
  Year                     = {1980},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {545--552},
  Volume                   = {6},

  Abstract                 = {Software maintenance is the dominant factor contributing to the high cost of software. In this paper, the software maintenance process and the important software quality attributes that affect the maintenance effort are discussed. One of the most important quality attributes of software maintainability is the stability of a program, which indicates the resistance to the potential ripple effect that the program would have when it is modified. Measures for estimating the stability of a program and the modules of which the program is composed are presented, and an algorithm for computing these stability measures is given. An algorithm for normalizing these measures is also given. Applications of these measures during the maintenance phase are discussed along with an example. An indirect validation of these stability measures is also given. Future research efforts involving application of these measures during the design phase, program restructuring based on these measures, and the development of an overall maintainability measure are also discussed.},
  Doi                      = {10.1109/TSE.1980.234503}
}

@InProceedings{2002:icse:ye,
  Title                    = {Supporting Reuse by Delivering Task-Relevant and Personalized Information},
  Author                   = {Yunwen Ye and Gerhard Fischer},
  Booktitle                = icse,
  Year                     = {2002},
  Pages                    = {513--523},

  Abstract                 = {Technical, cognitive, and social factors inhibit the widespread success of systematic software reuse. Our research is primarily concerned with the cognitive and social challenges faced by software developers: how to motivate them to reuse and how to reduce the difficulty of locating components from a large reuse repository. Our research has explored a new interaction style between software developers and reuse repository systems enabled by information delivery mechanisms. Instead of passively waiting for software developers to explore the reuse repository with explicit queries, information delivery autonomously locates and presents components by using the developers' partially written programs as implicit queries.We have designed, implemented, and evaluated a system called CodeBroker, which illustrates different techniques to address the essential challenges in information delivery: to make the delivered information relevant to the task-at-hand and personalized to the background knowledge of an individual developer. Empirical evaluations of CodeBroker show that information delivery is effective in promoting reuse.},
  Doi                      = {10.1145/581339.581402}
}

@InProceedings{2000:fse:ye,
  Title                    = {Integrating Active Information Delivery and Reuse Repository Systems},
  Author                   = {Yunwen Ye and Gerhard Fischer and Brent Reeves},
  Booktitle                = fse,
  Year                     = {2000},
  Pages                    = {60--68},

  Abstract                 = {Although software reuse can improve both the quality and productivity of software development, it will not do so until software developers stop believing that it is not worth their effort to find a component matching their current problem. In addition, if the developers do not anticipate the existence of a given component, they will not even make an effort to find it in the first place. Even the most sophisticated and powerful reuse repositories will not be effective if developers don't anticipate a certain component exists, or don't deem it worthwhile to seek for it. We argue that this crucial barrier to reuse is overcome by integrating active information delivery, which presents information without explicit queries from the user, and reuse repository systems. A prototype system, CodeBroker, illustrates this integration and raises several issues related to software reuse.},
  Doi                      = {10.1145/355045.355053}
}

@InProceedings{2007:pppj:ye,
  Title                    = {Searching the Library and Asking the Peers: Learning to Use {J}ava {API}s on Demand},
  Author                   = {Yunwen Ye and Yasuhiro Yamamoto and Kumiyo Nakakoji and Yoshiyuki Nishinaka and Mitsuhiro Asada},
  Booktitle                = pppj,
  Year                     = {2007},
  Pages                    = {41--50},

  Abstract                 = {The existence of large API libraries contributes significantly to the programming productivity and quality of Java programmers. The vast number of available library APIs, however, presents a learning challenge for Java programmers. Most Java programmers do not know all the APIs. Whenever their programming task requires API methods they do not yet know, they have to be able to find what they need and learn how to use them on demand. This paper describes a tool called STeP\_IN\_Java (a Socio-Technical Platform for In situ Networking of Java programmers) that helps Java programmers find APIs, and learn from both examples and experts how to use them on demand. STeP\_IN\_Java features a sophisticated yet easy-to-use search interface that enables programmers to conduct a personalized search and to progressively refine their search by limiting search scopes. Example programs are provided and embedded to assist programmers in using APIs. Furthermore, if a programmer still has questions about a particular API method, he or she can ask peer programmers. The STeP\_IN\_Java system automatically routes the question to a group of experts who are chosen based on two criteria: they have high expertise on the particular API method and they have a good social relationship with the programmer who is requesting the information.},
  Doi                      = {10.1145/1294325.1294332}
}

@Article{1997:toplas:yellin,
  Title                    = {Protocol specifications and component adaptors},
  Author                   = {Daniel M. Yellin and Robert E. Strom},
  Journal                  = toplas,
  Year                     = {1997},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {292--333},
  Volume                   = {19},

  Abstract                 = {In this article we examine the augmentation of application interfaces with enhanced specifications that include sequencing constraints called protocols. Protocols make explicit the relationship between messages (methods) supported by the application. These relationships are usually only given implicitly, either in the code or in textual comments. We define notions of interface compatibility based upon protocols and show how compatibility can be checked, discovering a class of errors that cannot be discovered via the type system alone. We then define software adaptors that can be used to bridge the difference between applications that have functionally compatible but type- and protocol-incompatible interfaces. We discuss what it means for an adaptor to be well formed. Leveraging the information provided by protocols, we show how adaptors can be automatically generated from a high-level description, called an interface mapping.},
  Doi                      = {10.1145/244795.244801}
}

@InProceedings{2011:fse:yin,
  Title                    = {How Do Fixes Become Bugs?},
  Author                   = {Yin, Zuoning and Yuan, Ding and Zhou, Yuanyuan and Pasupathy, Shankar and Bairavasundaram, Lakshmi},
  Booktitle                = esec_fse,
  Year                     = {2011},
  Pages                    = {26--36},

  Doi                      = {10.1145/2025113.2025121}
}

@Article{2004:tse:ying,
  Title                    = {Predicting Source Code Changes by Mining Change History},
  Author                   = {Ying, Annie T. T. and Murphy, Gail C. and Ng, Raymond and Chu-Carroll, Mark C.},
  Journal                  = tse,
  Year                     = {2004},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {574--586},
  Volume                   = {30},

  Abstract                 = {Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns---sets of files that were changed together frequently in the past---from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.},
  Doi                      = {10.1109/TSE.2004.52}
}

@InProceedings{2011:aosd:yokomori,
  Title                    = {Measuring the effects of aspect-oriented refactoring on component relationships: Two case studies},
  Author                   = {Yokomori, Reishi and Siy, Harvey and Yoshida, Norihiro and Noro, Masami and Inoue, Katsuro},
  Booktitle                = aosd,
  Year                     = {2011},
  Pages                    = {215--226},

  Abstract                 = {Aspect-oriented refactoring is a promising technique for improving modularity and reducing complexity of existing software systems through encapsulating crosscutting concerns. As complexity of a system is often linked to the degree to which its components (e.g., classes and aspects) are connected, we investigate in this paper the impact of such refactoring activities on component relationships. We analyze two aspect-refactoring projects to determine circumstances when such activities are effective at reducing component relationships and when they are not. We measure two kinds of relationships between components, use and clone relations. We compare how these metrics changed between the original and the refactored system. Our findings indicate that aspect-oriented refactoring is successful in improving the modularity and complexity of the base code. However, we obtain mixed results when aspects are accounted for. Based on these results, we also discuss constraints to the technology as well as other design considerations that may limit the effectiveness of aspect-oriented refactoring on actual systems.},
  Doi                      = {10.1145/1960275.1960301}
}

@Article{1994:toplas:yu,
  Title                    = {A linear-time scheme for version reconstruction},
  Author                   = {Yu, Lin and Rosenkrantz, Daniel J.},
  Journal                  = toplas,
  Year                     = {1994},

  Month                    = may,
  Number                   = {3},
  Pages                    = {775--797},
  Volume                   = {16},

  Abstract                 = {An efficient scheme to store and reconstruct versions of sequential files is presented. The reconstruction scheme involves building a data structure representing a complete version, and then successively modifying this data structure by applying a sequence of specially formatted differential files to it. Each application of a differential file produces a representation of an intermediate version, with the final data structure representing the requested version. The scheme uses a linked list to represent an intermediate version, instead of a sequential array, as is used traditionally. A new format for differential files specifying changes to this linked list data structure is presented. The specification of each change points directly to where the change is to take place, thereby obviating a search. Algorithms are presented for using such a new format differential file to transform the representation of a version, and for reconstructing a requested version. Algorithms are also presented for generating the new format differential files, both for the case of a forward differential specifying how to transform the representation of an old version to the representation of a new version, and for the case of a reverse differential specifying how to transform the representation of a new version to the representation of an old version. The new version reconstruction scheme takes time linear in the sum of the size of the initial complete version and the sizes of the file differences involved in reconstructing the requested version. In contrast, the classical scheme for reconstructing versions takes time proportional to the sum of the sizes of the sequence of versions involved in the reconstruction, and therefore has a worst-case time that is quadratic in the sum of the size of the initial complete version and the sizes of the file differences. The time cost of the new differential file generation scheme is comparable to the time cost of the classical differential file generation scheme.},
  Doi                      = {10.1145/177492.177705}
}

@InProceedings{2010:adma:yu,
  Title                    = {Predicting defect priority based on neural networks},
  Author                   = {Yu, Lian and Tsai, Wei-Tek and Zhao, Wei and Wu, Fang},
  Booktitle                = adma,
  Year                     = {2010},
  Number                   = {Part II},
  Pages                    = {356--367},
  Publisher                = {Springer},
  Series                   = lncs,
  Volume                   = {6441},

  Abstract                 = {Existing defect management tools provide little information on how important/urgent for developers to fix defects reported. Manually prioritizing defects is time-consuming and inconsistent among different people. To improve the efficiency of troubleshooting, the paper proposes to employ neural network techniques to predict the priorities of defects, adopt evolutionary training process to solve error problems associated with new features, and reuse data sets from similar software systems to speed up the convergence of training. A framework is built up for the model evaluation, and a series of experiments on five different software products of an international healthcare company to demonstrate the feasibility and effectiveness.},
  Doi                      = {10.1007/978-3-642-17313-4_35}
}

@InProceedings{2011:ase:yu,
  Title                    = {Specifying and Detecting Meaningful Changes in Programs},
  Author                   = {Yijun Yu and Thein Than Tun and Bashar Nuseibeh},
  Booktitle                = ase,
  Year                     = {2011},
  Pages                    = {273--282},

  Abstract                 = {Software developers are often interested in particular changes in programs that are relevant to their current tasks: not all changes to evolving software are equally important. However, most existing differencing tools, such as diff, notify developers of more changes than they wish to see. In this paper, we propose a technique to specify and automatically detect only those changes in programs deemed meaningful, or relevant, to a particular development task. Using four elementary annotations on the grammar of any programming language, namely Ignore, Order, Prefer and Scope, developers can specify, with limited effort, the type of change they wish to detect. Our algorithms use these annotations to transform the input programs into a normalised form, and to remove clones across different normalised programs in order to detect non-trivial and relevant differences. We evaluate our tool on a benchmark of programs to demonstrate its improved precision compared to other differencing approaches.},
  Doi                      = {10.1109/ASE.2011.6100063}
}

@InProceedings{2004:msr:yusof,
  Title                    = {Template Mining in Source-Code Digital Libraries},
  Author                   = {Yuhanis Yusof and Omer F. Rana},
  Booktitle                = msrw,
  Year                     = {2004},
  Pages                    = {122--126},

  Abstract                 = {As a greater number of software developers make their source code available, there is a need to store such opensource applications into a repository, and facilitate search over the repository. The objective of this research is to build a digital library of Java source code, to enable search and selection of source code. We believe that such a digital library will enable better sharing of experience amongst developers, and facilitate reuse of code segments. Information retrieval is often considered to be essential for the success of digital libraries, so they can achieve high level of effectiveness while at the same time affording ease of use to a diverse community of users. Four different matching mechanism: exact, generalization, reduction and nameOnly is used in retrieving Java programs based from information extracted through template mining.},
  Url                      = {http://msr.uwaterloo.ca/msr2004/MSR2004ProceedingsFINAL_IEE_Acrobat4.pdf}
}

@Article{2008:jsmerp:zaidman,
  Title                    = {Automatic identification of key classes in a software system using webmining techniques},
  Author                   = {Zaidman, Andy and Demeyer, Serge},
  Journal                  = jsmerp,
  Year                     = {2008},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {387--417},
  Volume                   = {20},

  Abstract                 = {Software engineers new to a project are often stuck sorting through hundreds of classes in order to find those few classes that offer a significant insight into the inner workings of the software project. To help stimulate this process, we propose a technique that can identify the most important classes in a system or the key classes of that system. Software engineers can use these classes to focus their understanding efforts when starting to work on a new software project. Those key classes are typically characterized with having a lot of lsquocontrolrsquo within the application. In order to find these controlling classes, we present a detection approach that is based on dynamic coupling and webmining. We demonstrate the potential of our technique using two open-source software systems that have a rich documentation set. During the case studies we use dynamically gathered coupling information that vary between a number of coupling metrics. The case studies show that we are able to retrieve 90\% of the classes deemed important by the original maintainers of the systems, while maintaining a level of precision of around 50\%.},
  Doi                      = {10.1002/smr.v20:6}
}

@InProceedings{2008:stvv:zaidman,
  Title                    = {Mining software repositories to study co-evolution of production \& test code},
  Author                   = {Zaidman, Andy and Van Rompaey, Bart and Demeyer, Serge and van Deursen, Arie},
  Booktitle                = stvv,
  Year                     = {2008},
  Pages                    = {220--229},

  Doi                      = {10.1109/ICST.2008.47}
}

@InProceedings{1993:fse:zaremski,
  Title                    = {Signature Matching: A Key to Reuse},
  Author                   = {Zaremski, Amy Moormann and Wing, Jeannette M.},
  Booktitle                = fse,
  Year                     = {1993},
  Pages                    = {182--190},

  Abstract                 = {Software reuse is only effective if it is easier to locate (and appropriately modify) a reusable component than to write it from scratch. We present signature matching as a method for achieving this goal by using signature information easily derived from the component. We consider two kinds of software components, functions and modules, and hence two kinds of matching, function mathcing and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match, but also various flavors of relaxed match. We briefly describe an experimental facility written in Standard ML for performing signature matching over a library of ML functions.},
  Doi                      = {10.1145/167049.167077}
}

@Article{2009:tpami:zaslavskiy,
  Title                    = {A Path Following Algorithm for the Graph Matching Problem},
  Author                   = {Zaslavskiy, Mikhail and Bach, Francis and Vert, Jean-Philippe},
  Journal                  = tpami,
  Year                     = {2009},

  Month                    = dec,
  Number                   = {12},
  Pages                    = {2227--2242},
  Volume                   = {31},

  Abstract                 = {We propose a convex-concave programming approach for the labeled weighted graph matching problem. The convex-concave programming formulation is obtained by rewriting the weighted graph matching problem as a least-square problem on the set of permutation matrices and relaxing it to two different optimization problems: a quadratic convex and a quadratic concave optimization problem on the set of doubly stochastic matrices. The concave relaxation has the same global minimum as the initial graph matching problem, but the search for its global minimum is also a hard combinatorial problem. We, therefore, construct an approximation of the concave problem solution by following a solution path of a convex-concave problem obtained by linear interpolation of the convex and concave formulations, starting from the convex relaxation. This method allows to easily integrate the information on graph label similarities into the optimization problem, and therefore, perform labeled weighted graph matching. The algorithm is compared with some of the best performing graph matching methods on four data sets: simulated graphs, QAPLib, retina vessel images, and handwritten Chinese characters. In all cases, the results are competitive with the state of the art.},
  Doi                      = {10.1109/TPAMI.2008.245}
}

@InCollection{2003:book:juristo:zelkowitz,
  Title                    = {Experimental validation of new software technology},
  Author                   = {Zelkowitz, Marvin V. and Wallace, Dolores R. and Binkley, David W.},
  Booktitle                = {Lecture Notes on Empirical Software Engineering},
  Publisher                = {World Scientific},
  Year                     = {2003},
  Editor                   = {Juristo, Natalia and Moreno, Ana M.},
  Pages                    = {229--263},

  Abstract                 = {When to apply a new technology in an organization is a critical decision for every software development organization. Earlier work defines a set of methods that the research community uses when a new technology is developed. This chapter presents a discussion of the set of methods that industrial organizations use before adopting a new technology. First there is a brief definition of the earlier research methods and then a definition of the set of industrial methods. A survey taken by experts from both the research and industrial communities provides insights into how these communities differ in their approach toward technology innovation and technology transfer.},
  Url                      = {http://dl.acm.org/citation.cfm?id=899834.899841}
}

@Article{2007:entcs:zeller,
  Title                    = {Where do bugs come from?},
  Author                   = {Andreas Zeller},
  Journal                  = entcs,
  Year                     = {2007},

  Month                    = {30 } # may,
  Number                   = {4},
  Pages                    = {55--59},
  Volume                   = {174},

  Abstract                 = {The analysis of bug databases reveals that some software components are far more failure-prone than others. Yet it is hard to find properties that are universally shared by failure-prone components. We have mined the Eclipse bug and version databases to map failures to Eclipse components. The resulting data set lists the defect density of all Eclipse components, and may thus help to find features that predict how defect-prone a component will be.},
  Doi                      = {10.1016/j.entcs.2006.12.029}
}

@InProceedings{2007:future:zeller,
  Title                    = {The Future of Programming Environments: Integration, Synergy, and Assistance},
  Author                   = {Zeller, Andreas},
  Booktitle                = fose,
  Year                     = {2007},
  Pages                    = {316--325},

  Abstract                 = {Modern programming environments foster the integration of automated, extensible, and reusable tools. New tools can thus leverage the available functionality and collect data from program and process. The synergy of both will allow the automation of current empirical approaches. This leads to automated assistance in all development decisions for programmers and managers alike: ``For this task, you should collaborate with Joe, because it will likely require risky work on the Mailbox class."},
  Doi                      = {10.1109/FOSE.2007.31}
}

@InProceedings{2003:aosd:zhang,
  Title                    = {Quantifying Aspects in Middleware Platforms},
  Author                   = {Charles Zhang and Hans-Arno Jacobsen},
  Booktitle                = aosd,
  Year                     = {2003},
  Pages                    = {130--139},

  Abstract                 = {Middleware technologies such as Web Services, CORBA and DCOM have been very successful in solving distributed computing problems for a large family of application domains. As middleware systems are getting widely adopted and more functionally mature, it is also increasingly difficult for the architecture of middleware to achieve a high level of adaptability and configurability, due to the limitations of traditional software decomposition methods. Aspect oriented programming has brought us new design perspectives because it permits the superimpositions of multiple abstraction models on top of one another. It is a very powerful technique in separating and simplifying design concerns. In this paper, we first show that, through the quantification of aspects in the legacy implementations, the modularity of middleware architecture is greatly hindered by the ubiquitous existence of tangled logic. We then go one step further by factoring out a number of aspects identified in the mining work and re-implementing them as aspect programs. The aspect oriented re-factorization allows us to apply a set of software engineering metrics to quantify the changes of the re-factored system in both the structural complexity and the runtime performance. The aspect oriented re-factoring proves that the aspect oriented programming is capable of composing orthogonal design requirements. The final ``woven" system is able to correctly provide both the fundamental functionality and the ``aspectized" functionality with negligible overhead and a leaner architecture. Further more, the configurability of middleware is dramatically increased because the ``aspectized" features can be configured in and out during the compile-time},
  Doi                      = {10.1145/643603.643617}
}

@InProceedings{2012:icse:zhang,
  Title                    = {Automatic parameter recommendation for practical {API} usage},
  Author                   = {Cheng Zhang and Juyuan Yang and Yi Zhang and Jing Fan and Xin Zhang and Jianjun Zhao and Peizhao Ou},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {826--836},

  Abstract                 = {Programmers extensively use application programming interfaces (APIs) to leverage existing libraries and frameworks. However, correctly and efficiently choosing and using APIs from unfamiliar libraries and frameworks is still a non-trivial task. Programmers often need to ruminate on API documentations (that are often incomplete) or inspect code examples (that are often absent) to learn API usage patterns. Recently, various techniques have been proposed to alleviate this problem by creating API summarizations, mining code examples, or showing common API call sequences. However, few techniques focus on recommending API parameters. In this paper, we propose an automated technique, called Precise, to address this problem. Differing from common code completion systems, Precise mines existing code bases, uses an abstract usage instance representation for each API usage example, and then builds a parameter usage database. Upon a request, Precise queries the database for abstract usage instances in similar contexts and generates parameter candidates by concretizing the instances adaptively. The experimental results show that our technique is more general and applicable than existing code completion systems, specially, 64\% of the parameter recommendations are useful and 53\% of the recommendations are exactly the same as the actual parameters needed. We have also performed a user study to show our technique is useful in practice.},
  Doi                      = {10.1109/ICSE.2012.6227136}
}

@Article{2009:ipm:zhang,
  Title                    = {Discovering power laws in computer programs},
  Author                   = {Zhang, Hongyu},
  Journal                  = ipm,
  Year                     = {2009},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {477--483},
  Volume                   = {45},

  Abstract                 = {The power-law regularities have been discovered behind many complex natural and social phenomenons. We discover that the power-law regularities, especially the Zipf's and Heaps' laws, also exist in large-scale software systems. We find that the distribution of lexical tokens in modern Java, C++ and C programs follows Zipf-Mandelbrot law, and the growth of program vocabulary follows Heaps' law. The results are obtained through empirical analysis of real-world software systems. We believe our discovery reveals the statistical regularities behind computer programming.},
  Doi                      = {10.1016/j.ipm.2009.02.001}
}

@InProceedings{2013:icse:zhang,
  Title                    = {Predicting Bug-fixing Time: An Empirical Study of Commercial Software Projects},
  Author                   = {Zhang, Hongyu and Gong, Liang and Versteeg, Steve},
  Booktitle                = icse,
  Year                     = {2013},
  Pages                    = {1042--1051},

  Url                      = {http://dl.acm.org/citation.cfm?id=2486788.2486931}
}

@InCollection{2010:book:aggarwal:zhang,
  Title                    = {A Survey on Streaming Algorithms for Massive Graphs},
  Author                   = {Jian Zhang},
  Booktitle                = {Managing and Mining Graph Data},
  Publisher                = {Springer},
  Year                     = {2010},
  Chapter                  = {13},
  Editor                   = {Charu C. Aggarwal and Haixun Wang},
  Pages                    = {393--420},
  Series                   = ads,
  Volume                   = {40},

  Abstract                 = {Streaming is an important paradigm for handling massive graphs that are too large to fit in the main memory. In the streaming computational model, algorithms are restricted to use much less space than they would need to store the input. Furthermore, the input is accessed in a sequential fashion, therefore, can be viewed as a stream of data elements. The restriction limits the model and yet, algorithms exist for many graph problems in the streaming model. We survey a set of algorithms that compute graph statistics, matching and distance in a graph, and random walks. These are basic graph problems and the algorithms that compute them may be used as building blocks in graph-data management and mining.},
  Doi                      = {10.1007/978-1-4419-6045-0_13}
}

@InProceedings{2011:issta:zhang,
  Title                    = {Combined static and dynamic automated test generation},
  Author                   = {Zhang, Sai and Saff, David and Bu, Yingyi and Ernst, Michael D.},
  Booktitle                = issta,
  Year                     = {2011},
  Pages                    = {353--363},

  Abstract                 = {In an object-oriented program, a unit test often consists of a sequence of method calls that create and mutate objects, then use them as arguments to a method under test. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible. This paper proposes a combined static and dynamic automated test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests. Our Palus tool implements this testing approach. We compared its effectiveness with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on several popular open-source Java programs. Tests generated by Palus achieved higher structural coverage and found more bugs. Palus is also internally used in Google. It has found 22 previously-unknown bugs in four well-tested Google products.},
  Doi                      = {10.1145/2001420.2001463}
}

@InProceedings{2005:esec_fse:zhang,
  Title                    = {Matching execution histories of program versions},
  Author                   = {Zhang, Xiangyu and Gupta, Rajiv},
  Booktitle                = esec_fse,
  Year                     = {2005},
  Pages                    = {197--206},

  Abstract                 = {We develop a method for matching dynamic histories of program executions of two program versions. The matches produced can be useful in many applications including software piracy detection and several debugging scenarios. Unlike some static approaches for matching program versions, our approach does not require access to source code of the two program versions because dynamic histories can be collected by running instrumented versions of program binaries. We base our matching algorithm on comparison of rich program execution histories which include: control flow taken, values produced, addresses referenced, as well as data dependences exercised. In developing a matching algorithm we had two goals: producing an accurate match and producing it quickly. By using rich execution history, we are able to compare the program versions across many behavioral dimensions. The result is a fast and highly precise matching algorithm. Our algorithm first uses individual histories of instructions to identify multiple potential matches and then it refines the set of matches by matching the data dependence structure established by the matching instructions. To test our algorithm we attempted matching of execution histories of unoptimized and optimized program versions. Our results show that our algorithm produces highly accurate matches which are highly effective when used in comparison checking approach to debugging optimized code.},
  Doi                      = {10.1145/1081706.1081738}
}

@InProceedings{2005:asian:zhang,
  Title                    = {Multi-labeled Graph Matching: An Algorithm Model for Schema Matching},
  Author                   = {Zhi Zhang and Haoyang Che and Pengfei Shi and Yong Sun and Jun Gu},
  Booktitle                = asian,
  Year                     = {2005},
  Pages                    = {90--103},
  Series                   = lncs,
  Volume                   = {3818},

  Abstract                 = {Schema matching is the task of finding semantic correspondences between elements of two schemas, which plays a key role in many database applications. In this paper, we treat the schema matching problem as a combinatorial problem. First, we propose an internal schema model, i.e., the multi-labeled graph, and transform schemas into multi-labeled graphs. Secondly, we discuss a generic graph similarity measure, and propose an optimization function based on multi-labeled graph similarity. Then, we cast schema matching problem into a multi-labeled graph matching problem, which is a classic combinational problem. Finally, we implement a greedy algorithm to find the feasible matching results.},
  Doi                      = {10.1007/11596370_9}
}

@Article{2003:jss:zhao,
  Title                    = {Quality assurance under the open source development model},
  Author                   = {Zhao, Luyin and Elbaum, Sebastian},
  Journal                  = jss,
  Year                     = {2003},

  Month                    = apr,
  Number                   = {1},
  Pages                    = {65--75},
  Volume                   = {66},

  Doi                      = {10.1016/S0164-1212(02)00064-X}
}

@Article{2000:sen:zhao,
  Title                    = {A survey on quality related activities in open source},
  Author                   = {Zhao, Luyin and Elbaum, Sebastian},
  Journal                  = sen,
  Year                     = {2000},

  Month                    = may,
  Number                   = {3},
  Pages                    = {54--57},
  Volume                   = {25},

  Doi                      = {10.1145/505863.505878}
}

@InProceedings{2006::zheng,
  Title                    = {A Lightweight Process for Change Identification and Regression Test Selection in Using {COTS} Components},
  Author                   = {Zheng, Jiang and Robinson, Brian and Williams, Laurie and Smiley, Karen},
  Booktitle                = iccbss,
  Year                     = {2006},
  Pages                    = {137--143},

  Abstract                 = {Various regression test selection techniques have been developed and have shown fault detection effectiveness. The majority of these test selection techniques rely on access to source code for change identification. However, when new releases of COTS components are made available for integration and testing, source code is often not available. In this paper we present a lightweight Integrated - Black-box Approach for Component Change Identification (IBACCI) process for selection of regression tests for user/glue code that uses COTS components. I-BACCI is applicable when component licensing agreements do not preclude analysis of the binary files. A case study of the process was conducted on an ABB product that uses a medium-scale internal ABB software component. Five releases of the component were examined to evaluate the efficacy of the proposed process. The result of the case study indicates this process can reduce the required number of regression tests by 54\% on average.},
  Doi                      = {10.1109/ICCBSS.2006.1}
}

@InProceedings{2010:icse:zhong,
  Title                    = {Mining {API} mapping for language migration},
  Author                   = {Hao Zhong and Thummalapenta, Suresh and Xie, Tao and Lu Zhang and Qing Wang},
  Booktitle                = icse,
  Year                     = {2010},
  Pages                    = {195--204},

  Abstract                 = {To address business requirements and to survive in competing markets, companies or open source organizations often have to release different versions of their projects in different languages. Manually migrating projects from one language to another (such as from Java to C\#) is a tedious and error-prone task. To reduce manual effort or human errors, tools can be developed for automatic migration of projects from one language to another. However, these tools require the knowledge of how Application Programming Interfaces (APIs) of one language are mapped to APIs of the other language, referred to as API mapping relations. In this paper, we propose a novel approach, called MAM (Mining API Mapping), that mines API mapping relations from one language to another using API client code. MAM accepts a set of projects each with two versions in two languages and mines API mapping relations between those two languages based on how APIs are used by the two versions. These mined API mapping relations assist in migration of projects from one language to another. We implemented a tool and conducted two evaluations to show the effectiveness of MAM. The results show that our tool mines 25,805 unique mapping relations of APIs between Java and C\# with more than 80% accuracy. The results also show that mined API mapping relations help reduce 54.4\% compilation errors and 43.0\% defects during migration of projects with an existing migration tool, called Java2CSharp. The reduction in compilation errors and defects is due to our new mined mapping relations that are not available with the existing migration tool.},
  Doi                      = {10.1145/1806799.1806831}
}

@InProceedings{2012:icse:zhou,
  Title                    = {Where Should the Bugs Be Fixed?: More Accurate Information Retrieval-Based Bug Localization Based on Bug Reports},
  Author                   = {Jian Zhou and Hongyu Zhang and David Lo},
  Booktitle                = icse,
  Year                     = {2012},
  Pages                    = {14--24},

  Abstract                 = {For a large and evolving software system, the project team could receive a large number of bug reports. Locating the source code files that need to be changed in order to fix the bugs is a challenging task. Once a bug report is received, it is desirable to automatically point out to the files that developers should change in order to fix the bug. In this paper, we propose BugLocator, an information retrieval based method for locating the relevant files for fixing a bug. BugLocator ranks all files based on the textual similarity between the initial bug report and the source code using a revised Vector Space Model (rVSM), taking into consideration information about similar bugs that have been fixed before. We perform large-scale experiments on four open source projects to localize more than 3,000 bugs. The results show that BugLocator can effectively locate the files where the bugs should be fixed. For example, relevant buggy files for 62.60\% Eclipse 3.1 bugs are ranked in the top ten among 12,863 files. Our experiments also show that BugLocator outperforms existing state-of-the-art bug localization methods.},
  Doi                      = {10.1109/ICSE.2012.6227210}
}

@Article{2004:sen:zhou,
  Title                    = {A comparative study of graph theory-based class cohesion measures},
  Author                   = {Yuming Zhou and Jiangtao Lu and Hongmin Lu and Baowen Xu},
  Journal                  = sen,
  Year                     = {2004},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {13:1--13:6},
  Volume                   = {29},

  Abstract                 = {Among a large number of cohesion measures for classes proposed in last decade, many measures abstract a class by an undirected or directed graph, in which the nodes represent the class members and the edges represent the relationships among these members. This paper compares six typical graph theory-based cohesion measures for classes, and states what problems should be ad-dressed during the development of new cohesion measures.},
  Doi                      = {10.1145/979743.979767}
}

@Article{2011:infosyst:zhu,
  Title                    = {Structure and attribute index for approximate graph matching in large graphs},
  Author                   = {Zhu, Linhong and Keong Ng, Wee and Cheng, James},
  Journal                  = infosyst,
  Year                     = {2011},

  Month                    = sep,
  Number                   = {6},
  Pages                    = {958--972},
  Volume                   = {36},

  Abstract                 = {The increasing popularity of graph data in various domains has lead to a renewed interest in developing efficient graph matching techniques, especially for processing large graphs. In this paper, we study the problem of approximate graph matching in a large attributed graph. Given a large attributed graph and a query graph, we compute a subgraph of the large graph that best matches the query graph. We propose a novel structure-aware and attribute-aware index to process approximate graph matching in a large attributed graph. We first construct an index on the similarity of the attributed graph, by partitioning the large search space into smaller subgraphs based on structure similarity and attribute similarity. Then, we construct a connectivity-based index to give a concise representation of inter-partition connections. We use the index to find a set of best matching paths. From these best matching paths, we compute the best matching answer graph using a greedy algorithm. Experimental results on real datasets demonstrate the efficiency of both index construction and query processing. We also show that our approach attains high-quality query answers.},
  Doi                      = {10.1016/j.is.2011.03.009}
}

@InProceedings{2011:dasfaa:zhu,
  Title                    = {Classifying graphs using theoretical metrics: a study of feasibility},
  Author                   = {Zhu, Linhong and Ng, Wee Keong and Han, Shuguo},
  Booktitle                = dasfaa,
  Year                     = {2011},
  Pages                    = {53--64},
  Series                   = lncs,
  Volume                   = {6637},

  Abstract                 = {Graph classification has become an increasingly important research topic in recent years due to its wide applications. However, one interesting problem about how to classify graphs based on the implicit properties of graphs has not been studied yet. To address it, this paper first conducts an extensive study on existing graph theoretical metrics and also propose various novel metrics to discover implicit graph properties. We then apply feature selection techniques to discover a subset of discriminative metrics by considering domain knowledge. Two classifiers are proposed to classify the graphs based on the subset of features. The feasibility of graph classification based on the proposed graph metrics and techniques has been experimentally studied.},
  Doi                      = {10.1007/978-3-642-20244-5\_6}
}

@InProceedings{2011:cikm:zhu,
  Title                    = {High efficiency and quality: Large graphs matching},
  Author                   = {Zhu, Yuanyuan and Qin, Lu and Yu, Jeffrey Xu and Ke, Yiping and Lin, Xuemin},
  Booktitle                = cikm,
  Year                     = {2011},
  Pages                    = {1755--1764},

  Abstract                 = {Graph matching plays an essential role in many real applications. In this paper, we study how to match two large graphs by maximizing the number of matched edges, which is known as maximum common subgraph matching and is NP-hard. To find exact matching, it cannot handle a graph with more than 30 nodes. To find an approximate matching, the quality can be very poor. We propose a novel two-step approach which can efficiently match two large graphs over thousands of nodes with high matching quality. In the first step, we propose an anchor-selection/expansion approach to compute a good initial matching. In the second step, we propose a new approach to refine the initial matching. We give the optimality of our refinement and discuss how to randomly refine the matching with different combinations. We conducted extensive testing using real and synthetic datasets, and will report our findings.},
  Doi                      = {10.1145/2063576.2063831}
}

@InProceedings{2006:kcsd:zimmermann,
  Title                    = {Knowledge collaboration by mining software repositories},
  Author                   = {Zimmermann, Thomas},
  Booktitle                = kcsd,
  Year                     = {2006},
  Pages                    = {64--65},

  Abstract                 = {We will give a short overview on recent approaches to support developers by mining software repositories and outline current and future challenges from which knowledge collaboration can benefit.}
}

@InProceedings{2008:icse:zimmermann,
  Title                    = {Predicting defects using network analysis on dependency graphs},
  Author                   = {Zimmermann, Thomas and Nagappan, Nachiappan},
  Booktitle                = icse,
  Year                     = {2008},
  Pages                    = {531--540},

  Abstract                 = {In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10\% points higher than for models built from complexity metrics. In addition, network measures could identify 60\% of the binaries that the Windows developers considered as critical---twice as many as identified by complexity metrics.},
  Doi                      = {10.1145/1368088.1368161}
}

@Article{2010:tse:zimmermann,
  Title                    = {What Makes a Good Bug Report?},
  Author                   = {Zimmermann, Thomas and Premraj, Rahul and Bettenburg, Nicolas and Just, Sascha and Schr{\"o}ter, Adrian and Weiss, Cathrin},
  Journal                  = tse,
  Year                     = {2010},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {618--643},
  Volume                   = {36},

  Abstract                 = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.},
  Doi                      = {10.1109/TSE.2010.63}
}

@InProceedings{2007:promise:zimmermann,
  Title                    = {Predicting Defects for {E}clipse},
  Author                   = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  Booktitle                = promise,
  Year                     = {2007},
  Pages                    = {9:1--9:7},

  Abstract                 = {We have mapped defects from the bug database of Eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the Eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
  Doi                      = {10.1109/PROMISE.2007.10}
}

@InProceedings{2004:msr:zimmermann,
  Title                    = {Preprocessing {CVS} data for fine-grained analysis},
  Author                   = {Zimmermann, Thomas and Wei{\ss}gerber, Peter},
  Booktitle                = msrw,
  Year                     = {2004},
  Pages                    = {2--6},

  Abstract                 = {All analyses of version archives have one phase in common: the preprocessing of data. Preprocessing has a direct impact on the quality of the results returned by an analysis. In this paper we discuss four essential preprocessing tasks necessary for a fine-grained analysis of CVS archives: (a) data extraction, (b) transaction recovery, (c) mapping of changes to fine-grained entities, and (d) data cleaning. We formalize the concept of sliding time windows and show how commit mails can relate revisions to transactions. We also present two approaches that map changes to the affected building blocks of a file, e.g. functions or sections.}
}

@Article{2005:tse:zimmermann,
  Title                    = {Mining Version Histories to Guide Software Changes},
  Author                   = {Thomas Zimmermann and Peter Wei{\ss}gerber and Stephan Diehl and Andreas Zeller},
  Journal                  = tse,
  Year                     = {2005},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {429--445},
  Volume                   = {31},

  Abstract                 = {We apply data mining to version histories in order to guide programmers along related changes: ``Programmers who changed these functions also changed...." Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent.},
  Doi                      = {10.1109/TSE.2005.72}
}

@InProceedings{2004:icse:zimmermann,
  Title                    = {Mining Version Histories to Guide Software Changes},
  Author                   = {Zimmermann, Thomas and Wei{\ss}gerber, Peter and Diehl, Stephan and Zeller, Andreas},
  Booktitle                = icse,
  Year                     = {2004},
  Pages                    = {563--572},

  Abstract                 = {We apply data mining to version histories in order toguide programmers along related changes: ``Programmers who changed these functions also changed. . . ". Given aset of existing changes, such rules (a) suggest and predictlikely further changes, (b) show up item coupling that is indetectableby program analysis, and (c) prevent errors dueto incomplete changes. After an initial change, our ROSE prototype can correctly predict 26\% of further files to be changed, and 15\% of the precise functions or variables. The topmost three suggestions contain a correct location with a likelihood of 64\%.},
  Doi                      = {10.1109/ICSE.2004.1317478}
}

@InProceedings{2003:wcre:zou,
  Title                    = {Detecting Merging and Splitting using Origin Analysis},
  Author                   = {Zou, Lijie and Godfrey, Michael W.},
  Booktitle                = wcre,
  Year                     = {2003},
  Pages                    = {146--154},

  Abstract                 = {Merging and splitting source code artifacts is a common activity during the lifespan of a software system; as developers rethink the essential structure of a system or plan fora new evolutionary direction, so must they be able to reorganize the design artifacts at various abstraction levels as seems appropriate. However, while the raw effects of such changes may be plainly evident in the new artifacts, the original intent of the design changes is often lost. In this paper, we discuss how we have extended origin analysis [10, 5] to aid in the detection of merging and splitting of files and functions in procedural code; in particular, we show how reasoning about how call relationships havechanged can aid a developer in locating where merges and splits have occurred, thereby helping to recover information about the intent of the design change. We also describe a case study of these techniques (as implemented in the Beagle tool) using the PostgreSQL database as the candidate system.},
  Doi                      = {10.1109/WCRE.2003.1287245}
}

@Book{2012:book:ozyer,
  Title                    = {Recent Trends in Information Reuse and Integration},
  Editor                   = {Tansel {\"{O}}zyer and Keivan Kianmehr and Mehmet Tan},
  Publisher                = {Springer},
  Year                     = {2012},

  Abstract                 = {We are delighted to see this edited book as the result of our intensive work over the past year. We succeeded in attracting high quality submissions of which we could only include 19 papers in this edited book. The present text aims at helping the reader whether researcher or practitioner to grasp the basic concept of reusability which is very essential in this rapidly growing information era. The authors emphasize the need for reusability and how it could be adapted in the right way. Actually reusability leads to satisfy a multiobjective optimization process, that is, to minimize the time, cost and effort spent to develop new products, technologies, information repositories, etc. Instead of always starting from scratch and reinventing the wheel in a process that consumes and wastes time, effort, and resources, practitioners and developers should always look into the possibility of reusing some of the existing entities to produce new ones. In other words, reuse and integration are essential concepts that must be enforced to avoid duplicating the effort. This problem is investigated from different perspectives. In organizations, high volumes of data from different sources form a big threat for filtering out the information for effective decision making. To address all these vital and serious concerns, this book covers the most recent advances in information reuse and integration. It contains high quality research papers written by experts in the field. Some of them are extended versions of the best papers which were presented at IEEE International Conference on Information Reuse and Integration, which was held in Las Vegas in August 2010. Chapter 1 by Udai Shanker, B. Vidya Reddi, and Anupam Shukla studies a real time commit protocol to improve the performance based on two approaches. They are: 1. Some of the locked data items which are unused after completion of processing of transaction can be unlocked immediately after end of processing phase to reduce data contention. 2. A lending transaction can lend its dirty data to more than one cohorts by creating only a single shadow in case of write--read conflicts to reduce the data inaccessibility. Its performance has been compared with existing protocols. Chapter 2 by Ladjel Bellatreche estimates the complexity of the problem of selecting dimension table(s) to partition a fact table. It also proposes strategies to perform their selection that take into account the main characteristics of queries such as access frequencies, size of tables, size of intermediate results of joins, etc. Experimental studies using a mathematical cost model and the obtained results have been executed on Oracle11GDBMS for validation, and a tool has been implemented to assist data warehouse administrators in their horizontal partitioning selection tasks. Chapter 3 by Shabnam Pourdehi, Dena Karimipour, Navid Noroozi, and Faridoon Shabani addresses an adaptive fuzzy controller for a large class of nonlinear systems in the presence of uncertainties, input nonlinearities, and unknown timedelay. Based on the combination of the sliding mode control (SMC) with fuzzy adaptive control, it presents a design algorithm to synthesize a robust fuzzy sliding mode controller. Later, an adaptive fuzzy observer-based SMC scheme is proposed for stabilization. The unknown nonlinear functions have been approximated with fuzzy logic systems in both proposed control schemes and its asymptotical stability according to the corresponding closed-loop system is shown with Lyapunov--Krasovskii approach. The synchronization of two nonidentical time-delayed chaotic systems is investigated as an application of control schemes with simulated examples to show the effectiveness of the proposed techniques. Chapter 4 by Ladjel Bellatreche, Guy Pierra, and Eric Sardet presents a technique for integrating automatically ontology-based data sources in a materialized architecture and a framework dealing with the asynchronous versioning problem. Existing solutions proposed in traditional databases is adapted to manage instance and schema changes. The problem of managing ontology changes is overcome by introducing a distinction between ontology evolution and ontology revolution. Finally, it proposes floating version model for ontology evolution. It fully automates the whole steps of building an ontology-based integration systems (OBIS). It is validated by a prototype using ECCO environment and the EXPRESS language. Chapter 5 by Iyad Suleiman, Shang Gao, Maha Arslan, Tamer Salman, Faruk Polat, Reda Alhajj, and Mick Ridley suggests a a computerized assessment tool that can learn the user's skills and adjust the assessment tests for assessment of school readiness. The user plays various sessions from various games, while the Genetic Algorithm (GA) selects the upcoming session or group of sessions to be chosen for the user according to his/her skills and status. It describes the modified GA and the learning procedure that is integrated with a penalizing system into the GA and a fitness heuristic for best choice selection. Two methods for learning are proposed, a memory system and a no memory system. Furthermore, it includes several methods for the improvement of the speed of learning. In addition, learning mechanisms that are based on the social network paradigm to address further usage of assessment automation is used. Chapter 6 by Gr{\'e}gory Claude, Ga{\"e}l Durand, Marc Boyer, and Florence S{\`e}des defends the idea that a defect carries information by the simple fact that it exists, the characteristics of the detected incident (the problem) and of the applied protocol to resolve it (the solution). The various actors who took part in its description and its resolution collect this information. This knowledge is essential to achieve for assistance in corrective actions for future defects and prevention of their emergence. Taking the advantage of this knowledge by working out a model of defect makes it possible to define a set of grouping criteria of defects that were solved in the past. These groups are the cornerstone of the corrective and preventive processes for new defects. Chapter 7 by Abdullah M. Elsheikh, Tamer N. Jarada, Taher Naser, Kelvin Chung, Armen Shimoon, Faruk Polat, Panagiotis Karampelas, Jon Rokne, Mick Ridley, and Reda Alhajj presents a comprehensive approach for the mapping between the object database language (ODL) and XML. It includes both structure specification and database content. It concentrates on deriving a separate set of transformation rules for each way mapping. For the first mapping (from ODL to XML). Chapter 8 by Taghi M. Khoshgoftaar, Kehan Gao, and Jason Van Hulse proposes a novel approach to feature selection for imbalanced data in the context of software quality engineering. This process follows a repetitive process of data sampling followed by feature ranking and finally aggregating the results generated during the repetitive process. It is compared against filter-based feature ranking technique alone on the original data, and data sampling and feature ranking techniques with two different scenarios. Chapter 9 by Zifang Huang and Mei-Ling Shyu presents a k-nearest neighbors (k-NN)-based least squares support vector machine (LS-SVM) model with multivalue integration to tackle the long-term time series prediction problem. A new distance function, which incorporates the Euclidean distance and the dissimilarity of the trend of a time series, is deployed. Chapter 10 by Carlos M. Cornejo, Iv{\'a}n Ruiz-Rube, and Juan Manuel Dodero discusses their approach to provide a complete set of services and applications to integrate diverse web-based contents of the cultural domain. It is purposed to extend over the web the knowledge base of cultural institutions, build user communities around it, and enable its exploitation in several environments. Chapter 11 by Abdelghani Bakhtouchi, Ladjel Bellatreche, Chedlia Chakroun, and Yamine A{\"\i}t-Ameur proposes an ontology-based integration system with mediator architecture. It exploits the presence of ontology referenced by selected sources to explicit their semantic. Instead of annotating different candidate keys of each ontology class, its set of functional dependencies are defined on its properties. Chapter 12 by Mark McKenney gives an efficient algorithm for the map construction algorithm (MCP). The algorithm is implemented and experiments show that it is significantly faster than the naive approach, but is prone to large memory usage when run over large data sets. An external memory version of the algorithm is also presented that is efficient for very large data sets, and requires significantly less memory than the original algorithm. Chapter 13 by Jairo Pava, Fausto Fleites, Shu-Ching Chen, and Keqi Zhang proposes a systemthat integrates storm surge projection, meteorological, topographical, and road data to simulate storm surge conditions. The motivation behind the system is to serve local governments seeking to overcome difficulties in persuading residents to adhere to evacuation notices. Chapter 14 by Richa Tiwari, Chengcui Zhang, Thamar Solorio, and Wei-Bang Chen describes a framework to extract information about co-expression relationships among genes from published literature using a supervised machine learning approach. Later it rank those papers to provide users with a complete specialized information retrieval system with Dynamic Conditional Random Fields (DCRFs) for training the model. They show its superiority against Bayes Net, SVM, and Nave Bayes. Chapter 15 by Brandeis Marshall identifies similar artists, which serves as a precursor to how music recommendation can handle the more complex issues of multiple genre artists and artist collaborations. It considers the individual most similar artist ranking from three public-useWeb APIs (Idiomag, Last.fm, and Echo Nest) as different perspectives of artist similarity. Then it aggregates these three rankings using five rank fusion algorithms. Chapter 16 by Ken Q. Pu and Russell Cheung presents a query facility called tag grid to support fuzzy search and queries of both tags and data items alike. By combining methods of Online Analytic Processing from multidimensional databases and collaborative filtering from information retrieval, tag grid enables users to search for interesting data items by navigating and discovering interesting tags. Chapter 17 by Stefan Silcher, Jorge Minguez, and Bernhard Mitschang introduces an SOA-based solution to the integration of all Product Lifecycle Management (PLM) phases. It uses an Enterprise Service Bus (ESB) as service-based integration and communication infrastructure. Three exemplary scenarios are used to illustrate the benefits of using an ESB as compared to alternative PLM infrastructures. Furthermore, it describes a service hierarchy that extends PLM functionality with value-added services by mapping business processes to data integration services. Chapter 18 by Awny Alnusair and Tian Zhao describes an ontology-based approach for identifying and retrieving relevant software components in large reuse libraries. It exploits the use of domain-specific ontologies to enrich a knowledge base initially populated with ontological descriptions of API components. Chapter 19 by Du Zhang proposes an algorithm for detecting several types of firewall rule inconsistency. It also defines a special type of inconsistency called setuid inconsistency and highlights various other types of inconsistencies in the aforementioned areas. Finally, it concludes that inconsistency is a very important phenomenon, and its utilities can never be underestimated in information security and digital forensics. Last but not the least, we would like to mention the hard workers behind the scene who have significant unseen contributions to the successful task that produced this valuable source of knowledge. We would like to thank the authors who submitted papers and the reviewers who produced detailed constructive reports which improved the quality of the papers. Various people from Springer deserve large credit for their help and support in all the issues related to publishing this book. In particular, we would like to thank Stephen Soehnlen for his dedications, seriousness, and generous support in terms of time and effort; he answered our emails on time despite his busy schedule, even when he was traveling.}
}

@Book{2001:book:eigenmann,
  Title                    = {Performance Evaluation and Benchmarking with Realistic Applications},
  Editor                   = {Rudolf Eigenmann},
  Publisher                = {MIT Press},
  Year                     = {2001},

  Abstract                 = {Performance evaluation and benchmarking are of concern to all computer-related disciplines. A benchmark is a standard program or set of programs that can be run on different computers to give an accurate measure of their performance. This book covers a variety of aspects of computer performance evaluation, with a focus on Standard Performance Evaluation Corporation (SPEC) benchmarks. SPEC is a nonprofit organization whose members represent industry, academia, and other organizations. The book discusses rationales for creating and updating benchmarks, the use of benchmarks in academic research, benchmarking methodologies, the relation of SPEC benchmarks to other benchmarking activities, shortcomings of current benchmarks, and the need for further benchmarking efforts.}
}

@Book{1995:book:nierstrasz,
  Title                    = {Object-Oriented Software Composition},
  Editor                   = {Oscar Nierstrasz and Dennis Tsichritzis},
  Publisher                = {Prentice Hall},
  Year                     = {1995}
}

@Misc{Dila,
  Title                    = {Dila: Dynamic load-time instrumentation library},
  HowPublished             = {http://wala.sourceforge.net/wiki/index.php/Getting\discretionary{}{}{}Started:wala.dila},

  Key                      = {Dila},
  Url                      = {http://wala.sourceforge.net/wiki/index.php/GettingStarted:wala.dila}
}

@Electronic{google_code_search_2012,
  Title                    = {Google Code Search},
  Note                     = {Accessed Jan. 2012},
  Url                      = {http://\discretionary{}{}{}www.\discretionary{}{}{}google.\discretionary{}{}{}com/\discretionary{}{}{}codesearch}
}

@Electronic{merobase_2012,
  Title                    = {Merobase},
  Note                     = {Accessed Jan. 2012},
  Url                      = {http://\discretionary{}{}{}www.\discretionary{}{}{}merobase.\discretionary{}{}{}com/}
}

@Electronic{sourcerer_2012,
  Title                    = {Sourcerer},
  Note                     = {Accessed Jan. 2012},
  Url                      = {http://\discretionary{}{}{}sourcerer.\discretionary{}{}{}ics.\discretionary{}{}{}uci.edu/sourcerer/search/index.jsp}
}

@Misc{2013:misc:eclipse-doc,
  HowPublished             = {\url{http://help.eclipse.org}},
  Year                     = {2013},

  Key                      = {Workbench User Guide}
}

@Book{dictionary,
  Title                    = {American Heritage Dictionary of the English Language},
  Publisher                = {Houghton Miflin Company},
  Year                     = {2009},
  Edition                  = {4th}
}

@Misc{2003:misc:rcp-help,
  HowPublished             = {\url{http://eclipse.org/rcp/restructuring_help.html}},
  Month                    = sep,
  Year                     = {2003},

  Key                      = {Help API change for Rich Client Platform}
}

@Misc{2003:misc:rcp-ui,
  HowPublished             = {\url{http://eclipse.org/rcp/generic_workbench_summary.html}},
  Month                    = dec,
  Year                     = {2003},

  Key                      = {Eclipse Rich Client Platform UI}
}

@Misc{2003:misc:rcp-workbench,
  HowPublished             = {\url{http://eclipse.org/rcp/generic_workbench_structure.html}},
  Month                    = nov,
  Year                     = {2003},

  Key                      = {Generic workbench structure}
}

@TechReport{nist,
  Title                    = {The Economic Impacts of Inadequate Infrastructure for Software Testing},
  Institution              = {National Institute of Standards {\&} Technology},
  Year                     = {2002},
  Month                    = may,
  Number                   = {02-3},
  Type                     = {Planning report},

  Abstract                 = {Software has become an intrinsic part of business over the last decade. Virtually every business in the U.S. in every sector depends on it to aid in the development, production, marketing, and support of its products and services. Advances in computers and related technology have provided the building blocks on which new industries have evolved. Innovations in the fields of robotic manufacturing, nanotechnologies, and human genetics research all have been enabled by low cost computational and control capabilities supplied by computers and software. In 2000, total sales of software reached approximately $180 billion. Rapid growth has created a significant and high-paid workforce, with 697,000 employed as software engineers and an additional 585,000 as computer programmers. Reducing the cost of software development and improving software quality are important objectives of the U.S. software industry. However, the complexity of the underlying software needed to support the U.S.'s computerized economy is increasing at an alarming rate. The size of software products is no longer measured in terms of thousands of lines of code, but millions of lines of code. This increasing complexity along with a decreasing average market life expectancy for many software products has heightened concerns over software quality. Software nonperformance and failure are expensive. The media is full of reports of the catastrophic impact of software failure. For example, a software failure interrupted the New York Mercantile Exchange and telephone service to several East Coast cities in February 1998 (Washington Technology, 1998). Headlines frequently read, ``If Microsoft made cars instead of computer programs, product-liability suits might now have driven them out of business.'' Estimates of the economic costs of faulty software in the U.S. range in the tens of billions of dollars per year and have been estimated to represent approximately just under 1 percent of the nation's gross domestic product (GDP).
In actuality many factors contribute to the quality issues facing the software industry. These include marketing strategies, limited liability by software vendors, and decreasing returns to testing and debugging. At the core of these issues is the difficulty in defining and measuring software quality. Common attributes include functionality, reliability, usability, efficiency, maintainability, and portability. But these quality metrics are largely subjective and do not support rigorous quantification that could be used to design testing methods for software developers or support information dissemination to consumers. Information problems are further complicated by the fact that even with substantial testing, software developers do not truly know how their products will perform until they encounter real scenarios. The objective of this study is to investigate the economic impact of an inadequate infrastructure for software testing in the U.S. The National Institute of Standards and Technology (NIST) undertook this study as part of joint planning with industry to help identify and assess technical needs that would improve the industry's software testing capabilities. The findings from this study are intended to identify the infrastructure needs that NIST can supply to industry through its research programs. To inform the study, RTI conducted surveys with both software developers and industry users of software. The data collected were used to develop quantitative estimates of the economic impact of inadequate software testing methods and tools. Two industry groups were selected for detailed analysis: automotive and aerospace equipment manufacturers and financial services providers and related electronic communications equipment manufacturers. The findings from these two industry groups were then used as the basis for estimating the total economic impact for U.S. manufacturing and services sectors. Based on the software developer and user surveys, the national annual costs of an inadequate infrastructure for software testing is estimated to range from $22.2 to $59.5 billion. Over half of these costs are borne by software users in the form of error avoidance and mitigation activities. The remaining costs are borne by software developers and reflect the additional testing resources that are consumed due to inadequate testing tools and methods.},
  Key                      = {NIST},
  Url                      = {http://www.nist.gov/director/prog-ofc/report02-3.pdf}
}

@comment{jabref-meta: selector_keywords:}

@comment{jabref-meta: selector_abstract:reusability;reusable;reuse;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:architectural evolution\;0\;1992:tse:kozaczynski\;1998
:cascon:erdogmus\;1999:misc:gregory\;1999:tools:christensen\;2000:ase:
fahmy\;2000:icsm:fahmy\;2001:ase:fahmy\;2001:esec_fse:van_der_hoek\;20
01:iwpse:mikkonen\;2001:pfe:maccari\;2002:gpce:butler\;2002:iwpse:baxt
er\;2002:iwpse:godfrey\;2002:seke:ambriola\;2003:icsm:gold\;2003:iwpse
:oreilly\;2003:phd:svahnberg\;2004:ase:jansen\;2004:fcds:kmiecik\;2004
:icsm:nguyen\;2004:informatica:ambriola\;2004:oopsla:boshernitsan\;200
4:tosem:roshandel\;2004:wicsa:postma\;2005:euromicro:barais\;2005:icse
:lago\;2005:iwpse:sadou\;2005:tse:xing\;2005:wicsa:barais\;2006:itpro:
erder\;2006:mansci:maccormack\;2006:pcs:ksenzov\;2006:seke:tamzalit\;2
007:chi:boshernitsan\;2007:compsac:tamzalit\;2007:infoscijenkins\;2008
:ainaw:cuadrado\;2008:apsec:slyngstad\;2008:book:mens:barais\;2008:com
psac:legoaer\;2008:icpc:dong\;2008:ijase:sutcliffe\;2008:jsw:nguyen\;2
008:models:robbes\;2008:msr:wermelinger\;2008:shark:legoaer\;2008:wics
a:lamantia\;2009:csmr:ackermann\;2009:icpc:feilkas\;2009:icse:garlan\;
2009:jsmerp:rosso\;2009:tr:christensen\;2009:wicsa_ecsa:garlan\;2010:c
smr:schrettner\;2010:ecbs:tamzalit\;2010:seke:legoaer\;2010:shark:nopp
en\;2011:csmr:belderrar\;2011:esec_fse:meng\;2011:icse:mcveigh\;2011:p
ldi:meng\;2011:qosa_isarcs:stal\;2012:ist:breivold\;2012:jss:breivold\
;2012:jss:desilva\;;
1 ExplicitGroup:reuse\;0\;1984:tse:standish\;1987:software:biggerstaff
\;1987:software:fischer\;1988:joop:johnson\;1989:book:biggerstaff:feat
her\;1989:chi:lange\;1989:chi:neal\;1990:ecoop_oopsla:helm\;1990:sej:g
arnett\;1990:sen:tracz\;1991:icse:fischer\;1991:sej:maiden\;1991:sigir
:henninger\;1991:software:barnes\;1992:cacm:maiden\;1992:csur:krueger\
;1992:ecoop:holland\;1992:icse:gaffney\;1992:phd:redmiles\;1992:thesis
:opdyke\;1992:tosem:ostertag\;1992:tse:kozaczynski\;1993:ecoop:rosson\
;1993:fse:moormann_zaremski\;1993:ibmsj:poulin\;1993:iwsr:castano\;199
3:sac:harandi\;1993:software:prieto-diaz\;1994:iccd:mayrhauser\;1994:i
cse:basili\;1994:icse:mili\;1994:icse:wasmund\;1994:icsm:canfora\;1994
:icsr:biggerstaff\;1994:icsr:merkl\;1994:icsr:neighbors\;1994:software
:card\;1994:software:henninger\;1995:cacm:frakes\;1995:computer:von_ma
yrhauser\;1995:interact:burkhardt\;1995:software:garlan\;1995:ssr:biem
an\;1995:ssr:dusink\;1995:ssr:jeng\;1995:ssr:tracz\;1995:vldbj:constan
topoulos\;1996:cacm:basili\;1996:oopsla:steyaert\;1996:tochi:rosson\;1
996:tse:frakes\;1997:cacm:johnson\;1997:cbr:tautz\;1997:ecoop:kiczales
\;1997:icse:froehlich\;1997:icsm:burd\;1997:ssr:bassett\;1997:tosem:he
nninger\;1997:tosem:moormann_zaremski\;1997:tse:sen\;1998:annse:bigger
staff\;1998:annse:mili\;1998:icse:harrold\;1998:software:weyuker\;1999
:ase:maletic\;1999:computer:boehm\;1999:icse:michail\;1999:oopsla:rina
t\;1999:ssr:ran\;2000:annsesneed\;2000:asset:krueger\;2000:csur:wang\;
2000:fse:ye\;2000:icse:lippert\;2000:icse:michail\;2000:popl:komondoor
\;2000:seke:gresse_von_wangenheim\;2000:tr:chai\;2001:csur:weihe\;2001
:icse:michail\;2001:icse:tarr\;2001:jss:frakes\;2001:tse:succi\;2002:i
cfem:liu\;2002:icse:ye\;2002:issre:fisher\;2002:tse:morisio\;2003:cacm
:ravichandran\;2003:cbr:bjornestad\;2003:sste:torkar\;2003:tse:rothenb
erger\;2004:fse:pan\;2004:icse:mohagheghi\;2004:iri:hummel\;2004:msr:y
usof\;2004:tse:morel\;2004:tse:parsons\;2005:aosd:garcia\;2005:esec_fs
e:estublier\;2005:pldi:mandelin\;2005:tse:frakes\;2005:tse:selby\;2005
:tse:van_ommering\;2006:fse:filho\;2006:icse:sindhgatta\;2006:jsmerp:g
irba\;2006:msr:sager\;2006:msr:xie\;2007:ase:lemos\;2007:ase:thummalap
enta\;2007:cbr:gomes\;2007:etx:jablonski\;2007:jss:ajila\;2007:oopsla:
lemos\;2007:software:bassett\;2007:tse:lau\;2007:tse:xing\;2007:xp:hum
mel\;2008:compsac:legoaer\;2008:icse:figueiredo\;2008:icsm:hou\;2008:i
jase:sutcliffe\;2008:ms:haefliger\;2008:phd:hummel\;2008:shark:legoaer
\;2008:software:hummel\;2008:software:jansen\;2008:software:ncube\;200
9:icse:hill\;2009:icse:reiss\;2009:icsr:hummel\;2009:ietsoft:aversano\
;2009:msr:german\;2009:sac:lemos\;2009:software:garlan\;2009:suite:gal
lardo-valencia\;2009:tse:basit\;2010:ase:german\;2010:gcms:verl\;2010:
msr:krinke\;2010:msr:ossher\;2010:suite:hummel\;2010:suite:janjic\;201
0:wcre:xue\;2011:ist:lemos\;2012:book:ozyer\;2012:icse:chowdhury\;2012
:icse:mcmillan\;2013:sosym:reimann\;;
1 ExplicitGroup:correspondence\;0\;1922:jrss:fisher\;1945:bb:wilcoxon\
;1952:jasa:kruskal\;1976:tr:hunt\;1978:cacm:heckel\;1980:csur:hall\;19
85:infocon:ukkonen\;1985:spe:miller\;1986:algorithmica:myers\;1988:tpa
mi:umeyama\;1990:pldi:horwitz\;1991:rta:baader\;1991:spe:yang\;1992:ic
sm:binkley\;1992:icsm:laski\;1992:tse:kozaczynski\;1992:usenix:wu\;199
2:usenixcpp:grass\;1993:fse:moormann_zaremski\;1994:icsm:jackson\;1994
:software:henninger\;1994:toplas:yu\;1995:ddj:coppieters\;1995:icsm:ch
en\;1995:ssr:jeng\;1995:tosem:moormann_zaremski\;1995:tse:dean\;1995:w
cre:baker\;1996:jetai:prietula\;1996:sigcse:wise\;1997:esec_fse:reps\;
1997:icsm:burd\;1997:patreclet:bunke\;1997:toplas:yellin\;1997:tosem:m
oormann_zaremski\;1998:icsm:baxter\;1998:usenixatc:baker\;1999:csss:ba
ker\;1999:icse:michail\;1999:ijase:penix\;1999:oopsla:rinat\;1999:tpam
i:pelillo\;2000:book:gladwell\;2000:ccece:tahvildari\;2000:fse:ye\;200
0:jilp:wang\;2000:popl:komondoor\;2000:seke:gresse_von_wangenheim\;200
0:sigir:buckley\;2000:tosem:inverardi\;2001:ase:marcus\;2001:csur:nava
rro\;2001:icsm:mitchell\;2001:jsmerp:wang\;2001:pfe:bosch\;2001:softwa
re:larman\;2002:apsec:fidge\;2002:compsac:merlo\;2002:icfem:liu\;2002:
icsm:hunt\;2002:iwpc:tu\;2002:iwpse:godfrey\;2002:msc:wagner\;2002:sss
pr:robles-kelly\;2002:tois:jarvelin\;2002:wcre:bull\;2003:cbr:champin\
;2003:esec_fse:ohst\;2003:tse:eisenbarth\;2003:wcre:zou\;2004:ase:apiw
attanapong\;2004:etx:xing\;2004:fse:pan\;2004:icsm:wen\;2004:ist:etzko
rn\;2004:msr:yusof\;2004:oopsla:ren\;2004:scam:wahler\;2004:tpami:cael
li\;2005:aij:burghardt\;2005:ase:xing\;2005:asian:zhang\;2005:esec_fse
:kim\;2005:esec_fse:zhang\;2005:gbrpr:sorlin\;2005:icsm:marcus\;2005:m
sr:neamtiu\;2005:profes:yamamoto\;2005:tse:godfrey\;2005:tse:xing\;200
6::zheng\;2006:cascon:muller_molina\;2006:gimm:kolovos\;2006:icpc:scho
field\;2006:jsmerp:ducasse\;2006:lmo:mens\;2006:mansci:maccormack\;200
6:msr:kim:a\;2006:patrecog:luo\;2006:patrecog:qiu\;2006:scam:collard\;
2006:sen:counsell\;2006:spe:atkinson\;2006:spin:de_la_camara\;2006:wcr
e:koschke\;2007:book:kandel:sorlin\;2007:esec_fse:latoza\;2007:etx:jab
lonski\;2007:icse:jiang\;2007:icse:kim:a\;2007:icsm:nagarajan\;2007:id
eal:berzal\;2007:ijase:apiwattanapong\;2007:ijase:xing\;2007:msr:canfo
ra\;2007:scam:muller_molina\;2007:sttt:pahl\;2007:tse:bellon\;2007:tse
:concas\;2007:tse:fluri\;2007:www:white\;2008:book:perner:jiang\;2008:
dma:kostylev\;2008:eit:stanek\;2008:entcs:moskal\;2008:ese:falke\;2008
:ese:kapser\;2008:icse:hoffman\;2008:patrecog:wilson\;2008:shark:legoa
er\;2008:syrcose:bulychev\;2008:wcre:kpodjedo\;2009:cviu:xiao\;2009:da
m:chandrasekaran\;2009:gbrpr:knossow\;2009:icpc:dekel\;2009:icpc:figue
iredo\;2009:issta:gorg\;2009:ivc:emms\;2009:iwsc:bulychev\;2009:jsmerp
:mende\;2009:msr:german\;2009:patrecog:emms\;2009:popl:brunel\;2009:pp
cp:le_clement\;2009:sac:lemos\;2009:scp:roy\;2009:tpami:zaslavskiy\;20
09:tse:basit\;2009:wcre:kpodjedo\;2010:aosd:palix\;2010:ase:german\;20
10:book:aggarwal:riesen\;2010:book:aggarwal:zhang\;2010:evocop:kpodjed
o\;2010:icse:loh\;2010:icse:nita\;2010:icse:pham\;2010:ijase:robbes\;2
010:iwmcp:barrett\;2010:msr:krinke\;2010:patreclet:raveaux\;2010:tosem
:duala-ekoko\;2010:wcre:xue\;2011:aosd:yokomori\;2011:ase:nguyen:b\;20
11:ase:yu\;2011:cbr:weber\;2011:cikm:zhu\;2011:codes+isss:stattelmann\
;2011:csmr:anquetil\;2011:ecoop:maoz\;2011:ese:robillard\;2011:esec_fs
e:maoz\;2011:fase:cai\;2011:gbrpr:weber\;2011:infosyst:zhu\;2011:issta
:zhang\;2011:ist:lemos\;2012:acmse:liu\;2012:infosci:miranskyy\;2013:i
jase:liu\;2013:tosem:walkinshaw\;;
1 ExplicitGroup:refactoring\;0\;1992:thesis:opdyke\;1992:tse:kozaczyns
ki\;1993:tosem:griswold\;1999:book:fowler\;2000:oopsla:demeyer\;2002:g
pce:butler\;2003:oopsla:tip\;2003:physreve:myers\;2004:ase:hill\;2004:
book:kerievsky\;2004:tse:mens\;2004:xp:lippert\;2005:aosd:hannemann\;2
005:esec_fse:kim\;2005:icse:henkel\;2005:icsm:dig\;2005:icsm:marin\;20
05:iwpc:gorg\;2005:macs:marin\;2005:msr:gorg\;2005:oopsla:balaban\;200
5:oopsla:weissgerber\;2005:paste:perkins\;2005:snpd_sawn:grunske\;2006
:ase:weissgerber\;2006:book:lippert\;2006:ecoop:dig\;2006:fse:filho\;2
006:icpc:schofield\;2006:icse:verbaere\;2006:icsm:xing\;2006:jsmerp:di
g\;2006:lmo:mens\;2006:msr:weissgerber:a\;2006:msr:weissgerber:b\;2006
:pcs:ksenzov\;2006:phd:xu\;2006:scam:collard\;2006:sen:dig\;2006:softw
are:murphy\;2006:wcre:xing\;2007:esem:ratzinger\;2007:fase:robbes\;200
7:gpce:savga\;2007:icse:kim:a\;2007:infoscijenkins\;2007:tr:hinsman\;2
007:wrt:fuhrer\;2008:gpce:savga\;2008:icse:dig\;2008:icse:hoffman\;200
8:icse:murphy-hill\;2008:icse:schafer\;2008:jsmerp:okeeffe\;2008:model
s:robbes\;2008:oopsla:tansey\;2008:tse:dig\;2009:entcs:dasilva\;2009:w
rt:murphy-hill\;2010:fse:kim\;2010:icse:loh\;2010:icsm:prete\;2011:aos
d:yokomori\;2011:esec_fse:meng\;2011:fase:cai\;2011:isec:kumar\;2011:p
ldi:meng\;2011:qosa_isarcs:stal\;2011:software:buschmann:a\;2011:softw
are:buschmann:b\;2012:acmse:liu\;;
1 ExplicitGroup:cossette-fse-2012\;0\;1980:ieee:lehman\;1994:icse:parn
as\;1996:icsm:chow\;1997:cacm:johnson\;1999:book:fowler\;2001:software
:larman\;2004:tse:morel\;2005:icse:henkel\;2005:oopsla:balaban\;2005:p
aste:perkins\;2005:tse:godfrey\;2006:ase:weissgerber\;2006:ecoop:dig\;
2006:jsmerp:dig\;2006:oopsla:bloch\;2007:icse:kim:a\;2007:tse:xing\;20
08:icse:dig\;2010:icse:nita\;2010:icse:wu\;2011:ese:robillard\;2011:to
sem:dagenais\;2012:book:gosling\;;
1 ExplicitGroup:walker-fse-2012\;0\;1972:cacm:parnas\;1985:tse:yau\;19
97:ecoop:kiczales\;1999:book:baldwin\;1999:icse:tarr\;2001:bioscience:
limpert\;2001:esec_fse:sullivan\;2002:tosem:mockus\;2003:aosd:zhang\;2
003:oose:german\;2005:aosd:garcia\;2005:icsm:marin\;2005:iwpc:revelle\
;2006:aosd:kienzle\;2006:fse:filho\;2006:mansci:maccormack\;2006:tse:k
elly\;2007:cascon:ayari\;2007:ecoop:greenwood\;2007:fase:lopezherrejon
\;2007:hicss:asundi\;2007:software:kitchenham\;2007:wcre:marin\;2008:i
cse:figueiredo\;2008:oopsla:baldi\;2008:sqj:bartsch\;2008:tosem:lourid
as\;2008:tse:eaddy\;2008:wicsa:lamantia\;2009:icpc:figueiredo\;2009:ie
tsoft:aversano\;2009:pse:ihara\;2009:pse:nurolahzade\;2009:siam:clause
t\;2010:cscw:breu\;2010:enase:przybylek\;2011:aosd:figueiredo\;2011:fa
se:przybylek\;;
1 ExplicitGroup:recommendation systems\;0\;1904:ajp:spearman\;2001:boo
k:carroll:terveen\;2004:tse:ying\;2005:tse:cubranic\;2008:icse:murphy-
hill\;2008:phd:hummel\;2008:re:castro-herrera\;2008:rsse:bruch\;2009:i
cse:hill\;2009:iseud:jeong\;2009:sac:castro-herrera\;2009:suite:robbes
\;2010:book:jannach\;2010:chi:hartmann:a\;2010:chi:hartmann:b\;2010:ic
sm:bavota\;2010:jsmerp:robillard\;2010:rsse:hummel\;2010:software:koru
\;2011:ecoop:duala-ekoko\;2011:fase:cai\;2011:isec:kumar\;2011:tosem:a
nvik\;2011:tosem:dagenais\;2012:book:ozyer\;2012:csmr:terra\;2012:fse:
bird\;2012:fse:murphy-hill\;2012:icse:chowdhury\;2012:icse:mcmillan\;2
012:icse:nguyen:b\;2012:icse:song\;2012:icse:zhang\;2012:umap:felferni
g\;2012:vlhcc:kononenko\;2013:sosym:reimann\;;
}

