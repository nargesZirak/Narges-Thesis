\chapter{Related Work}  \label{rw}
In this chapter, I review related work to the topics of my study including: the application of logging in real-world software systems (Section~\ref{logging}), understanding the existing logging practices (Section~\ref{understand-logging}),
 determining correspondences in source code (Section~\ref{ch7-corr}), data mining approaches to extract API usage patterns (Section~\ref{ch7-usage-patterns}), and anti-unification and its application to detect structural correspondences and construct generalizations (Section~\ref{ch7-au}). %, and clustering (Section~\ref{ch7-clustering}).
% constructing the structural generalizations (Section~\ref{ch7-generalization}

\section{Usage of logging}  \label{logging}
Logging is a conventional programming practice used to record a software system's runtime information, and it can be used in system analysis to trace the root causes of systems' activities. Log analysis is most often performed for failure diagnosis, system behavioural understanding, system security monitoring, and performance diagnosis purposes as described below:
\begin{itemize} [leftmargin=0.5in]
\item \emph{Log analysis for failure diagnosis:} \citet{xu2009detecting} use statistical techniques to learn a decision tree based signature from console logs and then utilize the signature to diagnose anomalies. \tool{SherLog} \cite{yuan2010sherlog} uses failure log messages to infer the source code paths that might have been executed during a failure. \citet{jiang2009understanding} study the effectiveness of logging in problem diagnosis. Their study shows that customer problems in software systems with logging resolve faster than those without logging by investigating the correlations between failure root causes and diagnosis time.
\item \emph{Log analysis for system behaviour understanding:} \citet{fu2013contextual} present an approach for understanding system behaviour through contextual analysis of logs. They first extracted execution patterns reflected by a sequence of system logs and then utilized the patterns to find contextual factors from logs that cause a specific system behavior. The Linux Trace Toolkit \cite{yaghmour2000measuringandcharacter} was created to record and analyze system behavior by providing an efficient kernel-level event logging infrastructure. A more flexible approach is taken by \tool{DTrace}~\cite{cantrill2004dynamic} which allows dynamic modification of kernel code.
\item \emph{Log analysis for system security monitoring:} \citet{bishop1989model} proposes a formal model of a system's security monitoring using logging and auditing. \citet{peisert2007toward} have developed a model that demonstrates a mechanism for extracting logging information to detect how an intrusion occurs in software systems.
%have developed?
% Jiang et al. [2009b] present an approach to automatically detect problems of load tests by mining the execution logs of an application. Many software systems must be load tested for their functional and performance problems diagnosis.
\item \emph{Log analysis for performance diagnosis:} \citet{nagaraj2012structured} developed an automated tool to assist developers in diagnosis and correction of performance issues in distributed systems by analyzing system behaviours extracted from the log data.
\end{itemize}



\section{Understanding existing logging practices}  \label{understand-logging}

Despite the importance of logging for software development and maintenance, few studies have been conducted in pursuit of understanding logging usage in real-world software.

\citet{yuan2012characterizing} provides a quantitative characteristic study to investigate log message modifications of four open-source software systems by mining their revision history.  They have also discovered that developers are continuously making an effort to modify the context of log statements to improve the quality of logging practices. \citet{shang2015studying} performed an empirical study to find the relation between logging characteristics and software quality. They found that log-related metrics are good predictors for post-release defects.

\citet{kabinna2016examining} empirically studied the stability of log statements in four open source software systems. They model the historical log statement changes using a data mining classifier. They find that developer experience, file ownership, log density and SLOC are important indicators of whether a log statement will change in the future or not.

\citet{kabinna2016logging} studied the logging library migration of several software systems within the Apache Software Foundation\@. They found that the migration of logging libraries happened in these systems in order to enhance the system's flexibility and performance, and also to utilize more advanced functionalities. Furthermore, they discovered that migration is not a trivial task, as 70\% of the migrated projects encounter post-migration bugs.
%verbs??

\citet{yuan2012conservative} developed \tool{Errlog}, a tool that automatically inserts a log statement in source code when a generic error condition happens, in order to assist failure diagnosis. \tool{LogEnhancer} \cite{yuan2012improving} automatically enhances existing log messages by detecting variables containing important values and inserting them into the log messages. However, these studies only consider source code fragments containing bugs that are needed to be logged and do not consider the other code fragments with no bugs that were still logged. Moreover, these studies mainly research log message modifications and potential enhancements of them; in contrast, the focus of my study is on understanding where log statements are used in source code.

\citet{fu2014developers} and \citet{zhu2015learning} applied a data mining approach to detect the main factors impacting the location of logging statements. \citet{fu2014developers} also conducted a survey to validate their findings. \citet{ding2015log2} proposed a constraint-based approach to decide whether to log for each logging request at run-time in order to minimize the performance overhead. In contrast, my study is the first work that automatically characterized logging usage in source code by constructing anti-unifiers that represent the structural similarities and differences among a set of source code fragments containing logging statements.
%\citet{ding2015log2} proposed an constraint-based approach to decide whether to log for each logging request at run-time in order to minimize the performance overhead.?
%automatically??
%\citet{fu2014developers} and \citet{zhu2015learning} applied a data mining approach to detect the main factors impacting the location of logging statements.?????
%\citet{fu2014developers} also conducted a questionnaire survey to validate their findings. ???
%\citet{fu2014developers} and \citet{zhu2015learning} CORRECT ??????


%However, this study is the first work which ..?
%\citet{fu2014developers,zhu2015learning} applied a data mining approach to automatically detecting the important factors that impact the location of logging statements. They also validated their findings by conducting a questionnaire survey. ??
%\citet{ding2015log2} proposed an constraint-based approach to decide whether to log for each logging request at run-time in order to minimize the performance overhead. ??
%Their study shows that developers spend great effort modifying log statements as after-thoughts, which indicates that they are not satisfied with the log quality in their first attempt.
%\cite{Yuan-open}  used a code clone detection approach  to automatically detecting inconsistent verbosity levels .
%However, these studies only consider source code fragments containing bugs that are needed to be logged and do not consider the other code fragments with no bugs but still needed to be logged.???

%\citet{yuan2012conservative} ??







% FU???
% Sadi
%Ding??
%Aspect?
%first?
\section{Correspondence}  \label{ch7-corr}
Several studies have been conducted to find the similarities and differences between source code fragments. \citet{baxter1998clone} develop an algorithm to detect code clones in source code that uses hash functions to partition subtrees of ASTs of a program and then finds common subtrees in the same partition through a tree comparison algorithm. \citet{apiwattanapong2004differencing} present a top-down approach to detect differences and correspondences between two versions of a \tool{Java} program, through comparison of the control flow graphs created from source code. \citet{2006:tse:holmes} recommend relevant code snippet examples from a source code repository for the sake of helping developers to find examples of how to use an API by heuristically matching the structure of the code under development with the code in the repository. \tool{Coogle} \cite{sager2006detecting} was developed to detect similar \tool{Java} classes by converting ASTs to a normalized format and then comparing them through tree similarity algorithms. However, none of these approaches construct a detailed view of structural generalizations needed in my context.

%Umami
%Holmes?


\citet{2014:uofc:cossette} present a new approach, called matching via structural generalization (MSG), to recommend replacements for API migration. They applied \tool{Jigsaw} to find structural correspondences; however, their algorithm does not suffice to construct structural generalizations that represent the detailed commonalities and differences of a set of source code fragments with special attention to log statements, which is required to solve my problem.
%. It also does not take the required constraints in determining correspondences needed to solve our problem.

\section{API usage patterns}  \label{ch7-usage-patterns}
Various data mining approaches have been used to extract API usage patterns out of source code such as unordered pattern mining and sequential pattern mining \cite{robillard2013automated}. Unordered pattern mining, such as association rule mining and itemset mining, extracts a set of API usage rules without considering their order \cite{agrawal1994fast}. \tool{CodeWeb} \cite{2000:icse:michail} uses data mining association rules to identify reuse patterns between a source code under development and a specific library. \tool{PR-Miner} \cite{li2005pr} uses frequent itemset mining to extract implicit programming rules from source code and detect violations. The sequential pattern mining technique is different from the unordered one in the way that it considers the order of API usage. As an example, \tool{MAPO} \cite{2006:msr:xie} combines frequent subsequence mining with clustering to extract API usage patterns from source code.

Another technique for extracting API usage patterns is through statistical source code analysis. For example, \tool{PopCon} \cite{holmes2007informing} is a tool developed to help developers understanding how to use APIs in their source code, by calculating popularity statistics for each API of a library. \citet{acharya2007mining} present a framework to extract API usage scenarios as partial orders, as specifications were extracted from frequent partial orders. They adapted a compile time model checker to generate control-flow-sensitive static traces of APIs, from which API usage scenarios were extracted. However, none of these approaches suffice to construct the detailed structural generalizations needed in my context.
%However, none of these approaches suffice to determine the detailed structural correspondences.

\section{Anti-unification}  \label{ch7-au}
Anti-unification is the problem of finding the most specific generalization of two terms. First-order syntactical anti-unification was introduced by \citet{plotkin1970note} and \citet{reynolds1970transformational}, independently. \citet{burghardt1996implementing} extend the notion of anti-unification to E-anti-unification to incorporate background knowledge to syntactical anti-unification, which is required for some applications. Anti-unification and its extensions have been applied in various studies for program analysis. \citet{2009:iwsc:bulychev} suggest an anti-unification algorithm to detect clones in ASTs. Their approach consists of three stages: first, identifying similar statements through anti-unification and grouping them into clusters; second, determining similar sequences of statements with the same cluster identifier; third, refining candidate statement sequences using an anti-unification based similarity measurement to generate final clones. However, their approach does not construct structural generalizations.
%?

\citet{2007:esec_fse:cottrell} propose \tool{Breakaway} to automatically determine structural correspondences between a pair of ASTs to create a generalized correspondence view. However, their approach does not allow the determination of the best structural correspondence for each AST node required to my context, and only approximates higher-order AU (not HOAUMT). \citet{2008:fse:cottrell} extended their earlier work to HOAUMT in order to develop \tool{Jigsaw}, a tool to help developers integrate small-scale reused code into their own source code by determining structural correspondences. Although I used the \tool{Jigsaw} framework to find potential correspondences between AST nodes, their approach does not suffice to construct structural generalizations from a set of source code fragments by considering the limitations of this study in determining correspondences.

%\item Sadi [2011] proposed an anti-unification algorithm to characterize the location of logging usages in the source code, however,
%\begin{itemize} [leftmargin=.3in]
%\item he has not applied anti-unification appropriately!!! \RW{You would need to explain what that means}
% Higher-order anti-unification modulo theories is formally undecidable [Burghardt, 2005].

%\section{Clustering}  \label{ch7-clustering}

%optimal clusters
%Clustering is an unsupervised machine mining technique that aims to organize a collection of data into clusters, such that intra-cluster similarity is maximized and the inter-cluster similarity is minimized \cite{karypis1999chameleon,grira2004unsupervised}. We divided existing clustering approaches into two major categories: partitional clustering and hierarchical clustering. Partitional clustering try to classify a data set into $k$ clusters such that the partition optimizes a pre-determined criterion \cite{karypis1999chameleon}. The most popular partitional clustering algorithm is $k$-means, which repeatedly assigns each data point to a cluster with the nearest centroid and computes the new cluster centroids accordingly until a pre-determined number of clusters is obtained \cite{bouguettaya2015efficient}. However, the $k$-means clustering algorithm is not a good fit to our problem, as it requires to predefine the number of clusters we need to come up with, which is not reasonable in our context. %?

%Hierarchical clustering algorithms produce a nested grouping of clusters, with single point clusters at the bottom and an all-inclusive cluster at the top \cite{karypis1999chameleon}. Agglomerative hierarchical clustering is one of the main stream clustering methods \cite{day1984efficient} and has applications in document retrieval \cite{voorhees1986implementing} and information retrieval from a search engine query log \cite{beeferman2000agglomerative}. It starts with singleton clusters, where each contains one data point. Then it repeatedly merges the two most similar clusters to form a bigger one until a pre-determined number of clusters is obtained or the similarity between the closest clusters becomes below a pre-determined threshold value. Hierarchical clustering algorithms work implicitly or explicitly with the $n \times n$ similarity matrix such that an element in row $i$ and column $j$ represents the similarity between the $i$th and the $j$th clusters \cite{karypis1999chameleon}.
% agglomerative hierarchical or hierarchical ???

%There are various versions of agglomerative hierarchical algorithms that mainly differ in how they update the similarity between clusters. There are various methods to measure the similarity between clusters, such as single linkage, complete linkage, average linkage, and centroids [Rasmussen, 1992] \RW{Put into bibtex}. In the single linkage method, the similarity is measured by the similarity of the closest pair of data points of two clusters. In the complete linkage method, the similarity is computed by the similarity of the farthest pair of data points of two clusters. In the average linkage method, the similarity is measured by the average of all pairwise similarities between data points of two clusters. In the centroids methods, each cluster is represented by a centroid of all data points in the cluster, and the similarity between two clusters is measured by the similarity of the clusters' centroids.
%However, in our application, each cluster is composed of one AUAST, and the similarity between two clusters is measured by the similarity between the clusters' AUASTs, which is the ratio of the number of common pieces over the total number of pieces of their anti-unifier.
% (see Section~\ref{meth-similarity}).
%optimal clusters- read dr. Denzinger email
%similarity and distance two sides of a coin
% k-means,the predetermined criterion is met
% Hierarchical clustering approaches produce clusters of higher quality, however, these approaches suffer from high time cost[Bouguettay]

\section{Summary}  \label{back-summary}
%machine learning, other generalization approaches

Despite the great importance of logging and its various applications in software development and maintenance, few studies have focused on understanding logging usage in source code.
Some work has been done on characterizing log messages modifications made by developers and to help them enhance the content of log messages. Several data mining and statistical source code analysis techniques have been used to extract API usage patterns, however, none of them enable us to construct the detailed structural generalizations of a set of source code fragments. On the other hand, using higher-order anti-unification modulo theories and an agglomerative hierarchical clustering algorithm allow us to construct generalizations representing the commonalities and differences between ASTs of logged methods and grouping them into clusters based on the structural correspondences.
%However, to my knowledge, no study has been conducted on characterizing where logging is used in source code via structural generalization and clustering.
